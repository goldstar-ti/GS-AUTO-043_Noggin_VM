This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: ../../../Untitled-1, ../Untitled-1, common/__init__.py, common/config.py, common/csv_importer.py, common/database.py, common/hash_manager.py, common/logger.py, common/object_types.py, common/rate_limiter.py, config, config/base_config.ini, config/coupling_compliance_check_config.ini, config/forklift_prestart_inspection_config.ini, config/load_compliance_check_driver_loader_config.ini, config/load_compliance_check_supervisor_manager_config.ini, config/sftp_config.ini, config/site_observations_config.ini, config/trailer_audits_config.ini, processors/__init__.py, processors/base_processor.py, processors/field_processor.py, processors/object_processor.py, processors/report_generator.py, scratch/noggin_sample_payloads_20260113_113816.json, scratch/fetch_noggin_samples.py, sys/log_maintenance.py, sys/system_audit.sh, sys/system_context.txt, web/templates/base.html, web/templates/dashboard.html, web/templates/hashes.html, web/templates/inspection_detail.html, web/templates/inspections.html, web/templates/service_status.html, web/web_config.md, web/app.py, web/get_server_config.sh, archive_monthly_sftp.py, hash_lookup_sync.py, manage_hashes.py, noggin_continuous_processor_modular.py, noggin_continuous_processor.py, noggin_processor_LCD_ONLY.py, noggin_processor_unified.py, noggin_processor.py, service_dashboard.py, sftp_download_tips.py, test_circuit_breaker.py, test_common.py, test_connection.py, test_csv_importer.py, test_database.py, test_hash_manager.py, test_sftp_downloader.py, manage_service.sh, test_systemd_service.sh
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
common/
  __init__.py
  config.py
  csv_importer.py
  database.py
  hash_manager.py
  logger.py
  object_types.py
  rate_limiter.py
config/
  base_config.ini
  coupling_compliance_check_config.ini
  forklift_prestart_inspection_config.ini
  load_compliance_check_driver_loader_config.ini
  load_compliance_check_supervisor_manager_config.ini
  sftp_config.ini
  site_observations_config.ini
  trailer_audits_config.ini
processors/
  __init__.py
  base_processor.py
  field_processor.py
  object_processor.py
  report_generator.py
sys/
  log_maintenance.py
  system_audit.sh
  system_context.txt
web/
  templates/
    base.html
    dashboard.html
    hashes.html
    inspection_detail.html
    inspections.html
    service_status.html
  app.py
  get_server_config.sh
  web_config.md
archive_monthly_sftp.py
hash_lookup_sync.py
manage_hashes.py
manage_service.sh
noggin_continuous_processor_modular.py
noggin_continuous_processor.py
noggin_processor_LCD_ONLY.py
noggin_processor_unified.py
noggin_processor.py
service_dashboard.py
sftp_download_tips.py
test_circuit_breaker.py
test_common.py
test_connection.py
test_csv_importer.py
test_database.py
test_hash_manager.py
test_sftp_downloader.py
test_systemd_service.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="noggin_processor_LCD_ONLY.py">
   1: """ noggin_processor.py
   2:     Module purpose
   3:     ---------------
   4:     This module processes "Load Compliance Check" (LCD) inspection records by querying
   5:     a PostgreSQL database for eligible TIP identifiers. It retrieves JSON payloads
   6:     from a remote API, stores metadata in PostgreSQL, downloads and validates
   7:     attachment media, and writes human-readable inspection reports and attachment
   8:     files to disk. It includes robust retry/backoff logic, a circuit breaker to
   9:     protect the API, and graceful shutdown handling.
  10: 
  11:     High-level behaviour
  12:     --------------------
  13:     - Queries the 'noggin_data' table for a batch of TIPs that are 'pending',
  14:       'csv_imported', or eligible for retry.
  15:     - Processing priority is strictly enforced:
  16:       failed -> interrupted -> partial -> api_failed -> pending -> csv_imported.
  17:     - For each TIP:
  18:         - Checks the Circuit Breaker status before making requests.
  19:         - Uses an endpoint template ($tip) to build the API URL and GETs the JSON.
  20:         - Updates the existing noggin_data row with parsed response fields and meta.
  21:         - Creates a dated folder structure and writes a formatted text payload file.
  22:         - Downloads listed attachments, validates file integrity, computes MD5 and
  23:           records attachment state in the attachments table.
  24:         - Tracks errors in processing_errors and updates retry/backoff state on errors.
  25:     - Honors graceful shutdown (SIGINT/SIGTERM) finishing the current TIP when
  26:       possible; a second signal forces immediate exit.
  27:     - Emits logging to both application logger and a session logger file.
  28: 
  29:     Primary public functions/classes
  30:     -------------------------------
  31:     - GracefulShutdownHandler(db_conn, logger_instance)
  32:         Handles SIGINT/SIGTERM, closes DB connections on exit.
  33:     - create_inspection_folder_structure(date_str, noggin_reference)
  34:         Builds and creates a hierarchical path base_path/YYYY/MM/YYYY-MM-DD <id>.
  35:     - save_formatted_payload_text_file(inspection_folder, response_data, noggin_reference)
  36:         Writes a human-readable inspection report to a text file.
  37:     - download_attachment(attachment_url, filename, noggin_reference, ...)
  38:         Downloads, validates, and hashes a single attachment; updates DB records.
  39:     - process_attachments(response_data, noggin_reference, tip_value)
  40:         Orchestrates saving the payload file and iteratively downloading attachments.
  41:     - insert_noggin_data_record(tip_value, response_data)
  42:         Parses response_data and UPDATEs the existing noggin_data table record.
  43:     - get_tips_to_process_from_database(limit=10)
  44:         Retrieves a batch of TIPs eligible for processing based on priority logic.
  45:     - main()
  46:         The main processing loop: fetches a batch of TIPs from the DB, coordinates
  47:         circuit breaker, API calls, DB updates, and attachment processing.
  48: """
  49: from __future__ import annotations
  50: import requests
  51: import json
  52: import logging
  53: import uuid
  54: from datetime import datetime, timedelta
  55: from pathlib import Path
  56: import time
  57: import signal
  58: import sys
  59: import atexit
  60: import hashlib
  61: from typing import Optional, List, Dict, Any, Tuple
  62: 
  63: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager, CircuitBreaker, CircuitBreakerError, UNKNOWN_TEXT
  64: 
  65: start_time: float = time.perf_counter()
  66: 
  67: config: ConfigLoader = ConfigLoader(
  68:     'config/base_config.ini',
  69:     'config/load_compliance_check_driver_loader_config.ini'
  70: )
  71: 
  72: batch_session_id: str = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_batch_{str(uuid.uuid4())[:8].upper()}"
  73: 
  74: logger_manager: LoggerManager = LoggerManager(config, script_name=Path(__file__).stem)
  75: logger_manager.configure_application_logger()
  76: session_logger: logging.Logger = logger_manager.create_session_logger(batch_session_id)
  77: 
  78: logger: logging.Logger = logging.getLogger(__name__)
  79: 
  80: db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
  81: hash_manager: HashManager = HashManager(config, db_manager)
  82: circuit_breaker: CircuitBreaker = CircuitBreaker(config)
  83: 
  84: base_url: str = config.get('api', 'base_url')
  85: attachment_base_url: str = config.get('api', 'media_service_url')
  86: headers: Dict[str, str] = config.get_api_headers()
  87: 
  88: # Force standard headers that Bruno/browsers send automatically
  89: headers['Accept'] = 'application/json'
  90: headers['Content-Type'] = 'application/json'
  91: # Many servers block the default 'python-requests' agent; use a custom one
  92: headers['User-Agent'] = 'NogginLCDProcessor/1.0 (Internal Integration)'
  93: 
  94: base_path: Path = Path(config.get('paths', 'base_output_path'))
  95: base_path.mkdir(parents=True, exist_ok=True)
  96: 
  97: too_many_requests_sleep_time: int = config.getint('processing', 'too_many_requests_sleep_time')
  98: attachment_pause: int = config.getint('processing', 'attachment_pause')
  99: max_api_retries: int = config.getint('processing', 'max_api_retries')
 100: api_backoff_factor: int = config.getint('processing', 'api_backoff_factor')
 101: api_max_backoff: int = config.getint('processing', 'api_max_backoff')
 102: api_timeout: int = config.getint('processing', 'api_timeout')
 103: 
 104: show_json_payload_in_text_file: bool = config.getboolean('output', 'show_json_payload_in_text_file', from_specific=True)
 105: show_compliance_status: bool = config.getboolean('output', 'show_compliance_status', from_specific=True)
 106: filename_image_stub: str = config.get('output', 'filename_image_stub', from_specific=True)
 107: unknown_response_output_text: str = config.get('output', 'unknown_response_output_text', from_specific=True)
 108: folder_pattern: str = config.get('output', 'folder_pattern', from_specific=True)
 109: attachment_pattern: str = config.get('output', 'attachment_pattern', from_specific=True)
 110: 
 111: abbreviation: str = config.get('object_type', 'abbreviation', from_specific=True)
 112: 
 113: object_type_config: Dict[str, str] = config.get_object_type_config()
 114: endpoint_template: str = object_type_config['endpoint']
 115: 
 116: # Hardcoded as per requirements
 117: object_type: str = 'LCD'
 118: 
 119: shutdown_requested: bool = False
 120: current_tip_being_processed: Optional[str] = None
 121: 
 122: logger.info("="*80)
 123: logger.info(f"NOGGIN PROCESSOR - {object_type}")
 124: logger.info("="*80)
 125: logger.info(f"Session ID:         {batch_session_id}")
 126: logger.info(f"Object Type:        {object_type}")
 127: logger.info(f"Abbreviation:       {abbreviation}")
 128: logger.info(f"Base Output Path:   {base_path}")
 129: logger.info(f"Folder Pattern:     {folder_pattern}")
 130: logger.info(f"Attachment Pattern: {attachment_pattern}")
 131: logger.info("="*80)
 132: 
 133: session_logger.info(f"SESSION START: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
 134: session_logger.info(f"SESSION ID: {batch_session_id}")
 135: session_logger.info(f"OBJECT TYPE: {object_type}")
 136: session_logger.info("")
 137: session_logger.info("TIMESTAMP\tTIP\tNOGGIN_REFERENCE\tATTACHMENTS_COUNT\tATTACHMENT_FILENAMES")
 138: 
 139: class GracefulShutdownHandler:
 140:     """Handles Ctrl+C and system shutdown signals"""
 141: 
 142:     def __init__(self, db_conn: DatabaseConnectionManager, logger_instance: logging.Logger) -> None:
 143:         self.db_conn: DatabaseConnectionManager = db_conn
 144:         self.logger: logging.Logger = logger_instance
 145:         self.shutdown_requested: bool = False
 146: 
 147:         signal.signal(signal.SIGINT, self._signal_handler)
 148:         signal.signal(signal.SIGTERM, self._signal_handler)
 149:         atexit.register(self._cleanup_on_exit)
 150: 
 151:         self.logger.info("Graceful shutdown handler initialised")
 152: 
 153:     def _signal_handler(self, signum: int, frame: Any) -> None:
 154:         global shutdown_requested
 155:         signal_name: str = "SIGINT (Ctrl+C)" if signum == signal.SIGINT else f"Signal {signum}"
 156: 
 157:         if not self.shutdown_requested:
 158:             self.shutdown_requested = True
 159:             shutdown_requested = True
 160:             self.logger.warning(f"\n{signal_name} received. Finishing current TIP then shutting down...")
 161:             self.logger.warning(f"Currently processing: {current_tip_being_processed or 'None'}")
 162:             self.logger.warning("Press Ctrl+C again to force immediate exit")
 163:         else:
 164:             self.logger.error("Second shutdown signal - forcing immediate exit")
 165:             self._emergency_cleanup()
 166:             sys.exit(1)
 167: 
 168:     def _cleanup_on_exit(self) -> None:
 169:         if self.db_conn:
 170:             try:
 171:                 self.db_conn.close_all()
 172:             except Exception as e:
 173:                 self.logger.error(f"Error during exit cleanup: {e}")
 174: 
 175:     def _emergency_cleanup(self) -> None:
 176:         try:
 177:             if self.db_conn:
 178:                 self.db_conn.close_all()
 179:         except:
 180:             pass
 181: 
 182:     def should_continue_processing(self) -> bool:
 183:         return not self.shutdown_requested
 184: 
 185: shutdown_handler: GracefulShutdownHandler = GracefulShutdownHandler(db_manager, logger)
 186: 
 187: def sanitise_filename(text: Optional[str]) -> str:
 188:     """Sanitise text for use in filenames"""
 189:     if not text:
 190:         return "unknown"
 191:     return (text.replace(" - ", "-").replace(" ", "_").replace("/", "")
 192:             .replace("\\", "").replace("*", "").replace("<", "")
 193:             .replace(">", "").replace("?", "").replace("|", "").replace(":", ""))
 194: 
 195: def flatten_json(nested_json: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
 196:     """Flatten nested JSON structure"""
 197:     items: List[Tuple[str, Any]] = []
 198:     for key, value in nested_json.items():
 199:         new_key: str = f"{parent_key}{sep}{key}" if parent_key else key
 200: 
 201:         if isinstance(value, dict):
 202:             items.extend(flatten_json(value, new_key, sep=sep).items())
 203:         elif isinstance(value, list):
 204:             for i, item in enumerate(value):
 205:                 if isinstance(item, dict):
 206:                     items.extend(flatten_json(item, f"{new_key}_{i}", sep=sep).items())
 207:                 else:
 208:                     items.append((f"{new_key}_{i}", item))
 209:         else:
 210:             items.append((new_key, value))
 211:     return dict(items)
 212: 
 213: def create_inspection_folder_structure(date_str: str, noggin_reference: str) -> Path:
 214:     """Create hierarchical folder structure for inspection using configured pattern.
 215: 
 216:     Pattern placeholders: {abbreviation}, {year}, {month}, {date}, {inspection_id}
 217:     """
 218:     try:
 219:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 220:         year: str = str(date_obj.year)
 221:         month: str = f"{date_obj.month:02d}"
 222:         formatted_date: str = date_obj.strftime('%Y-%m-%d')
 223: 
 224:         # Substitute pattern placeholders (mapping noggin_reference to inspection_id)
 225:         folder_path: str = folder_pattern.format(
 226:             abbreviation=abbreviation,
 227:             year=year,
 228:             month=month,
 229:             date=formatted_date,
 230:             inspection_id=noggin_reference
 231:         )
 232: 
 233:         inspection_folder: Path = base_path / folder_path
 234:         inspection_folder.mkdir(parents=True, exist_ok=True)
 235:         logger.debug(f"Created inspection folder: {inspection_folder}")
 236:         return inspection_folder
 237:     except (ValueError, AttributeError) as e:
 238:         logger.warning(f"Could not parse date '{date_str}': {e}")
 239:         folder_name = f"unknown-date {noggin_reference}"
 240:         fallback_folder: Path = base_path / abbreviation / "unknown_date" / folder_name
 241:         fallback_folder.mkdir(parents=True, exist_ok=True)
 242:         logger.info(f"Created fallback folder: {fallback_folder}")
 243:         return fallback_folder
 244: 
 245: def construct_attachment_filename(noggin_reference: str, date_str: str, attachment_num: int) -> str:
 246:     """Construct attachment filename using configured pattern."""
 247:     sanitised_id: str = sanitise_filename(noggin_reference)
 248: 
 249:     try:
 250:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 251:         date_part: str = date_obj.strftime('%Y-%m-%d')
 252:     except (ValueError, AttributeError):
 253:         logger.warning(f"Could not parse date '{date_str}', using 'unknown'")
 254:         date_part = "unknown"
 255: 
 256:     filename: str = attachment_pattern.format(
 257:         abbreviation=abbreviation,
 258:         inspection_id=sanitised_id,
 259:         date=date_part,
 260:         stub=filename_image_stub,
 261:         sequence=f"{attachment_num:03d}"
 262:     )
 263:     return filename
 264: 
 265: def calculate_md5_hash(file_path: Path) -> str:
 266:     """Calculate MD5 hash of file"""
 267:     md5_hash: hashlib._Hash = hashlib.md5()
 268:     try:
 269:         with open(file_path, 'rb') as f:
 270:             for chunk in iter(lambda: f.read(8192), b""):
 271:                 md5_hash.update(chunk)
 272:         return md5_hash.hexdigest()
 273:     except Exception as e:
 274:         logger.warning(f"Could not calculate MD5 for {file_path}: {e}")
 275:         return ""
 276: 
 277: def validate_attachment_file(file_path: Path, expected_min_size: int = 1024) -> Tuple[bool, int, Optional[str]]:
 278:     """Validate downloaded file integrity"""
 279:     try:
 280:         if not file_path.exists():
 281:             return False, 0, "File does not exist"
 282: 
 283:         file_size: int = file_path.stat().st_size
 284: 
 285:         if file_size < expected_min_size:
 286:             return False, file_size, f"File too small ({file_size} bytes)"
 287: 
 288:         with open(file_path, 'rb') as f:
 289:             header_bytes: bytes = f.read(10)
 290:             if len(header_bytes) == 0:
 291:                 return False, file_size, "File appears empty"
 292: 
 293:         return True, file_size, None
 294: 
 295:     except Exception as e:
 296:         return False, 0, f"Validation error: {e}"
 297: 
 298: def save_formatted_payload_text_file(inspection_folder: Path, response_data: Dict[str, Any],
 299:                                     noggin_reference: str) -> Optional[Path]:
 300:     """Generate formatted text file with inspection data"""
 301:     sanitised_ref: str = sanitise_filename(noggin_reference)
 302:     payload_filename: str = f"{sanitised_ref}_inspection_data.txt"
 303:     payload_path: Path = inspection_folder / payload_filename
 304: 
 305:     try:
 306:         with open(payload_path, 'w', encoding='utf-8') as f:
 307:             f.write("="*60 + "\n")
 308:             f.write("LOAD COMPLIANCE CHECK INSPECTION REPORT\n")
 309:             f.write(f"RECORD GENERATED: {datetime.now().strftime('%d-%m-%Y')}\n")
 310:             f.write("="*60 + "\n\n")
 311: 
 312:             f.write(f"Noggin Reference:      {response_data.get('lcdInspectionId', unknown_response_output_text)}\n\n")
 313:             f.write(f"Date:                  {response_data.get('date', unknown_response_output_text)}\n\n")
 314:             f.write(f"Inspected By:          {response_data.get('inspectedBy', unknown_response_output_text)}\n\n")
 315: 
 316:             vehicle_hash: str = response_data.get('vehicle', '')
 317:             vehicle_name: str = hash_manager.lookup_hash('vehicle', vehicle_hash, response_data.get('tip', ''), noggin_reference) if vehicle_hash else unknown_response_output_text
 318:             f.write(f"Vehicle:               {vehicle_name}\n\n")
 319:             f.write(f"Vehicle ID:            {response_data.get('vehicleId', unknown_response_output_text)}\n\n")
 320: 
 321:             trailer_hash: str = response_data.get('trailer', '')
 322:             trailer_name: str = hash_manager.lookup_hash('trailer', trailer_hash, response_data.get('tip', ''), noggin_reference) if trailer_hash else unknown_response_output_text
 323:             f.write(f"Trailer:               {trailer_name}\n\n")
 324:             f.write(f"Trailer ID:            {response_data.get('trailerId', unknown_response_output_text)}\n\n")
 325: 
 326:             trailer2_hash: str = response_data.get('trailer2', '')
 327:             if trailer2_hash:
 328:                 trailer2_name: str = hash_manager.lookup_hash('trailer', trailer2_hash, response_data.get('tip', ''), noggin_reference)
 329:                 f.write(f"Trailer 2:             {trailer2_name}\n\n")
 330:                 f.write(f"Trailer 2 ID:          {response_data.get('trailerId2', unknown_response_output_text)}\n\n")
 331: 
 332:             trailer3_hash: str = response_data.get('trailer3', '')
 333:             if trailer3_hash:
 334:                 trailer3_name: str = hash_manager.lookup_hash('trailer', trailer3_hash, response_data.get('tip', ''), noggin_reference)
 335:                 f.write(f"Trailer 3:             {trailer3_name}\n\n")
 336:                 f.write(f"Trailer 3 ID:          {response_data.get('trailerId3', unknown_response_output_text)}\n\n")
 337: 
 338:             f.write(f"Job Number:            {response_data.get('jobNumber', unknown_response_output_text)}\n\n")
 339:             f.write(f"Run Number:            {response_data.get('runNumber', unknown_response_output_text)}\n\n")
 340:             f.write(f"Driver/Loader Name:    {response_data.get('driverLoaderName', unknown_response_output_text)}\n\n")
 341: 
 342:             dept_hash: str = response_data.get('whichDepartmentDoesTheLoadBelongTo', '')
 343:             dept_name: str = hash_manager.lookup_hash('department', dept_hash, response_data.get('tip', ''), noggin_reference) if dept_hash else unknown_response_output_text
 344:             f.write(f"Department:            {dept_name}\n\n")
 345: 
 346:             team_hash: str = response_data.get('team', '')
 347:             team_name: str = hash_manager.lookup_hash('team', team_hash, response_data.get('tip', ''), noggin_reference) if team_hash else unknown_response_output_text
 348:             f.write(f"Team:                  {team_name}\n\n")
 349: 
 350:             if show_compliance_status:
 351:                 compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 352:                 compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 353:                 if compliant_yes:
 354:                     f.write("Load Compliance:       COMPLIANT\n\n")
 355:                 elif compliant_no:
 356:                     f.write("Load Compliance:       NON-COMPLIANT\n\n")
 357:                 else:
 358:                     f.write(f"Load Compliance:       {unknown_response_output_text}\n\n")
 359: 
 360:             straps_value = response_data.get('straps')
 361:             if straps_value is not None:
 362:                 f.write(f"Straps:                {straps_value}\n\n")
 363:                 no_of_straps = response_data.get('noOfStraps', UNKNOWN_TEXT)
 364:                 f.write(f"Number of Straps:      {no_of_straps}\n\n")
 365: 
 366:             chains_value = response_data.get('chains')
 367:             if chains_value is not None:
 368:                 f.write(f"Chains:                {chains_value}\n\n")
 369: 
 370:             mass_value = response_data.get('mass', UNKNOWN_TEXT)
 371:             f.write(f"Mass:                  {mass_value}\n\n")
 372: 
 373:             attachment_count: int = len(response_data.get('attachments', []))
 374:             f.write(f"Attachments:           {attachment_count}\n\n")
 375: 
 376:             if show_json_payload_in_text_file:
 377:                 f.write("-"*60 + "\n")
 378:                 f.write("COMPLETE TECHNICAL DATA (JSON FORMAT)\n")
 379:                 f.write("-"*60 + "\n\n")
 380:                 json.dump(response_data, f, indent=2, ensure_ascii=False)
 381: 
 382:         logger.info(f"Saved formatted payload to: {payload_path}")
 383:         return payload_path
 384: 
 385:     except IOError as e:
 386:         logger.error(f"IOError saving payload {payload_path}: {e}", exc_info=True)
 387:         return None
 388: 
 389: def make_api_request(url: str, headers: Dict[str, str], tip_value: str,
 390:                     max_retries: int = 5, backoff_factor: int = 2,
 391:                     timeout: int = 30, max_backoff: int = 60) -> requests.Response:
 392:     """Make API request with exponential backoff retry logic"""
 393:     last_exception: Optional[Exception] = None
 394: 
 395:     logger.debug(f"DEBUG OUTGOING HEADERS: {json.dumps(headers, default=str)}")
 396: 
 397:     # Force JSON acceptance which is often required but missing in default requests
 398:     if 'Accept' not in headers:
 399:         headers['Accept'] = 'application/json'
 400:     
 401:     logger.info(f"DEBUG: Request Headers: {json.dumps(headers, indent=2)}")
 402:     logger.info(f"DEBUG: Request URL: {url}")
 403: 
 404:     for attempt in range(max_retries):
 405:         try:
 406:             logger.debug(f"API request attempt {attempt + 1}/{max_retries} for TIP {tip_value}")
 407:             response: requests.Response = requests.get(url, headers=headers, timeout=timeout)
 408:             response._retry_count = attempt
 409:             logger.debug(f"Request attempt {attempt + 1} succeeded for TIP {tip_value}")
 410:             return response
 411: 
 412:         except requests.exceptions.ConnectionError as connection_error:
 413:             last_exception = connection_error
 414: 
 415:             if attempt == max_retries - 1:
 416:                 logger.error(f"All {max_retries} connection attempts failed for TIP {tip_value}", exc_info=True)
 417:                 raise connection_error
 418: 
 419:             wait_time: float = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 420:             logger.warning(f"Connection failed for TIP {tip_value}, retrying in {wait_time}s... "
 421:                           f"(attempt {attempt + 1}/{max_retries})")
 422:             time.sleep(wait_time)
 423: 
 424:         except requests.exceptions.Timeout as timeout_error:
 425:             last_exception = timeout_error
 426: 
 427:             if attempt == max_retries - 1:
 428:                 logger.error(f"All {max_retries} timeout attempts failed for TIP {tip_value}", exc_info=True)
 429:                 raise timeout_error
 430: 
 431:             wait_time = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 432:             logger.warning(f"Request timeout for TIP {tip_value}, retrying in {wait_time}s... "
 433:                           f"(attempt {attempt + 1}/{max_retries})")
 434:             time.sleep(wait_time)
 435: 
 436:         except requests.exceptions.RequestException as request_error:
 437:             last_exception = request_error
 438: 
 439:             if attempt == max_retries - 1:
 440:                 logger.error(f"Request failed permanently for TIP {tip_value}: {str(request_error)}", exc_info=True)
 441:                 raise request_error
 442: 
 443:             wait_time = backoff_factor
 444:             logger.warning(f"Request error for TIP {tip_value}, retrying in {wait_time}s... "
 445:                           f"(attempt {attempt + 1}/{max_retries})")
 446:             time.sleep(wait_time)
 447: 
 448:     if last_exception:
 449:         raise last_exception
 450:     else:
 451:         raise Exception(f"Unexpected error in retry logic for TIP {tip_value}")
 452: 
 453: def handle_api_error(response: requests.Response, tip_value: str, request_url: str) -> str:
 454:     """Generate detailed error message from API response"""
 455:     status_code: int = response.status_code
 456: 
 457:     try:
 458:         response_text: str = response.text
 459:         if response_text:
 460:             try:
 461:                 error_json: Dict[str, Any] = response.json()
 462:                 additional_info: str = f" Response body: {json.dumps(error_json, indent=2)}"
 463:             except json.JSONDecodeError:
 464:                 additional_info = f" Response body: {response_text[:500]}{'...' if len(response_text) > 500 else ''}"
 465:         else:
 466:             additional_info = " (No response body provided)"
 467:     except Exception:
 468:         additional_info = " (Could not read response body)"
 469: 
 470:     if status_code == 401:
 471:         error_message: str = (f"Authentication failed for TIP {tip_value}. "
 472:                              f"Status code: {status_code} (Unauthorised). "
 473:                              f"The access token is missing or invalid. "
 474:                              f"URL: {request_url}{additional_info}")
 475: 
 476:     elif status_code == 403:
 477:         error_message = (f"Access forbidden for TIP {tip_value}. "
 478:                         f"Status code: {status_code} (Forbidden). "
 479:                         f"You don't have permission to access this resource. "
 480:                         f"URL: {request_url}{additional_info}")
 481: 
 482:     elif status_code == 404:
 483:         error_message = (f"Resource not found for TIP {tip_value}. "
 484:                         f"Status code: {status_code} (Not Found). "
 485:                         f"The requested object does not exist. "
 486:                         f"URL: {request_url}{additional_info}")
 487: 
 488:     elif status_code == 429:
 489:         error_message = (f"Rate limit exceeded for TIP {tip_value}. "
 490:                         f"Status code: {status_code} (Too Many Requests). "
 491:                         f"URL: {request_url}{additional_info}")
 492: 
 493:     elif 400 <= status_code < 500:
 494:         error_message = (f"Client error for TIP {tip_value}. "
 495:                         f"Status code: {status_code}. "
 496:                         f"URL: {request_url}{additional_info}")
 497: 
 498:     elif 500 <= status_code < 600:
 499:         error_message = (f"Server error for TIP {tip_value}. "
 500:                         f"Status code: {status_code}. "
 501:                         f"URL: {request_url}{additional_info}")
 502: 
 503:     else:
 504:         error_message = (f"Unexpected response for TIP {tip_value}. "
 505:                         f"Status code: {status_code}. "
 506:                         f"URL: {request_url}{additional_info}")
 507: 
 508:     return error_message
 509: 
 510: def download_attachment(attachment_url: str, filename: str, noggin_reference: str,
 511:                        attachment_tip: str, inspection_folder: Path,
 512:                        record_tip: str, attachment_sequence: int) -> Tuple[bool, int, float, Optional[str]]:
 513:     """Download and validate attachment with database tracking"""
 514:     if attachment_url.startswith('/media'):
 515:         attachment_url = attachment_url[6:]
 516: 
 517:     full_url: str = attachment_base_url + attachment_url
 518:     output_path: Path = inspection_folder / filename
 519:     temp_path: Path = output_path.with_suffix('.tmp')
 520: 
 521:     existing_attachment: List[Dict[str, Any]] = db_manager.execute_query_dict(
 522:         "SELECT attachment_status, file_size_bytes FROM attachments WHERE record_tip = %s AND attachment_tip = %s",
 523:         (record_tip, attachment_tip)
 524:     )
 525: 
 526:     if existing_attachment and existing_attachment[0]['attachment_status'] == 'complete':
 527:         if output_path.exists():
 528:             is_valid, file_size, error_msg = validate_attachment_file(output_path)
 529:             if is_valid:
 530:                 file_size_mb: float = file_size / (1024 * 1024)
 531:                 logger.info(f"Skipping existing valid attachment: {filename} ({file_size_mb:.2f} MB)")
 532:                 return True, 0, file_size_mb, None
 533: 
 534:     download_start_time: float = time.perf_counter()
 535:     logger.info(f"Downloading {noggin_reference}: {filename}")
 536: 
 537:     db_manager.execute_update(
 538:         """
 539:         INSERT INTO attachments (
 540:             record_tip, attachment_tip, attachment_sequence, filename, file_path,
 541:             attachment_status, attachment_validation_status, download_started_at
 542:         ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
 543:         ON CONFLICT (record_tip, attachment_tip)
 544:         DO UPDATE SET
 545:             attachment_status = EXCLUDED.attachment_status,
 546:             download_started_at = EXCLUDED.download_started_at
 547:         """,
 548:         (record_tip, attachment_tip, attachment_sequence, filename, str(output_path.resolve()),
 549:          'downloading', 'not_validated', datetime.now())
 550:     )
 551: 
 552:     try:
 553:         response: requests.Response = make_api_request(full_url, headers, f"attachment {filename}", timeout=60)
 554:         retry_count: int = getattr(response, '_retry_count', 0)
 555: 
 556:         if response.status_code == 200:
 557:             with open(temp_path, 'wb') as f:
 558:                 f.write(response.content)
 559: 
 560:             is_valid, file_size, validation_error = validate_attachment_file(temp_path)
 561: 
 562:             if is_valid:
 563:                 temp_path.rename(output_path)
 564: 
 565:                 file_hash: str = calculate_md5_hash(output_path)
 566:                 download_duration: float = time.perf_counter() - download_start_time
 567:                 file_size_mb = file_size / (1024 * 1024)
 568: 
 569:                 db_manager.execute_update(
 570:                     """
 571:                     UPDATE attachments
 572:                     SET attachment_status = %s,
 573:                         attachment_validation_status = %s,
 574:                         file_size_bytes = %s,
 575:                         file_hash_md5 = %s,
 576:                         download_completed_at = %s,
 577:                         download_duration_seconds = %s
 578:                     WHERE record_tip = %s AND attachment_tip = %s
 579:                     """,
 580:                     ('complete', 'valid', file_size, file_hash, datetime.now(),
 581:                      round(download_duration, 2), record_tip, attachment_tip)
 582:                 )
 583: 
 584:                 logger.info(f"Downloaded: {filename} ({file_size_mb:.2f} MB) in {download_duration:.2f}s")
 585:                 return True, retry_count, file_size_mb, None
 586:             else:
 587:                 if temp_path.exists():
 588:                     temp_path.unlink()
 589: 
 590:                 error_msg: str = f"Validation failed: {validation_error}"
 591:                 logger.error(f"Download validation failed: {error_msg}")
 592: 
 593:                 db_manager.execute_update(
 594:                     """
 595:                     UPDATE attachments
 596:                     SET attachment_status = %s,
 597:                         attachment_validation_status = %s,
 598:                         validation_error_message = %s,
 599:                         last_error_message = %s
 600:                     WHERE record_tip = %s AND attachment_tip = %s
 601:                     """,
 602:                     ('failed', 'validation_failed', validation_error, error_msg, record_tip, attachment_tip)
 603:                 )
 604: 
 605:                 db_manager.execute_update(
 606:                     """
 607:                     INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 608:                     VALUES (%s, %s, %s, %s)
 609:                     """,
 610:                     (record_tip, 'attachment_failed', error_msg, json.dumps({
 611:                         'filename': filename,
 612:                         'attachment_tip': attachment_tip,
 613:                         'validation_error': validation_error
 614:                     }))
 615:                 )
 616: 
 617:                 return False, retry_count, 0, error_msg
 618:         else:
 619:             if temp_path.exists():
 620:                 temp_path.unlink()
 621: 
 622:             error_msg = f"HTTP {response.status_code}"
 623:             download_duration = time.perf_counter() - download_start_time
 624:             error_details: str = handle_api_error(response, f"attachment {filename}", full_url)
 625:             logger.error(f"Download failed: {error_details}")
 626: 
 627:             db_manager.execute_update(
 628:                 """
 629:                 UPDATE attachments
 630:                 SET attachment_status = %s,
 631:                     last_error_message = %s
 632:                 WHERE record_tip = %s AND attachment_tip = %s
 633:                 """,
 634:                 ('failed', error_msg, record_tip, attachment_tip)
 635:             )
 636: 
 637:             db_manager.execute_update(
 638:                 """
 639:                 INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 640:                 VALUES (%s, %s, %s, %s)
 641:                 """,
 642:                 (record_tip, 'attachment_failed', error_msg, json.dumps({
 643:                     'filename': filename,
 644:                     'attachment_tip': attachment_tip,
 645:                     'http_status': response.status_code,
 646:                     'url': full_url
 647:                 }))
 648:             )
 649: 
 650:             return False, retry_count, 0, error_msg
 651: 
 652:     except Exception as e:
 653:         if temp_path.exists():
 654:             temp_path.unlink()
 655: 
 656:         error_msg = f"Exception: {str(e)}"
 657:         download_duration = time.perf_counter() - download_start_time
 658:         logger.error(f"Download exception: {filename} - {error_msg}", exc_info=True)
 659: 
 660:         db_manager.execute_update(
 661:             """
 662:             UPDATE attachments
 663:             SET attachment_status = %s,
 664:                 last_error_message = %s
 665:             WHERE record_tip = %s AND attachment_tip = %s
 666:             """,
 667:             ('failed', error_msg, record_tip, attachment_tip)
 668:         )
 669: 
 670:         db_manager.execute_update(
 671:             """
 672:             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 673:             VALUES (%s, %s, %s, %s)
 674:             """,
 675:             (record_tip, 'attachment_failed', error_msg, json.dumps({
 676:                 'filename': filename,
 677:                 'attachment_tip': attachment_tip,
 678:                 'exception': str(e)
 679:             }))
 680:         )
 681: 
 682:         return False, 0, 0, error_msg
 683: 
 684: def process_attachments(response_data: Dict[str, Any], noggin_reference: str, tip_value: str) -> None:
 685:     """Process all attachments for an inspection with database tracking"""
 686:     global shutdown_requested
 687: 
 688:     if shutdown_requested:
 689:         logger.warning(f"Shutdown requested during {noggin_reference}")
 690:         db_manager.execute_update(
 691:             "UPDATE noggin_data SET processing_status = %s WHERE tip = %s",
 692:             ('interrupted', tip_value)
 693:         )
 694:         return
 695: 
 696:     processing_start_time: float = time.perf_counter()
 697: 
 698:     date_str: str = response_data.get('date', '')
 699:     inspection_folder: Path = create_inspection_folder_structure(date_str, noggin_reference)
 700:     save_formatted_payload_text_file(inspection_folder, response_data, noggin_reference)
 701: 
 702:     if 'attachments' not in response_data or not response_data['attachments']:
 703:         processing_end_time: float = time.perf_counter()
 704:         processing_duration: float = processing_end_time - processing_start_time
 705: 
 706:         logger.info(f"No attachments found for {noggin_reference}")
 707:         session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{noggin_reference}\t0\tNONE")
 708: 
 709:         db_manager.execute_update(
 710:             """
 711:             UPDATE noggin_data
 712:             SET processing_status = %s,
 713:                 total_attachments = 0,
 714:                 completed_attachment_count = 0,
 715:                 all_attachments_complete = TRUE
 716:             WHERE tip = %s
 717:             """,
 718:             ('complete', tip_value)
 719:         )
 720:         return
 721: 
 722:     attachments: List[str] = response_data['attachments']
 723:     logger.info(f"Processing {len(attachments)} attachments for {noggin_reference}")
 724: 
 725:     db_manager.execute_update(
 726:         "UPDATE noggin_data SET total_attachments = %s WHERE tip = %s",
 727:         (len(attachments), tip_value)
 728:     )
 729: 
 730:     successful_downloads: int = 0
 731:     attachment_filenames: List[str] = []
 732:     total_attachment_retries: int = 0
 733:     total_file_size_mb: float = 0.0
 734: 
 735:     for i, attachment_url in enumerate(attachments, 1):
 736:         if shutdown_requested:
 737:             logger.warning(f"Shutdown during attachment {i}/{len(attachments)} for {noggin_reference}")
 738:             break
 739: 
 740:         attachment_tip: str = attachment_url.split('tip=')[-1] if 'tip=' in attachment_url else 'unknown'
 741:         filename: str = construct_attachment_filename(noggin_reference, date_str, i)
 742: 
 743:         success, retry_count, file_size_mb, error_msg = download_attachment(
 744:             attachment_url, filename, noggin_reference, attachment_tip,
 745:             inspection_folder, tip_value, i
 746:         )
 747: 
 748:         total_attachment_retries += retry_count
 749: 
 750:         if success:
 751:             successful_downloads += 1
 752:             attachment_filenames.append(filename)
 753:             total_file_size_mb += file_size_mb
 754: 
 755:         if attachment_pause > 0 and i < len(attachments):
 756:             logger.debug(f"Pausing {attachment_pause}s before next attachment")
 757:             time.sleep(attachment_pause)
 758: 
 759:     processing_end_time = time.perf_counter()
 760:     processing_duration = processing_end_time - processing_start_time
 761: 
 762:     if shutdown_requested:
 763:         final_status: str = 'interrupted'
 764:     elif successful_downloads == len(attachments):
 765:         final_status = 'complete'
 766:     elif successful_downloads > 0:
 767:         final_status = 'partial'
 768:     else:
 769:         final_status = 'failed'
 770: 
 771:     db_manager.execute_update(
 772:         """
 773:         UPDATE noggin_data
 774:         SET processing_status = %s
 775:         WHERE tip = %s
 776:         """,
 777:         (final_status, tip_value)
 778:     )
 779: 
 780:     logger.info(f"Inspection complete for {noggin_reference}: {successful_downloads}/{len(attachments)} attachments")
 781: 
 782:     attachment_names_str: str = ";".join(attachment_filenames) if attachment_filenames else "FAILED"
 783:     session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{noggin_reference}\t{successful_downloads}\t{attachment_names_str}")
 784: 
 785: def insert_noggin_data_record(tip_value: str, response_data: Dict[str, Any]) -> None:
 786:     """Insert or update noggin_data record with API response"""
 787:     meta: Dict[str, Any] = response_data.get('$meta', {})
 788: 
 789:     noggin_reference: Optional[str] = response_data.get('lcdInspectionId')
 790:     coupling_id: Optional[str] = response_data.get('couplingId')
 791: 
 792:     inspection_date_str: Optional[str] = response_data.get('date')
 793:     inspection_date: Optional[datetime] = None
 794:     if inspection_date_str:
 795:         try:
 796:             inspection_date = datetime.fromisoformat(inspection_date_str.replace('Z', '+00:00'))
 797:         except (ValueError, AttributeError):
 798:             logger.warning(f"Could not parse date: {inspection_date_str}")
 799: 
 800:     vehicle_hash: Optional[str] = response_data.get('vehicle')
 801:     vehicle: Optional[str] = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, noggin_reference) if vehicle_hash else None
 802:     if vehicle_hash:
 803:         vehicle = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, noggin_reference)
 804:         hash_manager.update_lookup_type_if_unknown(vehicle_hash, 'vehicle')
 805: 
 806:     trailer_hash: Optional[str] = response_data.get('trailer')
 807:     trailer: Optional[str] = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, noggin_reference) if trailer_hash else None
 808:     if trailer_hash:
 809:         trailer = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, noggin_reference)
 810:         hash_manager.update_lookup_type_if_unknown(trailer_hash, 'trailer')
 811: 
 812:     trailer2_hash: Optional[str] = response_data.get('trailer2')
 813:     trailer2: Optional[str] = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, noggin_reference) if trailer2_hash else None
 814:     if trailer2_hash:
 815:         trailer2 = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, noggin_reference)
 816:         hash_manager.update_lookup_type_if_unknown(trailer2_hash, 'trailer')
 817:         
 818:     trailer3_hash: Optional[str] = response_data.get('trailer3')
 819:     trailer3: Optional[str] = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, noggin_reference) if trailer3_hash else None
 820:     if trailer3_hash:
 821:         trailer3 = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, noggin_reference)
 822:         hash_manager.update_lookup_type_if_unknown(trailer3_hash, 'trailer')
 823:         
 824:     department_hash: Optional[str] = response_data.get('whichDepartmentDoesTheLoadBelongTo')
 825:     department: Optional[str] = hash_manager.lookup_hash('department', department_hash, tip_value, noggin_reference) if department_hash else None
 826:     if department_hash:
 827:         department = hash_manager.lookup_hash('department', department_hash, tip_value, noggin_reference)
 828:         hash_manager.update_lookup_type_if_unknown(department_hash, 'department')
 829: 
 830:     team_hash: Optional[str] = response_data.get('team')
 831:     team: Optional[str] = hash_manager.lookup_hash('team', team_hash, tip_value, noggin_reference) if team_hash else None
 832:     if team_hash:
 833:         team = hash_manager.lookup_hash('team', team_hash, tip_value, noggin_reference)
 834:         hash_manager.update_lookup_type_if_unknown(team_hash, 'team')
 835: 
 836:     compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 837:     compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 838:     if compliant_yes:
 839:         load_compliance: str = 'COMPLIANT'
 840:     elif compliant_no:
 841:         load_compliance = 'NON-COMPLIANT'
 842:     else:
 843:         load_compliance = 'UNKNOWN'
 844: 
 845:     has_unknown: bool = any([
 846:         vehicle and vehicle.startswith('Unknown'),
 847:         trailer and trailer.startswith('Unknown'),
 848:         trailer2 and trailer2.startswith('Unknown'),
 849:         trailer3 and trailer3.startswith('Unknown'),
 850:         department and department.startswith('Unknown'),
 851:         team and team.startswith('Unknown')
 852:     ])
 853: 
 854:     api_meta_created: Optional[datetime] = None
 855:     api_meta_modified: Optional[datetime] = None
 856:     if meta.get('createdDate'):
 857:         try:
 858:             api_meta_created = datetime.fromisoformat(meta['createdDate'].replace('Z', '+00:00'))
 859:         except (ValueError, AttributeError):
 860:             pass
 861:     if meta.get('modifiedDate'):
 862:         try:
 863:             api_meta_modified = datetime.fromisoformat(meta['modifiedDate'].replace('Z', '+00:00'))
 864:         except (ValueError, AttributeError):
 865:             pass
 866: 
 867:     parent_array: Optional[List[str]] = meta.get('parent')
 868: 
 869:     db_manager.execute_update(
 870:         """
 871:         INSERT INTO noggin_data (
 872:             tip, object_type, inspection_date, noggin_reference, coupling_id,
 873:             inspected_by, vehicle_hash, vehicle, vehicle_id,
 874:             trailer_hash, trailer, trailer_id,
 875:             trailer2_hash, trailer2, trailer2_id,
 876:             trailer3_hash, trailer3, trailer3_id,
 877:             job_number, run_number, driver_loader_name,
 878:             department_hash, department, team_hash, team,
 879:             load_compliance, processing_status, has_unknown_hashes,
 880:             total_attachments, csv_imported_at,
 881:             straps, no_of_straps, chains, mass,
 882:             api_meta_created_date, api_meta_modified_date,
 883:             api_meta_security, api_meta_type, api_meta_tip,
 884:             api_meta_sid, api_meta_branch, api_meta_parent,
 885:             api_meta_errors, api_meta_raw, api_payload_raw, raw_json
 886:         ) VALUES (
 887:             %s, %s, %s, %s, %s,
 888:             %s, %s, %s, %s,
 889:             %s, %s, %s,
 890:             %s, %s, %s,
 891:             %s, %s, %s,
 892:             %s, %s, %s,
 893:             %s, %s, %s, %s,
 894:             %s, %s, %s,
 895:             %s, %s,
 896:             %s, %s, %s, %s,
 897:             %s, %s,
 898:             %s, %s, %s,
 899:             %s, %s, %s,
 900:             %s, %s, %s, %s
 901:         )
 902:         ON CONFLICT (tip) DO UPDATE SET
 903:             object_type = EXCLUDED.object_type,
 904:             inspection_date = EXCLUDED.inspection_date,
 905:             noggin_reference = EXCLUDED.noggin_reference,
 906:             coupling_id = EXCLUDED.coupling_id,
 907:             inspected_by = EXCLUDED.inspected_by,
 908:             vehicle_hash = EXCLUDED.vehicle_hash,
 909:             vehicle = EXCLUDED.vehicle,
 910:             vehicle_id = EXCLUDED.vehicle_id,
 911:             trailer_hash = EXCLUDED.trailer_hash,
 912:             trailer = EXCLUDED.trailer,
 913:             trailer_id = EXCLUDED.trailer_id,
 914:             trailer2_hash = EXCLUDED.trailer2_hash,
 915:             trailer2 = EXCLUDED.trailer2,
 916:             trailer2_id = EXCLUDED.trailer2_id,
 917:             trailer3_hash = EXCLUDED.trailer3_hash,
 918:             trailer3 = EXCLUDED.trailer3,
 919:             trailer3_id = EXCLUDED.trailer3_id,
 920:             job_number = EXCLUDED.job_number,
 921:             run_number = EXCLUDED.run_number,
 922:             driver_loader_name = EXCLUDED.driver_loader_name,
 923:             department_hash = EXCLUDED.department_hash,
 924:             department = EXCLUDED.department,
 925:             team_hash = EXCLUDED.team_hash,
 926:             team = EXCLUDED.team,
 927:             load_compliance = EXCLUDED.load_compliance,
 928:             processing_status = EXCLUDED.processing_status,
 929:             has_unknown_hashes = EXCLUDED.has_unknown_hashes,
 930:             total_attachments = EXCLUDED.total_attachments,
 931:             straps = EXCLUDED.straps,
 932:             no_of_straps = EXCLUDED.no_of_straps,
 933:             chains = EXCLUDED.chains,
 934:             mass = EXCLUDED.mass,
 935:             api_meta_created_date = EXCLUDED.api_meta_created_date,
 936:             api_meta_modified_date = EXCLUDED.api_meta_modified_date,
 937:             api_meta_security = EXCLUDED.api_meta_security,
 938:             api_meta_type = EXCLUDED.api_meta_type,
 939:             api_meta_tip = EXCLUDED.api_meta_tip,
 940:             api_meta_sid = EXCLUDED.api_meta_sid,
 941:             api_meta_branch = EXCLUDED.api_meta_branch,
 942:             api_meta_parent = EXCLUDED.api_meta_parent,
 943:             api_meta_errors = EXCLUDED.api_meta_errors,
 944:             api_meta_raw = EXCLUDED.api_meta_raw,
 945:             api_payload_raw = EXCLUDED.api_payload_raw,
 946:             raw_json = EXCLUDED.raw_json,
 947:             updated_at = CURRENT_TIMESTAMP
 948:         """,
 949:         (
 950:             tip_value, object_type, inspection_date, noggin_reference, coupling_id,
 951:             response_data.get('inspectedBy'), vehicle_hash, vehicle, response_data.get('vehicleId'),
 952:             trailer_hash, trailer, response_data.get('trailerId'),
 953:             trailer2_hash, trailer2, response_data.get('trailerId2'),
 954:             trailer3_hash, trailer3, response_data.get('trailerId3'),
 955:             response_data.get('jobNumber'), response_data.get('runNumber'), response_data.get('driverLoaderName'),
 956:             department_hash, department, team_hash, team,
 957:             load_compliance, 'api_success', has_unknown,
 958:             len(response_data.get('attachments', [])), None,
 959:             response_data.get('straps'), response_data.get('noOfStraps'), 
 960:             response_data.get('chains'), response_data.get('mass'),
 961:             api_meta_created, api_meta_modified,
 962:             meta.get('security'), meta.get('type'), meta.get('tip'),
 963:             meta.get('sid'), meta.get('branch'), parent_array,
 964:             json.dumps(meta.get('errors', [])), json.dumps(meta), json.dumps(response_data),
 965:             json.dumps(response_data)
 966:         )
 967:     )
 968: 
 969:     logger.debug(f"Inserted/updated noggin_data record for TIP {tip_value}")
 970: 
 971: def calculate_next_retry_time(retry_count: int) -> datetime:
 972:     """Calculate next retry time with exponential backoff"""
 973:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
 974:     retry_backoff_multiplier: int = config.getint('retry', 'retry_backoff_multiplier')
 975: 
 976:     if retry_count >= max_retry_attempts:
 977:         return datetime.now() + timedelta(days=365)
 978: 
 979:     backoff_seconds: int = (retry_backoff_multiplier ** retry_count) * 60
 980:     max_backoff_seconds: int = 3600
 981:     backoff_seconds = min(backoff_seconds, max_backoff_seconds)
 982: 
 983:     return datetime.now() + timedelta(seconds=backoff_seconds)
 984: 
 985: 
 986: def get_tips_to_process_from_database(limit: int = 10) -> List[Dict[str, Any]]:
 987:     """
 988:     Query database for TIPs that need processing
 989:     Priority: failed -> interrupted -> partial -> api_failed -> pending -> csv_imported
 990:     """
 991:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
 992: 
 993:     query: str = """
 994:         SELECT tip, processing_status, retry_count, next_retry_at
 995:         FROM noggin_data
 996:         WHERE permanently_failed = FALSE
 997:           AND (
 998:               (processing_status IN ('failed', 'interrupted', 'partial', 'api_failed')
 999:                AND retry_count < %s
1000:                AND (next_retry_at IS NULL OR next_retry_at <= CURRENT_TIMESTAMP))
1001:               OR processing_status IN ('pending', 'csv_imported')
1002:           )
1003:         ORDER BY
1004:             CASE processing_status
1005:                 WHEN 'failed' THEN 1
1006:                 WHEN 'interrupted' THEN 2
1007:                 WHEN 'partial' THEN 3
1008:                 WHEN 'api_failed' THEN 4
1009:                 WHEN 'pending' THEN 5
1010:                 WHEN 'csv_imported' THEN 6
1011:                 ELSE 7
1012:             END,
1013:             csv_imported_at ASC
1014:         LIMIT %s
1015:     """
1016: 
1017:     tips: List[Dict[str, Any]] = db_manager.execute_query_dict(query, (max_retry_attempts, limit))
1018:     return tips
1019: 
1020: 
1021: def mark_permanently_failed(tip_value: str) -> None:
1022:     """Mark TIP as permanently failed after max retries"""
1023:     db_manager.execute_update(
1024:         """
1025:         UPDATE noggin_data
1026:         SET permanently_failed = TRUE,
1027:             processing_status = 'failed',
1028:             last_error_message = 'Max retry attempts exceeded'
1029:         WHERE tip = %s
1030:         """,
1031:         (tip_value,)
1032:     )
1033:     logger.warning(f"TIP {tip_value} marked as permanently failed")
1034: 
1035: def should_process_tip(tip_value: str) -> Tuple[bool, Optional[int]]:
1036:     """
1037:     Check if TIP should be processed based on database state
1038: 
1039:     Returns:
1040:         Tuple of (should_process, current_retry_count)
1041:     """
1042:     result: List[Dict[str, Any]] = db_manager.execute_query_dict(
1043:         """
1044:         SELECT processing_status, all_attachments_complete, retry_count,
1045:                permanently_failed, next_retry_at
1046:         FROM noggin_data
1047:         WHERE tip = %s
1048:         """,
1049:         (tip_value,)
1050:     )
1051: 
1052:     if not result:
1053:         logger.debug(f"TIP {tip_value} not in database - will process")
1054:         return True, 0
1055: 
1056:     record: Dict[str, Any] = result[0]
1057:     status: str = record['processing_status']
1058:     all_complete: bool = record['all_attachments_complete']
1059:     retry_count: int = record['retry_count'] or 0
1060:     permanently_failed: bool = record['permanently_failed']
1061:     next_retry_at: Optional[datetime] = record['next_retry_at']
1062: 
1063:     if permanently_failed:
1064:         logger.info(f"TIP {tip_value} permanently failed - skipping")
1065:         return False, retry_count
1066: 
1067:     if status == 'complete' and all_complete:
1068:         logger.info(f"TIP {tip_value} already completed successfully - skipping")
1069:         return False, retry_count
1070: 
1071:     if next_retry_at and datetime.now() < next_retry_at:
1072:         wait_seconds: float = (next_retry_at - datetime.now()).total_seconds()
1073:         logger.debug(f"TIP {tip_value} in backoff period - retry in {wait_seconds:.0f}s")
1074:         return False, retry_count
1075: 
1076:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1077:     if retry_count >= max_retry_attempts:
1078:         mark_permanently_failed(tip_value)
1079:         return False, retry_count
1080: 
1081:     logger.info(f"TIP {tip_value} needs processing (status: {status}, retry: {retry_count})")
1082:     return True, retry_count
1083: 
1084: 
1085: def update_progress_tracking(processed_count: int, total_count: int, start_time_val: float) -> None:
1086:     """Display progress updates with time estimates"""
1087:     if processed_count == 0:
1088:         return
1089: 
1090:     elapsed_time: float = time.perf_counter() - start_time_val
1091:     tips_per_second: float = processed_count / elapsed_time
1092:     remaining_tips: int = total_count - processed_count
1093: 
1094:     if tips_per_second > 0:
1095:         estimated_remaining_seconds: float = remaining_tips / tips_per_second
1096: 
1097:         if estimated_remaining_seconds >= 3600:
1098:             time_estimate: str = f"{estimated_remaining_seconds/3600:.1f} hours"
1099:         elif estimated_remaining_seconds >= 60:
1100:             time_estimate = f"{estimated_remaining_seconds/60:.1f} minutes"
1101:         else:
1102:             time_estimate = f"{estimated_remaining_seconds:.1f} seconds"
1103: 
1104:         progress_percentage: float = (processed_count / total_count) * 100
1105: 
1106:         logger.info(f"Progress: {processed_count}/{total_count} ({progress_percentage:.1f}%) - "
1107:                    f"Rate: {tips_per_second:.2f} TIPs/sec - ETA: {time_estimate}")
1108: 
1109: 
1110: def log_shutdown_summary(processed_count: int, total_count: int, start_time_val: float, reason: str = "manual") -> None:
1111:     """Log comprehensive shutdown summary"""
1112:     elapsed_time: float = time.perf_counter() - start_time_val
1113:     completion_percentage: float = (processed_count / total_count) * 100 if total_count > 0 else 0
1114: 
1115:     logger.info("="*80)
1116:     logger.info("SHUTDOWN SUMMARY")
1117:     logger.info("="*80)
1118:     logger.info(f"Shutdown reason: {reason}")
1119:     logger.info(f"TIPs processed:  {processed_count:,} of {total_count:,} ({completion_percentage:.1f}%)")
1120:     logger.info(f"Processing time: {elapsed_time/3600:.1f} hours")
1121:     if processed_count > 0:
1122:         logger.info(f"Average rate: {processed_count/elapsed_time:.2f} TIPs/second")
1123:         remaining_estimate: float = (total_count - processed_count) * (elapsed_time / processed_count)
1124:         logger.info(f"Estimated time for remaining: {remaining_estimate/3600:.1f} hours")
1125:     logger.info("All work saved to PostgreSQL database")
1126:     logger.info("="*80)
1127: 
1128: 
1129: def main() -> int:
1130:     """Main processing function - DB Driven"""
1131:     global current_tip_being_processed
1132: 
1133:     # Configuration for batch size
1134:     batch_limit: int = 100
1135:     
1136:     logger.info("Fetching TIPs to process from database...")
1137:     tips_to_process: List[Dict[str, Any]] = get_tips_to_process_from_database(limit=batch_limit)
1138:     
1139:     total_tip_count: int = len(tips_to_process)
1140:     
1141:     if total_tip_count == 0:
1142:         logger.info("No eligible TIPs found in database (pending or due for retry).")
1143:         return 0
1144: 
1145:     logger.info(f"Found {total_tip_count} TIPs to process")
1146: 
1147:     processed_count: int = 0
1148:     main_start_time: float = time.perf_counter()
1149: 
1150:     for i, tip_record in enumerate(tips_to_process, start=1):
1151:         if not shutdown_handler.should_continue_processing():
1152:             logger.warning(f"Graceful shutdown after processing {processed_count} TIPs")
1153:             break
1154: 
1155:         tip_value: str = tip_record['tip'].strip()
1156:         
1157:         # We can use the retry count directly from the DB record since we just queried it
1158:         current_retry_count: int = tip_record.get('retry_count', 0)
1159: 
1160:         current_tip_being_processed = tip_value
1161:         processed_count += 1
1162: 
1163:         if processed_count % 10 == 0:
1164:             update_progress_tracking(processed_count, total_tip_count, main_start_time)
1165: 
1166:         try:
1167:             circuit_breaker.before_request()
1168:         except CircuitBreakerError as e:
1169:             logger.warning(f"Circuit breaker blocked request for TIP {tip_value}: {e}")
1170:             # If circuit breaker is open, we probably shouldn't hammer it with the rest of the batch
1171:             time.sleep(10)
1172:             continue
1173: 
1174:         endpoint: str = endpoint_template.replace('$tip', tip_value)
1175:         url: str = base_url + endpoint
1176:         logger.info(f"Processing TIP {processed_count}/{total_tip_count}: {tip_value} (retry: {current_retry_count})")
1177:         logger.debug(f"Request URL: {url}")
1178: 
1179:         current_headers = headers.copy()
1180:         current_headers['Accept'] = 'application/json'
1181:         current_headers['Content-Type'] = 'application/json'
1182:         current_headers['User-Agent'] = 'NogginLCDProcessor/1.0 (Internal Integration)'
1183:         
1184:         logger.debug(f"DEBUG HEADERS SENT: {json.dumps(current_headers, default=str)}")
1185: 
1186:         try:
1187:             api_start_time: float = time.perf_counter()
1188:             response: requests.Response = requests.get(url, headers=headers, timeout=api_timeout)
1189:             circuit_breaker.record_success()
1190:             api_retry_count: int = 0
1191: 
1192:             if response.status_code == 429:
1193:                 circuit_breaker.record_failure()
1194:                 logger.warning(f"Rate limited for TIP {tip_value}. Sleeping {too_many_requests_sleep_time}s")
1195:                 time.sleep(too_many_requests_sleep_time)
1196:                 try:
1197:                     circuit_breaker.before_request()
1198:                     response = requests.get(url, headers=headers, timeout=api_timeout)
1199:                     circuit_breaker.record_success()
1200:                     api_retry_count = 1
1201:                 except CircuitBreakerError as cb_error:
1202:                     logger.warning(f"Circuit breaker blocked retry: {cb_error}")
1203:                     
1204:                     new_retry_count: int = current_retry_count + 1
1205:                     next_retry: datetime = calculate_next_retry_time(new_retry_count)
1206:                     
1207:                     db_manager.execute_update(
1208:                         """
1209:                         INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1210:                                                 retry_count, last_retry_at, next_retry_at)
1211:                         VALUES (%s, %s, %s, %s, %s, %s, %s)
1212:                         ON CONFLICT (tip) DO UPDATE SET
1213:                             processing_status = EXCLUDED.processing_status,
1214:                             last_error_message = EXCLUDED.last_error_message,
1215:                             retry_count = EXCLUDED.retry_count,
1216:                             last_retry_at = EXCLUDED.last_retry_at,
1217:                             next_retry_at = EXCLUDED.next_retry_at
1218:                         """,
1219:                         (tip_value, object_type, 'api_failed', str(cb_error), 
1220:                          new_retry_count, datetime.now(), next_retry)
1221:                     )
1222:                     continue
1223:                 except requests.exceptions.RequestException as retry_error:
1224:                     circuit_breaker.record_failure()
1225:                     logger.error(f"Retry failed for TIP {tip_value}: {retry_error}", exc_info=True)
1226: 
1227:                     new_retry_count = current_retry_count + 1
1228:                     next_retry = calculate_next_retry_time(new_retry_count)
1229: 
1230:                     db_manager.execute_update(
1231:                         """
1232:                         INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1233:                                                 retry_count, last_retry_at, next_retry_at)
1234:                         VALUES (%s, %s, %s, %s, %s, %s, %s)
1235:                         ON CONFLICT (tip) DO UPDATE SET
1236:                             processing_status = EXCLUDED.processing_status,
1237:                             last_error_message = EXCLUDED.last_error_message,
1238:                             retry_count = EXCLUDED.retry_count,
1239:                             last_retry_at = EXCLUDED.last_retry_at,
1240:                             next_retry_at = EXCLUDED.next_retry_at
1241:                         """,
1242:                         (tip_value, object_type, 'api_failed', str(retry_error), 
1243:                          new_retry_count, datetime.now(), next_retry)
1244:                     )
1245: 
1246:                     db_manager.execute_update(
1247:                         """
1248:                         INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1249:                         VALUES (%s, %s, %s, %s)
1250:                         """,
1251:                         (tip_value, 'api_failed', str(retry_error), json.dumps({'url': url}))
1252:                     )
1253:                     continue
1254: 
1255:             if response.status_code == 200:
1256:                 logger.info(f"Successful API response for TIP {tip_value}")
1257:                 response_data: Dict[str, Any] = response.json()
1258: 
1259:                 insert_noggin_data_record(tip_value, response_data)
1260: 
1261:                 noggin_reference: str = response_data.get('lcdInspectionId', 'unknown')
1262: 
1263:                 process_attachments(response_data, noggin_reference, tip_value)
1264: 
1265:             else:
1266:                 circuit_breaker.record_failure()
1267:                 error_details: str = handle_api_error(response, tip_value, url)
1268:                 logger.error(error_details)
1269:                 session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\tAPI_ERROR_{response.status_code}\t0\tERROR")
1270: 
1271:                 new_retry_count = current_retry_count + 1
1272:                 next_retry = calculate_next_retry_time(new_retry_count)
1273: 
1274:                 db_manager.execute_update(
1275:                     """
1276:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1277:                                             retry_count, last_retry_at, next_retry_at)
1278:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1279:                     ON CONFLICT (tip) DO UPDATE SET
1280:                         processing_status = EXCLUDED.processing_status,
1281:                         last_error_message = EXCLUDED.last_error_message,
1282:                         retry_count = EXCLUDED.retry_count,
1283:                         last_retry_at = EXCLUDED.last_retry_at,
1284:                         next_retry_at = EXCLUDED.next_retry_at
1285:                     """,
1286:                     (tip_value, object_type, 'api_failed', error_details, 
1287:                      new_retry_count, datetime.now(), next_retry)
1288:                 )
1289: 
1290:                 db_manager.execute_update(
1291:                     """
1292:                     INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1293:                     VALUES (%s, %s, %s, %s)
1294:                     """,
1295:                     (tip_value, 'api_failed', error_details, json.dumps({
1296:                         'http_status': response.status_code,
1297:                         'url': url
1298:                     }))
1299:                 )
1300: 
1301:         except requests.exceptions.ConnectionError as connection_error:
1302:             circuit_breaker.record_failure()
1303:             logger.error(f"Connection error for TIP {tip_value}: {connection_error}", exc_info=True)
1304: 
1305:             new_retry_count = current_retry_count + 1
1306:             next_retry = calculate_next_retry_time(new_retry_count)
1307: 
1308:             db_manager.execute_update(
1309:                 """
1310:                 INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1311:                                         retry_count, last_retry_at, next_retry_at)
1312:                 VALUES (%s, %s, %s, %s, %s, %s, %s)
1313:                 ON CONFLICT (tip) DO UPDATE SET
1314:                     processing_status = EXCLUDED.processing_status,
1315:                     last_error_message = EXCLUDED.last_error_message,
1316:                     retry_count = EXCLUDED.retry_count,
1317:                     last_retry_at = EXCLUDED.last_retry_at,
1318:                     next_retry_at = EXCLUDED.next_retry_at
1319:                 """,
1320:                 (tip_value, object_type, 'api_failed', str(connection_error), 
1321:                  new_retry_count, datetime.now(), next_retry)
1322:             )
1323:             continue
1324: 
1325:         except requests.exceptions.RequestException as request_error:
1326:             circuit_breaker.record_failure()
1327:             logger.error(f"Request error for TIP {tip_value}: {request_error}", exc_info=True)
1328: 
1329:             new_retry_count = current_retry_count + 1
1330:             next_retry = calculate_next_retry_time(new_retry_count)
1331: 
1332:             db_manager.execute_update(
1333:                 """
1334:                 INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1335:                                         retry_count, last_retry_at, next_retry_at)
1336:                 VALUES (%s, %s, %s, %s, %s, %s, %s)
1337:                 ON CONFLICT (tip) DO UPDATE SET
1338:                     processing_status = EXCLUDED.processing_status,
1339:                     last_error_message = EXCLUDED.last_error_message,
1340:                     retry_count = EXCLUDED.retry_count,
1341:                     last_retry_at = EXCLUDED.last_retry_at,
1342:                     next_retry_at = EXCLUDED.next_retry_at
1343:                 """,
1344:                 (tip_value, object_type, 'api_failed', str(request_error), 
1345:                  new_retry_count, datetime.now(), next_retry)
1346:             )
1347:             continue
1348: 
1349:         except Exception as e:
1350:             circuit_breaker.record_failure()
1351:             logger.error(f"Unexpected error processing TIP {tip_value}: {e}", exc_info=True)
1352: 
1353:             new_retry_count = current_retry_count + 1
1354:             next_retry = calculate_next_retry_time(new_retry_count)
1355: 
1356:             db_manager.execute_update(
1357:                 """
1358:                 INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message, 
1359:                                         retry_count, last_retry_at, next_retry_at)
1360:                 VALUES (%s, %s, %s, %s, %s, %s, %s)
1361:                 ON CONFLICT (tip) DO UPDATE SET
1362:                     processing_status = EXCLUDED.processing_status,
1363:                     last_error_message = EXCLUDED.last_error_message,
1364:                     retry_count = EXCLUDED.retry_count,
1365:                     last_retry_at = EXCLUDED.last_retry_at,
1366:                     next_retry_at = EXCLUDED.next_retry_at
1367:                 """,
1368:                 (tip_value, object_type, 'failed', str(e), 
1369:                  new_retry_count, datetime.now(), next_retry)
1370:             )
1371:             continue
1372: 
1373:         current_tip_being_processed = None
1374: 
1375:     return processed_count
1376: 
1377: if __name__ == "__main__":
1378:     try:
1379:         logger.info("Starting main processing loop")
1380:         processed_count: int = main()
1381: 
1382:         logger.info(f"Processing completed. Total TIPs processed: {processed_count}")
1383:         logger.info("Script completed successfully")
1384: 
1385:     except KeyboardInterrupt:
1386:         logger.warning("Processing interrupted by user")
1387:         log_shutdown_summary(0, 0, start_time, "keyboard_interrupt")
1388: 
1389:     except Exception as e:
1390:         logger.error(f"Unexpected error: {e}", exc_info=True)
1391:         log_shutdown_summary(0, 0, start_time, "error")
1392:         raise
1393: 
1394:     finally:
1395:         if 'db_manager' in locals():
1396:             db_manager.close_all()
1397: 
1398:         end_time: float = time.perf_counter()
1399:         total_duration: float = end_time - start_time
1400:         logger.info(f"Total execution time: {total_duration/3600:.2f} hours ({total_duration:.2f} seconds)")
1401:         logger.info(f"Session ID: {batch_session_id}")
1402:         logger.info("="*80)
1403: 
1404:         session_logger.info(f"\nSESSION END: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
1405:         session_logger.info(f"TOTAL EXECUTION TIME: {total_duration:.2f} seconds")
</file>

<file path="processors/__init__.py">
 1: """
 2: Processors Package
 3: 
 4: Provides modular, config-driven processing for all Noggin object types.
 5: 
 6: Main components:
 7: - ObjectProcessor: Main processing orchestrator
 8: - FieldProcessor: Config-driven field extraction
 9: - ReportGenerator: Template-based report generation
10: - Base utilities: API client, attachment downloader, etc.
11: 
12: Usage:
13:     from processors import ObjectProcessor
14:     
15:     processor = ObjectProcessor(
16:         base_config_path='config/base_config.ini',
17:         specific_config_path='config/coupling_compliance_check_config.ini'
18:     )
19:     processor.run()
20: """
21: 
22: from .object_processor import ObjectProcessor, create_processor
23: from .field_processor import FieldProcessor, DatabaseRecordManager
24: from .report_generator import ReportGenerator, DefaultReportGenerator, create_report_generator
25: from .base_processor import (
26:     GracefulShutdownHandler,
27:     APIClient,
28:     AttachmentDownloader,
29:     FolderManager,
30:     RetryManager,
31:     ProgressTracker,
32:     sanitise_filename,
33:     flatten_json,
34:     calculate_md5_hash,
35:     validate_attachment_file
36: )
37: 
38: __all__ = [
39:     # Main processor
40:     'ObjectProcessor',
41:     'create_processor',
42:     
43:     # Field processing
44:     'FieldProcessor',
45:     'DatabaseRecordManager',
46:     
47:     # Report generation
48:     'ReportGenerator',
49:     'DefaultReportGenerator',
50:     'create_report_generator',
51:     
52:     # Base utilities
53:     'GracefulShutdownHandler',
54:     'APIClient',
55:     'AttachmentDownloader',
56:     'FolderManager',
57:     'RetryManager',
58:     'ProgressTracker',
59:     
60:     # Helper functions
61:     'sanitise_filename',
62:     'flatten_json',
63:     'calculate_md5_hash',
64:     'validate_attachment_file',
65: ]
</file>

<file path="processors/base_processor.py">
  1: """
  2: Base Processor Module
  3: 
  4: Contains common functionality shared by all object type processors:
  5: - API request handling with retry/backoff
  6: - Attachment downloading and validation
  7: - Graceful shutdown handling
  8: - Progress tracking
  9: - Folder structure creation
 10: 
 11: Object-type-specific logic is handled by:
 12: - Field mappings from config (see field_processor.py)
 13: - Report templates from config (see report_generator.py)
 14: """
 15: 
 16: from __future__ import annotations
 17: import requests
 18: import json
 19: import logging
 20: import uuid
 21: import hashlib
 22: import time
 23: import signal
 24: import atexit
 25: import re
 26: from datetime import datetime, timedelta
 27: from pathlib import Path
 28: from typing import Optional, Dict, Any, Tuple, List, Callable
 29: 
 30: logger: logging.Logger = logging.getLogger(__name__)
 31: 
 32: 
 33: class GracefulShutdownHandler:
 34:     """Handles Ctrl+C and system shutdown signals"""
 35: 
 36:     def __init__(self, db_manager: 'DatabaseConnectionManager', 
 37:                  logger_instance: logging.Logger,
 38:                  on_shutdown: Optional[Callable] = None) -> None:
 39:         self.db_manager: 'DatabaseConnectionManager' = db_manager
 40:         self.logger: logging.Logger = logger_instance
 41:         self.shutdown_requested: bool = False
 42:         self.force_exit: bool = False
 43:         self.current_tip: Optional[str] = None
 44:         self.on_shutdown: Optional[Callable] = on_shutdown
 45: 
 46:         signal.signal(signal.SIGINT, self._signal_handler)
 47:         signal.signal(signal.SIGTERM, self._signal_handler)
 48:         atexit.register(self._cleanup_on_exit)
 49: 
 50:         self.logger.info("Graceful shutdown handler initialised")
 51: 
 52:     def _signal_handler(self, signum: int, frame: Any) -> None:
 53:         signal_name: str = "SIGINT (Ctrl+C)" if signum == signal.SIGINT else f"Signal {signum}"
 54: 
 55:         if not self.shutdown_requested:
 56:             self.shutdown_requested = True
 57:             self.logger.warning(f"{signal_name} received. Finishing current TIP then shutting down...")
 58:             self.logger.warning(f"Currently processing: {self.current_tip or 'None'}")
 59:             self.logger.warning("Press Ctrl+C again to force immediate exit")
 60:         else:
 61:             self.logger.error("Second shutdown signal - forcing immediate exit")
 62:             self.force_exit = True
 63:             self._emergency_cleanup()
 64: 
 65:     def _emergency_cleanup(self) -> None:
 66:         self.logger.warning("Emergency cleanup initiated")
 67:         try:
 68:             if self.db_manager:
 69:                 self.db_manager.close_all()
 70:         except Exception as e:
 71:             self.logger.error(f"Error during emergency cleanup: {e}")
 72:         
 73:         import sys
 74:         sys.exit(1)
 75: 
 76:     def _cleanup_on_exit(self) -> None:
 77:         self.logger.info("Normal exit cleanup")
 78:         if self.on_shutdown:
 79:             try:
 80:                 self.on_shutdown()
 81:             except Exception as e:
 82:                 self.logger.error(f"Error in shutdown callback: {e}")
 83:         
 84:         try:
 85:             if self.db_manager:
 86:                 self.db_manager.close_all()
 87:         except Exception as e:
 88:             self.logger.error(f"Error closing database connections: {e}")
 89: 
 90:     def should_continue(self) -> bool:
 91:         return not self.shutdown_requested
 92: 
 93:     def set_current_tip(self, tip: Optional[str]) -> None:
 94:         self.current_tip = tip
 95: 
 96: 
 97: def sanitise_filename(text: str) -> str:
 98:     """Sanitise string for use in filenames"""
 99:     if not text:
100:         return "unknown"
101:     
102:     sanitised: str = re.sub(r'[<>:"/\\|?*]', '_', str(text))
103:     sanitised = re.sub(r'\s+', '_', sanitised)
104:     sanitised = re.sub(r'_+', '_', sanitised)
105:     sanitised = sanitised.strip('_')
106:     
107:     return sanitised[:100] if sanitised else "unknown"
108: 
109: 
110: def flatten_json(nested_json: Any, parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
111:     """Flatten nested JSON into single-level dict with concatenated keys"""
112:     items: List[Tuple[str, Any]] = []
113:     
114:     if isinstance(nested_json, dict):
115:         for k, v in nested_json.items():
116:             new_key: str = f"{parent_key}{sep}{k}" if parent_key else k
117:             items.extend(flatten_json(v, new_key, sep).items())
118:     elif isinstance(nested_json, list):
119:         for i, v in enumerate(nested_json):
120:             new_key = f"{parent_key}{sep}{i}" if parent_key else str(i)
121:             items.extend(flatten_json(v, new_key, sep).items())
122:     else:
123:         items.append((parent_key, nested_json))
124:     
125:     return dict(items)
126: 
127: 
128: def calculate_md5_hash(file_path: Path) -> str:
129:     """Calculate MD5 hash of file"""
130:     try:
131:         hash_md5 = hashlib.md5()
132:         with open(file_path, 'rb') as f:
133:             for chunk in iter(lambda: f.read(8192), b''):
134:                 hash_md5.update(chunk)
135:         return hash_md5.hexdigest()
136:     except Exception as e:
137:         logger.error(f"Error calculating MD5 for {file_path}: {e}")
138:         return ""
139: 
140: 
141: def validate_attachment_file(file_path: Path, expected_min_size: int = 1024) -> Tuple[bool, int, Optional[str]]:
142:     """
143:     Validate downloaded attachment file
144:     
145:     Returns:
146:         Tuple of (is_valid, file_size_bytes, error_message)
147:     """
148:     try:
149:         if not file_path.exists():
150:             return False, 0, "File does not exist"
151: 
152:         file_size: int = file_path.stat().st_size
153: 
154:         if file_size < expected_min_size:
155:             return False, file_size, f"File too small ({file_size} bytes)"
156: 
157:         with open(file_path, 'rb') as f:
158:             header_bytes: bytes = f.read(10)
159:             if len(header_bytes) == 0:
160:                 return False, file_size, "File appears empty"
161: 
162:         return True, file_size, None
163: 
164:     except Exception as e:
165:         return False, 0, f"Validation error: {e}"
166: 
167: 
168: class APIClient:
169:     """Handles API requests with retry logic and circuit breaker integration"""
170:     
171:     def __init__(self, config: 'ConfigLoader', circuit_breaker: 'CircuitBreaker') -> None:
172:         self.config = config
173:         self.circuit_breaker = circuit_breaker
174:         
175:         self.base_url: str = config.get('api', 'base_url')
176:         self.attachment_base_url: str = config.get('api', 'media_service_url')
177:         self.headers: Dict[str, str] = config.get_api_headers()
178:         
179:         self.max_retries: int = config.getint('processing', 'max_api_retries')
180:         self.backoff_factor: int = config.getint('processing', 'api_backoff_factor')
181:         self.max_backoff: int = config.getint('processing', 'api_max_backoff')
182:         self.timeout: int = config.getint('processing', 'api_timeout')
183:         self.too_many_requests_sleep: int = config.getint('processing', 'too_many_requests_sleep_time')
184:     
185:     def make_request(self, url: str, tip_value: str) -> requests.Response:
186:         """Make API request with exponential backoff retry logic"""
187:         last_exception: Optional[Exception] = None
188: 
189:         for attempt in range(self.max_retries):
190:             try:
191:                 logger.debug(f"API request attempt {attempt + 1}/{self.max_retries} for TIP {tip_value}")
192:                 response: requests.Response = requests.get(url, headers=self.headers, timeout=self.timeout)
193:                 response._retry_count = attempt
194:                 logger.debug(f"Request attempt {attempt + 1} succeeded for TIP {tip_value}")
195:                 return response
196: 
197:             except requests.exceptions.ConnectionError as e:
198:                 last_exception = e
199:                 if attempt == self.max_retries - 1:
200:                     logger.error(f"All {self.max_retries} connection attempts failed for TIP {tip_value}")
201:                     raise
202:                 wait_time: float = min((self.backoff_factor ** attempt) * self.backoff_factor, self.max_backoff)
203:                 logger.warning(f"Connection failed for TIP {tip_value}, retrying in {wait_time}s (attempt {attempt + 1}/{self.max_retries})")
204:                 time.sleep(wait_time)
205: 
206:             except requests.exceptions.Timeout as e:
207:                 last_exception = e
208:                 if attempt == self.max_retries - 1:
209:                     logger.error(f"All {self.max_retries} timeout attempts failed for TIP {tip_value}")
210:                     raise
211:                 wait_time = min((self.backoff_factor ** attempt) * self.backoff_factor, self.max_backoff)
212:                 logger.warning(f"Request timeout for TIP {tip_value}, retrying in {wait_time}s (attempt {attempt + 1}/{self.max_retries})")
213:                 time.sleep(wait_time)
214: 
215:             except requests.exceptions.RequestException as e:
216:                 last_exception = e
217:                 if attempt == self.max_retries - 1:
218:                     logger.error(f"Request failed permanently for TIP {tip_value}: {e}")
219:                     raise
220:                 wait_time = self.backoff_factor
221:                 logger.warning(f"Request error for TIP {tip_value}, retrying in {wait_time}s (attempt {attempt + 1}/{self.max_retries})")
222:                 time.sleep(wait_time)
223: 
224:         if last_exception:
225:             raise last_exception
226:         raise Exception(f"Unexpected error in retry logic for TIP {tip_value}")
227:     
228:     def handle_error(self, response: requests.Response, tip_value: str, request_url: str) -> str:
229:         """Generate detailed error message from API response"""
230:         status_code: int = response.status_code
231: 
232:         try:
233:             response_text: str = response.text
234:             if response_text:
235:                 try:
236:                     error_json: Dict[str, Any] = response.json()
237:                     additional_info: str = f" Response body: {json.dumps(error_json, indent=2)}"
238:                 except json.JSONDecodeError:
239:                     additional_info = f" Response body: {response_text[:500]}{'...' if len(response_text) > 500 else ''}"
240:             else:
241:                 additional_info = " (No response body provided)"
242:         except Exception:
243:             additional_info = " (Could not read response body)"
244: 
245:         error_messages = {
246:             401: f"Authentication failed for TIP {tip_value}. Status code: {status_code} (Unauthorised). URL: {request_url}{additional_info}",
247:             403: f"Access forbidden for TIP {tip_value}. Status code: {status_code} (Forbidden). URL: {request_url}{additional_info}",
248:             404: f"Resource not found for TIP {tip_value}. Status code: {status_code} (Not Found). URL: {request_url}{additional_info}",
249:             429: f"Rate limit exceeded for TIP {tip_value}. Status code: {status_code} (Too Many Requests). URL: {request_url}{additional_info}",
250:         }
251: 
252:         if status_code in error_messages:
253:             return error_messages[status_code]
254:         elif 400 <= status_code < 500:
255:             return f"Client error for TIP {tip_value}. Status code: {status_code}. URL: {request_url}{additional_info}"
256:         elif 500 <= status_code < 600:
257:             return f"Server error for TIP {tip_value}. Status code: {status_code}. URL: {request_url}{additional_info}"
258:         else:
259:             return f"Unexpected response for TIP {tip_value}. Status code: {status_code}. URL: {request_url}{additional_info}"
260: 
261: 
262: class AttachmentDownloader:
263:     """Handles attachment downloading with validation and database tracking"""
264:     
265:     def __init__(self, config: 'ConfigLoader', db_manager: 'DatabaseConnectionManager',
266:                  api_client: APIClient) -> None:
267:         self.config = config
268:         self.db_manager = db_manager
269:         self.api_client = api_client
270:         
271:         self.attachment_pause: int = config.getint('processing', 'attachment_pause')
272:         self.min_file_size: int = config.getint('processing', 'min_attachment_size', fallback=1024)
273:     
274:     def download(self, attachment_url: str, filename: str, inspection_id: str,
275:                 attachment_tip: str, inspection_folder: Path,
276:                 record_tip: str, attachment_sequence: int) -> Tuple[bool, int, float, Optional[str]]:
277:         """
278:         Download and validate attachment with database tracking
279:         
280:         Returns:
281:             Tuple of (success, retry_count, file_size_mb, error_message)
282:         """
283:         if attachment_url.startswith('/media'):
284:             attachment_url = attachment_url[6:]
285: 
286:         full_url: str = self.api_client.attachment_base_url + attachment_url
287:         output_path: Path = inspection_folder / filename
288:         temp_path: Path = output_path.with_suffix('.tmp')
289: 
290:         download_start: datetime = datetime.now()
291: 
292:         # Insert initial attachment record
293:         try:
294:             self.db_manager.execute_update(
295:                 """
296:                 INSERT INTO attachments (record_tip, attachment_tip, attachment_sequence, filename, 
297:                                         file_path, attachment_status, download_started_at)
298:                 VALUES (%s, %s, %s, %s, %s, %s, %s)
299:                 ON CONFLICT (record_tip, attachment_tip) DO UPDATE SET
300:                     attachment_status = 'downloading',
301:                     download_started_at = EXCLUDED.download_started_at,
302:                     filename = EXCLUDED.filename,
303:                     file_path = EXCLUDED.file_path
304:                 """,
305:                 (record_tip, attachment_tip, attachment_sequence, filename, 
306:                  str(output_path), 'downloading', download_start)
307:             )
308:         except Exception as e:
309:             logger.warning(f"Could not insert attachment record: {e}")
310: 
311:         try:
312:             response = self.api_client.make_request(full_url, attachment_tip)
313:             retry_count: int = getattr(response, '_retry_count', 0)
314: 
315:             if response.status_code != 200:
316:                 error_msg = self.api_client.handle_error(response, attachment_tip, full_url)
317:                 self._update_attachment_failed(record_tip, attachment_tip, error_msg)
318:                 return False, retry_count, 0, error_msg
319: 
320:             # Write to temp file then rename
321:             with open(temp_path, 'wb') as f:
322:                 f.write(response.content)
323: 
324:             # Validate
325:             is_valid, file_size, validation_error = validate_attachment_file(temp_path, self.min_file_size)
326: 
327:             if not is_valid:
328:                 temp_path.unlink(missing_ok=True)
329:                 error_msg = f"Validation failed: {validation_error}"
330:                 self._update_attachment_failed(record_tip, attachment_tip, error_msg)
331:                 return False, retry_count, 0, error_msg
332: 
333:             # Rename temp to final
334:             temp_path.rename(output_path)
335: 
336:             # Calculate hash
337:             file_hash: str = calculate_md5_hash(output_path)
338: 
339:             download_end: datetime = datetime.now()
340:             download_duration: float = (download_end - download_start).total_seconds()
341: 
342:             # Update successful
343:             self.db_manager.execute_update(
344:                 """
345:                 UPDATE attachments SET
346:                     attachment_status = 'complete',
347:                     attachment_validation_status = 'valid',
348:                     file_size_bytes = %s,
349:                     file_hash_md5 = %s,
350:                     download_completed_at = %s,
351:                     download_duration_seconds = %s
352:                 WHERE record_tip = %s AND attachment_tip = %s
353:                 """,
354:                 (file_size, file_hash, download_end, download_duration, record_tip, attachment_tip)
355:             )
356: 
357:             file_size_mb: float = file_size / (1024 * 1024)
358:             logger.info(f"Downloaded attachment {attachment_sequence}: {filename} ({file_size_mb:.2f} MB)")
359: 
360:             return True, retry_count, file_size_mb, None
361: 
362:         except Exception as e:
363:             error_msg = f"Download exception: {e}"
364:             logger.error(f"Attachment download failed for {inspection_id}: {error_msg}")
365:             temp_path.unlink(missing_ok=True)
366:             self._update_attachment_failed(record_tip, attachment_tip, error_msg)
367:             return False, 0, 0, error_msg
368:     
369:     def _update_attachment_failed(self, record_tip: str, attachment_tip: str, error_msg: str) -> None:
370:         """Update attachment record as failed"""
371:         try:
372:             self.db_manager.execute_update(
373:                 """
374:                 UPDATE attachments SET
375:                     attachment_status = 'failed',
376:                     attachment_validation_status = 'validation_failed',
377:                     last_error_message = %s
378:                 WHERE record_tip = %s AND attachment_tip = %s
379:                 """,
380:                 (error_msg, record_tip, attachment_tip)
381:             )
382:         except Exception as e:
383:             logger.warning(f"Could not update attachment failure: {e}")
384: 
385: 
386: class FolderManager:
387:     """Manages folder structure creation for inspections"""
388:     
389:     def __init__(self, config: 'ConfigLoader', object_type_abbrev: str) -> None:
390:         self.config = config
391:         self.object_type_abbrev = object_type_abbrev
392:         self.base_path: Path = Path(config.get('paths', 'base_output_path'))
393:         self.base_path.mkdir(parents=True, exist_ok=True)
394:         
395:         # Get folder pattern from config or use default
396:         output_patterns = config.get_output_patterns()
397:         self.folder_pattern: str = output_patterns.get('folder_pattern', 
398:             '{abbreviation}/{year}/{month}/{date} {inspection_id}')
399:         self.attachment_pattern: str = output_patterns.get('attachment_pattern',
400:             '{abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg')
401:         self.filename_stub: str = config.get('output', 'filename_image_stub', 
402:                                              fallback='photo', from_specific=True)
403:     
404:     def create_inspection_folder(self, date_str: str, inspection_id: str) -> Path:
405:         """Create folder structure for an inspection"""
406:         sanitised_id: str = sanitise_filename(inspection_id)
407:         
408:         try:
409:             if date_str:
410:                 # Parse date
411:                 date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
412:                 year: str = date_obj.strftime('%Y')
413:                 month: str = date_obj.strftime('%m')
414:                 date_formatted: str = date_obj.strftime('%Y-%m-%d')
415:             else:
416:                 raise ValueError("Empty date")
417:         except (ValueError, AttributeError):
418:             year = 'unknown_year'
419:             month = 'unknown_month'
420:             date_formatted = 'unknown_date'
421:         
422:         # Apply folder pattern
423:         folder_name: str = self.folder_pattern.format(
424:             abbreviation=self.object_type_abbrev,
425:             year=year,
426:             month=month,
427:             date=date_formatted,
428:             inspection_id=sanitised_id
429:         )
430:         
431:         inspection_folder: Path = self.base_path / folder_name
432:         inspection_folder.mkdir(parents=True, exist_ok=True)
433:         
434:         return inspection_folder
435:     
436:     def construct_attachment_filename(self, inspection_id: str, date_str: str, 
437:                                       sequence: int) -> str:
438:         """Construct standardised attachment filename"""
439:         sanitised_id: str = sanitise_filename(inspection_id)
440:         
441:         try:
442:             if date_str:
443:                 date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
444:                 date_formatted: str = date_obj.strftime('%Y%m%d')
445:             else:
446:                 raise ValueError("Empty date")
447:         except (ValueError, AttributeError):
448:             date_formatted = 'unknown'
449:         
450:         filename: str = self.attachment_pattern.format(
451:             abbreviation=self.object_type_abbrev,
452:             inspection_id=sanitised_id,
453:             date=date_formatted,
454:             stub=self.filename_stub,
455:             sequence=str(sequence).zfill(3)
456:         )
457:         
458:         return filename
459: 
460: 
461: class RetryManager:
462:     """Manages retry logic and backoff calculations"""
463:     
464:     def __init__(self, config: 'ConfigLoader') -> None:
465:         self.max_retries: int = config.getint('retry', 'max_retry_attempts')
466:         self.backoff_multiplier: float = config.getfloat('retry', 'retry_backoff_multiplier')
467:         self.base_delay_minutes: int = config.getint('retry', 'base_retry_delay_minutes', fallback=5)
468:         self.max_delay_hours: int = config.getint('retry', 'max_retry_delay_hours', fallback=24)
469:     
470:     def calculate_next_retry_time(self, retry_count: int) -> datetime:
471:         """Calculate next retry time with exponential backoff"""
472:         if retry_count >= self.max_retries:
473:             # Far future = permanently failed
474:             return datetime.now() + timedelta(days=365 * 10)
475:         
476:         delay_minutes: float = self.base_delay_minutes * (self.backoff_multiplier ** retry_count)
477:         max_delay_minutes: float = self.max_delay_hours * 60
478:         delay_minutes = min(delay_minutes, max_delay_minutes)
479:         
480:         return datetime.now() + timedelta(minutes=delay_minutes)
481:     
482:     def should_retry(self, retry_count: int) -> bool:
483:         """Check if more retries are allowed"""
484:         return retry_count < self.max_retries
485: 
486: 
487: class ProgressTracker:
488:     """Tracks processing progress and estimates"""
489:     
490:     def __init__(self, total_count: int) -> None:
491:         self.total_count: int = total_count
492:         self.processed_count: int = 0
493:         self.start_time: float = time.perf_counter()
494:         self.last_log_time: float = self.start_time
495:         self.log_interval: int = 10  # seconds
496:     
497:     def increment(self) -> None:
498:         self.processed_count += 1
499:     
500:     def should_log_progress(self) -> bool:
501:         current_time = time.perf_counter()
502:         if current_time - self.last_log_time >= self.log_interval:
503:             self.last_log_time = current_time
504:             return True
505:         return False
506:     
507:     def get_progress_stats(self) -> Dict[str, Any]:
508:         """Get current progress statistics"""
509:         elapsed: float = time.perf_counter() - self.start_time
510:         
511:         if self.processed_count > 0:
512:             rate: float = self.processed_count / elapsed
513:             remaining: int = self.total_count - self.processed_count
514:             eta_seconds: float = remaining / rate if rate > 0 else 0
515:         else:
516:             rate = 0
517:             eta_seconds = 0
518:         
519:         return {
520:             'processed': self.processed_count,
521:             'total': self.total_count,
522:             'elapsed_seconds': elapsed,
523:             'rate_per_second': rate,
524:             'eta_seconds': eta_seconds,
525:             'percent_complete': (self.processed_count / self.total_count * 100) if self.total_count > 0 else 0
526:         }
527:     
528:     def log_progress(self) -> None:
529:         """Log current progress"""
530:         stats = self.get_progress_stats()
531:         
532:         eta_minutes: float = stats['eta_seconds'] / 60
533:         elapsed_minutes: float = stats['elapsed_seconds'] / 60
534:         
535:         logger.info(
536:             f"Progress: {stats['processed']}/{stats['total']} "
537:             f"({stats['percent_complete']:.1f}%) - "
538:             f"{stats['rate_per_second']:.2f} TIPs/sec - "
539:             f"Elapsed: {elapsed_minutes:.1f}m - "
540:             f"ETA: {eta_minutes:.1f}m"
541:         )
542:     
543:     def log_shutdown_summary(self, reason: str = "complete") -> None:
544:         """Log final summary"""
545:         stats = self.get_progress_stats()
546:         elapsed_minutes: float = stats['elapsed_seconds'] / 60
547:         
548:         logger.info("=" * 60)
549:         logger.info(f"PROCESSING {reason.upper()}")
550:         logger.info(f"Processed: {stats['processed']}/{stats['total']} TIPs")
551:         logger.info(f"Duration: {elapsed_minutes:.1f} minutes")
552:         logger.info(f"Average rate: {stats['rate_per_second']:.2f} TIPs/sec")
553:         logger.info("=" * 60)
</file>

<file path="processors/object_processor.py">
  1: """
  2: Object Processor Module
  3: 
  4: Main processing orchestrator that coordinates:
  5: - API requests via APIClient
  6: - Field extraction via FieldProcessor
  7: - Report generation via ReportGenerator
  8: - Attachment downloading via AttachmentDownloader
  9: - Database operations via DatabaseRecordManager
 10: 
 11: This is the generic processor used by all object type scripts.
 12: Each object type script just specifies which config file to use.
 13: """
 14: 
 15: from __future__ import annotations
 16: import csv
 17: import json
 18: import logging
 19: import time
 20: import uuid
 21: from datetime import datetime
 22: from pathlib import Path
 23: from typing import Optional, Dict, Any, List, Tuple
 24: 
 25: from .base_processor import (
 26:     GracefulShutdownHandler,
 27:     APIClient,
 28:     AttachmentDownloader,
 29:     FolderManager,
 30:     RetryManager,
 31:     ProgressTracker,
 32:     sanitise_filename
 33: )
 34: from .field_processor import FieldProcessor, DatabaseRecordManager
 35: from .report_generator import create_report_generator
 36: 
 37: logger: logging.Logger = logging.getLogger(__name__)
 38: 
 39: 
 40: class ObjectProcessor:
 41:     """
 42:     Generic object processor for any Noggin inspection type
 43:     
 44:     Usage:
 45:         processor = ObjectProcessor(
 46:             base_config_path='config/base_config.ini',
 47:             specific_config_path='config/coupling_compliance_check_config.ini'
 48:         )
 49:         processor.run()
 50:     """
 51:     
 52:     def __init__(self, base_config_path: str, specific_config_path: str) -> None:
 53:         # Import here to avoid circular imports
 54:         from common import (
 55:             ConfigLoader, LoggerManager, DatabaseConnectionManager,
 56:             HashManager, CircuitBreaker
 57:         )
 58:         
 59:         # Load configuration
 60:         self.config: ConfigLoader = ConfigLoader(base_config_path, specific_config_path)
 61:         
 62:         # Get object type info
 63:         obj_config = self.config.get_object_type_config()
 64:         self.object_type: str = obj_config['object_type']
 65:         self.abbreviation: str = obj_config['abbreviation']
 66:         self.endpoint_template: str = obj_config['endpoint']
 67:         
 68:         # Generate session ID
 69:         self.session_id: str = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{self.abbreviation}_{str(uuid.uuid4())[:8].upper()}"
 70:         
 71:         # Set up logging
 72:         self.logger_manager: LoggerManager = LoggerManager(
 73:             self.config, 
 74:             script_name=f"processor_{self.abbreviation.lower()}"
 75:         )
 76:         self.logger_manager.configure_application_logger()
 77:         self.session_logger = self.logger_manager.create_session_logger(self.session_id)
 78:         
 79:         # Database connection
 80:         self.db_manager: DatabaseConnectionManager = DatabaseConnectionManager(self.config)
 81:         
 82:         # Hash manager
 83:         self.hash_manager: HashManager = HashManager(self.config, self.db_manager)
 84:         
 85:         # Circuit breaker
 86:         self.circuit_breaker: CircuitBreaker = CircuitBreaker(self.config)
 87:         
 88:         # API client
 89:         self.api_client: APIClient = APIClient(self.config, self.circuit_breaker)
 90:         
 91:         # Field processor
 92:         self.field_processor: FieldProcessor = FieldProcessor(self.config, self.hash_manager)
 93:         
 94:         # Database record manager
 95:         self.record_manager: DatabaseRecordManager = DatabaseRecordManager(
 96:             self.db_manager, self.field_processor
 97:         )
 98:         
 99:         # Report generator
100:         self.report_generator = create_report_generator(self.config, self.hash_manager)
101:         
102:         # Folder manager
103:         self.folder_manager: FolderManager = FolderManager(self.config, self.abbreviation)
104:         
105:         # Attachment downloader
106:         self.attachment_downloader: AttachmentDownloader = AttachmentDownloader(
107:             self.config, self.db_manager, self.api_client
108:         )
109:         
110:         # Retry manager
111:         self.retry_manager: RetryManager = RetryManager(self.config)
112:         
113:         # Shutdown handler
114:         self.shutdown_handler: GracefulShutdownHandler = GracefulShutdownHandler(
115:             self.db_manager, logger, self._on_shutdown
116:         )
117:         
118:         # Processing settings
119:         self.attachment_pause: int = self.config.getint('processing', 'attachment_pause')
120:         self.base_url: str = self.config.get('api', 'base_url')
121:         
122:         # Log startup
123:         self._log_startup()
124:     
125:     def _log_startup(self) -> None:
126:         """Log startup information"""
127:         logger.info("=" * 80)
128:         logger.info(f"NOGGIN PROCESSOR - {self.object_type.upper()}")
129:         logger.info("=" * 80)
130:         logger.info(f"Session ID:       {self.session_id}")
131:         logger.info(f"Object Type:      {self.object_type}")
132:         logger.info(f"Abbreviation:     {self.abbreviation}")
133:         logger.info(f"Base Output Path: {self.folder_manager.base_path}")
134:         logger.info("=" * 80)
135:         
136:         # Session logger header
137:         output_patterns = self.config.get_output_patterns()
138:         header = output_patterns.get('session_log_header', 
139:             'TIMESTAMP\tTIP\tINSPECTION_ID\tATTACHMENTS_COUNT\tATTACHMENT_FILENAMES')
140:         
141:         self.session_logger.info(f"SESSION START: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
142:         self.session_logger.info(f"SESSION ID: {self.session_id}")
143:         self.session_logger.info(f"OBJECT TYPE: {self.object_type}")
144:         self.session_logger.info("")
145:         self.session_logger.info(header)
146:     
147:     def _on_shutdown(self) -> None:
148:         """Callback when shutdown is triggered"""
149:         logger.info("Shutdown callback triggered")
150:     
151:     def run(self, csv_file_path: Optional[str] = None, 
152:             batch_size: int = 10,
153:             from_database: bool = False) -> int:
154:         """
155:         Main processing entry point
156:         
157:         Args:
158:             csv_file_path: Path to CSV file with TIPs (if not using database)
159:             batch_size: Number of TIPs to process per batch
160:             from_database: If True, get TIPs from database instead of CSV
161:             
162:         Returns:
163:             Number of TIPs processed
164:         """
165:         if from_database:
166:             return self._run_from_database(batch_size)
167:         elif csv_file_path:
168:             return self._run_from_csv(csv_file_path)
169:         else:
170:             # Default: look for tip.csv in input folder
171:             input_folder = Path(self.config.get('paths', 'input_folder_path'))
172:             default_csv = input_folder / f"{self.abbreviation.lower()}_tips.csv"
173:             
174:             if not default_csv.exists():
175:                 default_csv = input_folder / "tip.csv"
176:             
177:             if default_csv.exists():
178:                 return self._run_from_csv(str(default_csv))
179:             else:
180:                 logger.warning(f"No CSV file found and from_database=False")
181:                 return 0
182:     
183:     def _run_from_csv(self, csv_file_path: str) -> int:
184:         """Process TIPs from CSV file"""
185:         csv_path = Path(csv_file_path)
186:         
187:         if not csv_path.exists():
188:             logger.error(f"CSV file not found: {csv_path}")
189:             return 0
190:         
191:         # Count TIPs
192:         total_count = self._count_tips_in_csv(csv_path)
193:         logger.info(f"Found {total_count} TIPs in {csv_path}")
194:         
195:         if total_count == 0:
196:             logger.warning("No TIPs to process")
197:             return 0
198:         
199:         # Progress tracker
200:         progress = ProgressTracker(total_count)
201:         
202:         # Process TIPs
203:         processed_count = 0
204:         
205:         with open(csv_path, 'r', encoding='utf-8') as f:
206:             reader = csv.DictReader(f)
207:             
208:             # Find TIP column (case-insensitive)
209:             tip_column = None
210:             for col in reader.fieldnames or []:
211:                 if col.lower() == 'tip':
212:                     tip_column = col
213:                     break
214:             
215:             if not tip_column:
216:                 logger.error("No 'tip' column found in CSV")
217:                 return 0
218:             
219:             for row in reader:
220:                 if not self.shutdown_handler.should_continue():
221:                     logger.info("Shutdown requested, stopping processing")
222:                     break
223:                 
224:                 tip_value = row.get(tip_column, '').strip()
225:                 
226:                 if not tip_value:
227:                     continue
228:                 
229:                 self.shutdown_handler.set_current_tip(tip_value)
230:                 
231:                 success = self._process_single_tip(tip_value)
232:                 
233:                 if success:
234:                     processed_count += 1
235:                     progress.increment()
236:                 
237:                 if progress.should_log_progress():
238:                     progress.log_progress()
239:         
240:         self.shutdown_handler.set_current_tip(None)
241:         
242:         # Log summary
243:         reason = "interrupted" if not self.shutdown_handler.should_continue() else "complete"
244:         progress.log_shutdown_summary(reason)
245:         
246:         return processed_count
247:     
248:     def _run_from_database(self, batch_size: int = 10) -> int:
249:         """Process TIPs from database queue"""
250:         processed_count = 0
251:         
252:         while self.shutdown_handler.should_continue():
253:             # Get batch of TIPs
254:             tips = self.record_manager.get_tips_to_process(self.object_type, batch_size)
255:             
256:             if not tips:
257:                 logger.info("No TIPs to process in queue")
258:                 break
259:             
260:             for tip_record in tips:
261:                 if not self.shutdown_handler.should_continue():
262:                     break
263:                 
264:                 tip_value = tip_record['tip']
265:                 self.shutdown_handler.set_current_tip(tip_value)
266:                 
267:                 success = self._process_single_tip(tip_value)
268:                 
269:                 if success:
270:                     processed_count += 1
271:         
272:         self.shutdown_handler.set_current_tip(None)
273:         
274:         return processed_count
275:     
276:     def _process_single_tip(self, tip_value: str) -> bool:
277:         """
278:         Process a single TIP
279:         
280:         Returns:
281:             True if processed successfully (or non-retriable error)
282:         """
283:         logger.info(f"Processing TIP: {tip_value}")
284:         
285:         # Build API URL
286:         request_url = self.base_url + self.endpoint_template.replace('$tip', tip_value)
287:         
288:         try:
289:             # Circuit breaker check
290:             from common import CircuitBreakerError
291:             
292:             try:
293:                 self.circuit_breaker.before_request()
294:             except CircuitBreakerError as e:
295:                 logger.warning(f"Circuit breaker open: {e}")
296:                 return False
297:             
298:             # Make API request
299:             response = self.api_client.make_request(request_url, tip_value)
300:             
301:             if response.status_code == 200:
302:                 self.circuit_breaker.record_success()
303:                 return self._handle_successful_response(response, tip_value)
304:             
305:             elif response.status_code == 429:
306:                 # Rate limited
307:                 self.circuit_breaker.record_failure()
308:                 sleep_time = self.api_client.too_many_requests_sleep
309:                 logger.warning(f"Rate limited, sleeping {sleep_time}s")
310:                 time.sleep(sleep_time)
311:                 return False
312:             
313:             elif response.status_code == 404:
314:                 # Not found - mark as permanent failure
315:                 error_msg = self.api_client.handle_error(response, tip_value, request_url)
316:                 logger.warning(error_msg)
317:                 self.record_manager.update_processing_status(tip_value, 'not_found', error_msg)
318:                 return True  # Don't retry 404s
319:             
320:             else:
321:                 # Other error
322:                 self.circuit_breaker.record_failure()
323:                 error_msg = self.api_client.handle_error(response, tip_value, request_url)
324:                 logger.error(error_msg)
325:                 self._handle_api_error(tip_value, error_msg)
326:                 return False
327:         
328:         except Exception as e:
329:             error_msg = f"Exception processing TIP {tip_value}: {e}"
330:             logger.error(error_msg, exc_info=True)
331:             self._handle_api_error(tip_value, error_msg)
332:             return False
333:     
334:     def _handle_successful_response(self, response, tip_value: str) -> bool:
335:         """Handle successful API response"""
336:         try:
337:             response_data = response.json()
338:         except json.JSONDecodeError as e:
339:             logger.error(f"Invalid JSON response for TIP {tip_value}: {e}")
340:             self.record_manager.update_processing_status(tip_value, 'api_error', f"Invalid JSON: {e}")
341:             return False
342:         
343:         # Extract inspection ID
344:         inspection_id = self.field_processor.extract_inspection_id(response_data) or tip_value[:16]
345:         
346:         # Insert/update database record
347:         try:
348:             self.record_manager.insert_or_update_record(response_data, tip_value)
349:         except Exception as e:
350:             logger.error(f"Failed to insert record for TIP {tip_value}: {e}")
351:             return False
352:         
353:         # Process attachments and generate report
354:         self._process_attachments_and_report(response_data, inspection_id, tip_value)
355:         
356:         return True
357:     
358:     def _process_attachments_and_report(self, response_data: Dict[str, Any],
359:                                         inspection_id: str, tip_value: str) -> None:
360:         """Process attachments and generate report"""
361:         # Get date for folder creation
362:         date_str = response_data.get('date', '')
363:         
364:         # Create inspection folder
365:         inspection_folder = self.folder_manager.create_inspection_folder(date_str, inspection_id)
366:         
367:         # Generate and save report
368:         report = self.report_generator.generate_report(response_data, inspection_id)
369:         self.report_generator.save_report(report, inspection_folder, inspection_id)
370:         
371:         # Process attachments
372:         attachments = response_data.get('attachments', [])
373:         
374:         if not attachments:
375:             logger.info(f"No attachments for {inspection_id}")
376:             self._log_session_record(tip_value, inspection_id, 0, [])
377:             self.record_manager.update_attachment_counts(tip_value, 0, 0, True)
378:             self.record_manager.update_processing_status(tip_value, 'complete')
379:             return
380:         
381:         logger.info(f"Processing {len(attachments)} attachments for {inspection_id}")
382:         
383:         # Download attachments
384:         successful_downloads = 0
385:         attachment_filenames: List[str] = []
386:         
387:         for i, attachment_url in enumerate(attachments, 1):
388:             if not self.shutdown_handler.should_continue():
389:                 logger.warning(f"Shutdown during attachment {i}/{len(attachments)}")
390:                 break
391:             
392:             # Extract attachment TIP
393:             attachment_tip = attachment_url.split('tip=')[-1] if 'tip=' in attachment_url else f'unknown_{i}'
394:             
395:             # Construct filename
396:             filename = self.folder_manager.construct_attachment_filename(
397:                 inspection_id, date_str, i
398:             )
399:             
400:             # Download
401:             success, retry_count, file_size_mb, error_msg = self.attachment_downloader.download(
402:                 attachment_url, filename, inspection_id, attachment_tip,
403:                 inspection_folder, tip_value, i
404:             )
405:             
406:             if success:
407:                 successful_downloads += 1
408:                 attachment_filenames.append(filename)
409:             
410:             # Pause between attachments
411:             if self.attachment_pause > 0 and i < len(attachments):
412:                 time.sleep(self.attachment_pause)
413:         
414:         # Determine final status
415:         if not self.shutdown_handler.should_continue():
416:             final_status = 'interrupted'
417:         elif successful_downloads == len(attachments):
418:             final_status = 'complete'
419:         elif successful_downloads > 0:
420:             final_status = 'partial'
421:         else:
422:             final_status = 'failed'
423:         
424:         # Update database
425:         all_complete = successful_downloads == len(attachments)
426:         self.record_manager.update_attachment_counts(
427:             tip_value, len(attachments), successful_downloads, all_complete
428:         )
429:         self.record_manager.update_processing_status(tip_value, final_status)
430:         
431:         # Log to session
432:         self._log_session_record(tip_value, inspection_id, successful_downloads, attachment_filenames)
433:         
434:         logger.info(f"Completed {inspection_id}: {successful_downloads}/{len(attachments)} attachments")
435:     
436:     def _handle_api_error(self, tip_value: str, error_msg: str) -> None:
437:         """Handle API error with retry logic"""
438:         # Get current retry count
439:         result = self.db_manager.execute_query_dict(
440:             "SELECT retry_count FROM noggin_data WHERE tip = %s",
441:             (tip_value,)
442:         )
443:         
444:         current_retry = result[0]['retry_count'] if result else 0
445:         new_retry = current_retry + 1
446:         
447:         if self.retry_manager.should_retry(new_retry):
448:             next_retry = self.retry_manager.calculate_next_retry_time(new_retry)
449:             self.record_manager.update_retry_info(tip_value, new_retry, next_retry)
450:             self.record_manager.update_processing_status(tip_value, 'api_error', error_msg)
451:             logger.info(f"Scheduled retry {new_retry} for TIP {tip_value} at {next_retry}")
452:         else:
453:             self.record_manager.mark_permanently_failed(tip_value, f"Max retries exceeded: {error_msg}")
454:             logger.warning(f"TIP {tip_value} permanently failed after {new_retry} retries")
455:         
456:         # Record error
457:         self.record_manager.record_processing_error(
458:             tip_value, 'api_error', error_msg,
459:             {'retry_count': new_retry}
460:         )
461:     
462:     def _log_session_record(self, tip_value: str, inspection_id: str,
463:                            attachment_count: int, filenames: List[str]) -> None:
464:         """Log to session log file"""
465:         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
466:         filenames_str = ";".join(filenames) if filenames else "NONE"
467:         
468:         self.session_logger.info(
469:             f"{timestamp}\t{tip_value}\t{inspection_id}\t{attachment_count}\t{filenames_str}"
470:         )
471:     
472:     def _count_tips_in_csv(self, csv_path: Path) -> int:
473:         """Count valid TIPs in CSV file"""
474:         count = 0
475:         
476:         with open(csv_path, 'r', encoding='utf-8') as f:
477:             reader = csv.DictReader(f)
478:             
479:             tip_column = None
480:             for col in reader.fieldnames or []:
481:                 if col.lower() == 'tip':
482:                     tip_column = col
483:                     break
484:             
485:             if not tip_column:
486:                 return 0
487:             
488:             for row in reader:
489:                 if row.get(tip_column, '').strip():
490:                     count += 1
491:         
492:         return count
493:     
494:     def process_single(self, tip_value: str) -> bool:
495:         """Process a single TIP (for testing or manual processing)"""
496:         return self._process_single_tip(tip_value)
497: 
498: 
499: def create_processor(base_config: str, specific_config: str) -> ObjectProcessor:
500:     """Factory function to create an ObjectProcessor"""
501:     return ObjectProcessor(base_config, specific_config)
</file>

<file path="processors/report_generator.py">
  1: """
  2: Report Generator Module
  3: 
  4: Generates human-readable inspection reports using templates from config files.
  5: 
  6: Template syntax:
  7: - <field_name>: Replace with field value
  8: - <field_name_resolved>: Replace with resolved hash value
  9: - <if:field_name>...</if:field_name>: Conditional block (include if field has value)
 10: - <generation_date>: Current date
 11: - <full_name>: Object type full name
 12: - <abbreviation>: Object type abbreviation
 13: - <json_payload>: Full JSON payload
 14: - <attachment_count>: Number of attachments
 15: """
 16: 
 17: from __future__ import annotations
 18: import json
 19: import re
 20: import logging
 21: from datetime import datetime
 22: from pathlib import Path
 23: from typing import Dict, Any, Optional, List
 24: 
 25: logger: logging.Logger = logging.getLogger(__name__)
 26: 
 27: 
 28: class ReportGenerator:
 29:     """Generates inspection reports from templates"""
 30:     
 31:     def __init__(self, config: 'ConfigLoader', hash_manager: 'HashManager') -> None:
 32:         self.config = config
 33:         self.hash_manager = hash_manager
 34:         
 35:         # Load object type info
 36:         obj_config = config.get_object_type_config()
 37:         self.object_type: str = obj_config['object_type']
 38:         self.abbreviation: str = obj_config['abbreviation']
 39:         self.full_name: str = obj_config['full_name']
 40:         
 41:         # Load template
 42:         self.template: str = config.get_template_content()
 43:         
 44:         # Output settings
 45:         self.show_json: bool = config.getboolean('output', 'show_json_payload_in_text_file', 
 46:                                                   fallback=True, from_specific=True)
 47:         self.show_all_fields: bool = config.getboolean('output', 'show_all_fields',
 48:                                                         fallback=False, from_specific=True)
 49:         self.unknown_text: str = config.get('output', 'unknown_response_output_text',
 50:                                             fallback='Unknown', from_specific=True)
 51:         
 52:         # Parse field mappings for hash resolution
 53:         self.field_mappings = config.get_field_mappings()
 54:         
 55:         logger.debug(f"ReportGenerator initialised for {self.abbreviation}")
 56:     
 57:     def generate_report(self, response_data: Dict[str, Any], 
 58:                        inspection_id: str) -> str:
 59:         """
 60:         Generate report from template and API response
 61:         
 62:         Args:
 63:             response_data: API response JSON
 64:             inspection_id: Inspection ID for hash lookups
 65:             
 66:         Returns:
 67:             Formatted report string
 68:         """
 69:         # Build context with all available values
 70:         context = self._build_context(response_data, inspection_id)
 71:         
 72:         # Process template
 73:         report = self._process_template(self.template, context)
 74:         
 75:         # Clean up extra blank lines
 76:         report = re.sub(r'\n{3,}', '\n\n', report)
 77:         
 78:         return report
 79:     
 80:     def _build_context(self, response_data: Dict[str, Any], 
 81:                        inspection_id: str) -> Dict[str, Any]:
 82:         """Build context dictionary for template substitution"""
 83:         context: Dict[str, Any] = {
 84:             'generation_date': datetime.now().strftime('%d-%m-%Y'),
 85:             'full_name': self.full_name.upper(),
 86:             'abbreviation': self.abbreviation,
 87:             'attachment_count': len(response_data.get('attachments', [])),
 88:             'json_payload': json.dumps(response_data, indent=2, ensure_ascii=False),
 89:             'show_json_payload_in_text_file': self.show_json,
 90:             'show_all_fields': self.show_all_fields,
 91:         }
 92:         
 93:         # Add all response fields directly
 94:         for key, value in response_data.items():
 95:             if key == '$meta':
 96:                 continue
 97:             context[key] = value if value is not None else self.unknown_text
 98:         
 99:         # Process hash fields and add resolved values
100:         tip_value = response_data.get('$meta', {}).get('tip', '')
101:         
102:         for api_field, (db_column, field_type, hash_type) in self.field_mappings.items():
103:             value = response_data.get(api_field)
104:             
105:             if field_type == 'hash' and value and hash_type:
106:                 # Resolve hash to human-readable value
107:                 resolved = self.hash_manager.lookup_hash(
108:                     hash_type, str(value), tip_value, inspection_id
109:                 )
110:                 context[f"{api_field}_resolved"] = resolved
111:                 
112:                 # Also add without _resolved suffix for simpler templates
113:                 base_name = api_field.replace('_hash', '').replace('Hash', '')
114:                 context[f"{base_name}_resolved"] = resolved
115:         
116:         return context
117:     
118:     def _process_template(self, template: str, context: Dict[str, Any]) -> str:
119:         """Process template with context substitution"""
120:         result = template
121:         
122:         # Process conditional blocks first (nested supported)
123:         result = self._process_conditionals(result, context)
124:         
125:         # Replace placeholders
126:         result = self._replace_placeholders(result, context)
127:         
128:         return result
129:     
130:     def _process_conditionals(self, template: str, context: Dict[str, Any]) -> str:
131:         """
132:         Process conditional blocks <if:field>...</if:field>
133:         
134:         Supports nested conditionals.
135:         """
136:         # Regex for conditional blocks (non-greedy, handles nesting by processing innermost first)
137:         pattern = r'<if:(\w+)>(.*?)</if:\1>'
138:         
139:         max_iterations = 10  # Prevent infinite loops
140:         iteration = 0
141:         
142:         while iteration < max_iterations:
143:             matches = list(re.finditer(pattern, template, re.DOTALL))
144:             if not matches:
145:                 break
146:             
147:             # Process from end to start to preserve positions
148:             for match in reversed(matches):
149:                 field_name = match.group(1)
150:                 content = match.group(2)
151:                 
152:                 # Check if condition is met
153:                 should_include = self._evaluate_condition(field_name, context)
154:                 
155:                 if should_include:
156:                     # Include content (will be processed for nested conditionals in next iteration)
157:                     replacement = content
158:                 else:
159:                     # Remove entire block
160:                     replacement = ''
161:                 
162:                 template = template[:match.start()] + replacement + template[match.end():]
163:             
164:             iteration += 1
165:         
166:         return template
167:     
168:     def _evaluate_condition(self, field_name: str, context: Dict[str, Any]) -> bool:
169:         """Evaluate if a conditional field should be included"""
170:         value = context.get(field_name)
171:         
172:         if value is None:
173:             return False
174:         
175:         if isinstance(value, bool):
176:             return value
177:         
178:         if isinstance(value, str):
179:             # Empty string or "Unknown" = false
180:             return bool(value) and value != self.unknown_text
181:         
182:         if isinstance(value, (list, dict)):
183:             return len(value) > 0
184:         
185:         # Numbers: 0 = false, anything else = true
186:         if isinstance(value, (int, float)):
187:             return value != 0
188:         
189:         return bool(value)
190:     
191:     def _replace_placeholders(self, template: str, context: Dict[str, Any]) -> str:
192:         """Replace <placeholder> with values from context"""
193:         
194:         def replacer(match):
195:             field_name = match.group(1)
196:             value = context.get(field_name)
197:             
198:             if value is None:
199:                 return self.unknown_text
200:             
201:             if isinstance(value, bool):
202:                 return 'Yes' if value else 'No'
203:             
204:             if isinstance(value, datetime):
205:                 return value.strftime('%Y-%m-%d %H:%M:%S')
206:             
207:             if isinstance(value, (dict, list)):
208:                 return json.dumps(value, indent=2, ensure_ascii=False)
209:             
210:             return str(value)
211:         
212:         # Match <field_name> but not <if: or </if:
213:         pattern = r'<(?!if:|/if:)(\w+)>'
214:         
215:         return re.sub(pattern, replacer, template)
216:     
217:     def save_report(self, report: str, inspection_folder: Path, 
218:                    inspection_id: str) -> Optional[Path]:
219:         """
220:         Save report to file
221:         
222:         Args:
223:             report: Report content
224:             inspection_folder: Folder to save in
225:             inspection_id: Used for filename
226:             
227:         Returns:
228:             Path to saved file or None on error
229:         """
230:         # Sanitise ID for filename
231:         safe_id = re.sub(r'[<>:"/\\|?*]', '_', str(inspection_id))
232:         safe_id = safe_id[:100]
233:         
234:         filename = f"{safe_id}_inspection_data.txt"
235:         file_path = inspection_folder / filename
236:         
237:         try:
238:             with open(file_path, 'w', encoding='utf-8') as f:
239:                 f.write(report)
240:             
241:             logger.info(f"Saved report to: {file_path}")
242:             return file_path
243:             
244:         except IOError as e:
245:             logger.error(f"Failed to save report to {file_path}: {e}")
246:             return None
247: 
248: 
249: class DefaultReportGenerator:
250:     """
251:     Fallback report generator when no template is configured
252:     
253:     Generates a simple key-value report from the API response.
254:     """
255:     
256:     def __init__(self, config: 'ConfigLoader', hash_manager: 'HashManager') -> None:
257:         self.config = config
258:         self.hash_manager = hash_manager
259:         
260:         obj_config = config.get_object_type_config()
261:         self.full_name: str = obj_config['full_name']
262:         self.abbreviation: str = obj_config['abbreviation']
263:         
264:         self.show_json: bool = config.getboolean('output', 'show_json_payload_in_text_file',
265:                                                   fallback=True, from_specific=True)
266:         self.unknown_text: str = config.get('output', 'unknown_response_output_text',
267:                                             fallback='Unknown', from_specific=True)
268:         
269:         self.field_mappings = config.get_field_mappings()
270:     
271:     def generate_report(self, response_data: Dict[str, Any],
272:                        inspection_id: str) -> str:
273:         """Generate a default report without template"""
274:         lines: List[str] = []
275:         
276:         lines.append("=" * 60)
277:         lines.append(self.full_name.upper())
278:         lines.append(f"RECORD GENERATED: {datetime.now().strftime('%d-%m-%Y')}")
279:         lines.append("=" * 60)
280:         lines.append("")
281:         
282:         tip_value = response_data.get('$meta', {}).get('tip', '')
283:         
284:         # Output all mapped fields
285:         for api_field, (db_column, field_type, hash_type) in self.field_mappings.items():
286:             value = response_data.get(api_field)
287:             
288:             if value is None:
289:                 continue
290:             
291:             # Format field name nicely
292:             display_name = self._format_field_name(api_field)
293:             
294:             if field_type == 'hash' and hash_type:
295:                 resolved = self.hash_manager.lookup_hash(
296:                     hash_type, str(value), tip_value, inspection_id
297:                 )
298:                 lines.append(f"{display_name}: {resolved}")
299:             elif field_type == 'bool':
300:                 lines.append(f"{display_name}: {'Yes' if value else 'No'}")
301:             else:
302:                 lines.append(f"{display_name}: {value}")
303:         
304:         lines.append("")
305:         
306:         # Attachments
307:         attachment_count = len(response_data.get('attachments', []))
308:         lines.append(f"Attachments: {attachment_count}")
309:         lines.append("")
310:         
311:         # Optional JSON payload
312:         if self.show_json:
313:             lines.append("-" * 60)
314:             lines.append("COMPLETE TECHNICAL DATA (JSON FORMAT)")
315:             lines.append("-" * 60)
316:             lines.append("")
317:             lines.append(json.dumps(response_data, indent=2, ensure_ascii=False))
318:         
319:         return '\n'.join(lines)
320:     
321:     def _format_field_name(self, field_name: str) -> str:
322:         """Convert camelCase to Title Case with spaces"""
323:         # Insert space before capitals
324:         spaced = re.sub(r'([A-Z])', r' \1', field_name)
325:         # Handle consecutive capitals (e.g., "ID" -> "ID" not "I D")
326:         spaced = re.sub(r'([A-Z]+)([A-Z][a-z])', r'\1 \2', spaced)
327:         # Title case and strip
328:         return spaced.strip().title()
329:     
330:     def save_report(self, report: str, inspection_folder: Path,
331:                    inspection_id: str) -> Optional[Path]:
332:         """Save report to file"""
333:         safe_id = re.sub(r'[<>:"/\\|?*]', '_', str(inspection_id))[:100]
334:         filename = f"{safe_id}_inspection_data.txt"
335:         file_path = inspection_folder / filename
336:         
337:         try:
338:             with open(file_path, 'w', encoding='utf-8') as f:
339:                 f.write(report)
340:             logger.info(f"Saved report to: {file_path}")
341:             return file_path
342:         except IOError as e:
343:             logger.error(f"Failed to save report: {e}")
344:             return None
345: 
346: 
347: def create_report_generator(config: 'ConfigLoader', 
348:                            hash_manager: 'HashManager') -> ReportGenerator:
349:     """
350:     Factory function to create appropriate report generator
351:     
352:     Returns ReportGenerator if template configured, else DefaultReportGenerator.
353:     """
354:     try:
355:         template = config.get_template_content()
356:         if template and template.strip():
357:             return ReportGenerator(config, hash_manager)
358:     except Exception as e:
359:         logger.warning(f"Could not load template, using default: {e}")
360:     
361:     return DefaultReportGenerator(config, hash_manager)
</file>

<file path="sys/log_maintenance.py">
 1: import logging
 2: import sys
 3: import os
 4: from pathlib import Path
 5: 
 6: # Add the current directory to path so we can import 'common' if running directly
 7: sys.path.append(str(Path(__file__).parent.parent))
 8: 
 9: try:
10:     from common.logger import LoggerManager
11: except ImportError:
12:     # Fallback if testing locally without the package structure
13:     from logger import LoggerManager
14: 
15: class MaintenanceConfig:
16:     """
17:     Simple config wrapper to match ConfigLoader interface.
18:     Hardcodes paths based on your environment logs.
19:     """
20:     def get(self, section, key, fallback=None):
21:         if key == 'base_log_path':
22:             return '/mnt/data/noggin/log'
23:         if key == 'log_filename_pattern':
24:             return 'maintenance_{date}.log'
25:         return fallback
26: 
27:     def getint(self, section, key, fallback=None):
28:         if key == 'log_retention_days':
29:             return 30
30:         return fallback
31: 
32: def run_maintenance():
33:     """
34:     Runs daily log maintenance:
35:     1. Compresses logs older than 7 days.
36:     2. Deletes logs older than 30 days.
37:     """
38:     try:
39:         config = MaintenanceConfig()
40:         
41:         # Initialize manager
42:         manager = LoggerManager(config, script_name='maintenance')
43:         manager.configure_application_logger()
44:         
45:         logger = logging.getLogger("maintenance")
46:         logger.info("Starting log maintenance task...")
47:         
48:         # 1. Compress logs older than 7 days
49:         compressed = manager.compress_old_logs(days_before_compress=7)
50:         
51:         # 2. Delete logs older than 30 days
52:         cleaned = manager.cleanup_old_logs(days_to_keep=30)
53:         
54:         logger.info(f"Maintenance complete. Files Compressed: {compressed}, Files Deleted: {cleaned}")
55:         
56:     except Exception as e:
57:         # Fallback print if logger fails
58:         print(f"CRITICAL: Maintenance script failed: {e}", file=sys.stderr)
59:         sys.exit(1)
60: 
61: if __name__ == "__main__":
62:     run_maintenance()
</file>

<file path="sys/system_audit.sh">
 1: #!/bin/bash
 2: 
 3: OUTPUT_FILE="system_context.txt"
 4: 
 5: # Clear the file first
 6: > "$OUTPUT_FILE"
 7: 
 8: echo "Generating system audit for NotebookLM..."
 9: 
10: {
11:     echo "=========================================="
12:     echo " SYSTEM CONTEXT REPORT"
13:     echo "=========================================="
14:     echo "Generated on: $(date)"
15:     echo ""
16: 
17:     echo "### 1. OPERATING SYSTEM DETAILS"
18:     echo "--------------------------------"
19:     # Get standard OS release info
20:     if [ -f /etc/os-release ]; then
21:         cat /etc/os-release | grep -E 'PRETTY_NAME|VERSION_ID'
22:     fi
23:     echo "Kernel: $(uname -sr)"
24:     echo "Architecture: $(uname -m)"
25:     echo ""
26: 
27:     echo "### 2. PYTHON ENVIRONMENT"
28:     echo "--------------------------------"
29:     # Check for python3 specifically
30:     if command -v python3 &> /dev/null; then
31:         echo "Python Executable: $(which python3)"
32:         echo "Python Version: $(python3 --version)"
33:     else
34:         echo "WARNING: python3 not found in PATH."
35:     fi
36:     echo ""
37: 
38:     echo "### 3. INSTALLED PACKAGES (pip freeze)"
39:     echo "--------------------------------"
40:     # Only run if pip is available
41:     if command -v pip3 &> /dev/null; then
42:         pip3 freeze
43:     else
44:         echo "pip3 command not found."
45:     fi
46:     echo ""
47: 
48:     echo "### 4. RELEVANT ENVIRONMENT VARIABLES"
49:     echo "--------------------------------"
50:     # We filter for relevant vars to avoid noise (like LS_COLORS)
51:     printenv | grep -E 'PYTHON|PATH|LANG|VIRTUAL_ENV|SHELL' | sort
52:     echo ""
53: 
54:     echo "### 5. HARDWARE RESOURCE OVERVIEW"
55:     echo "--------------------------------"
56:     # Quick check of memory and disk space
57:     free -h | grep "Mem:" | awk '{print "Total Memory: " $2}'
58:     df -h / | tail -1 | awk '{print "Disk Space (/): " $2 " Total, " $4 " Free"}'
59: 
60: } >> "$OUTPUT_FILE"
61: 
62: echo "Done! created $OUTPUT_FILE"
63: echo "You can now upload $OUTPUT_FILE along with your Repomix output."
</file>

<file path="sys/system_context.txt">
 1: ==========================================
 2:  SYSTEM CONTEXT REPORT
 3: ==========================================
 4: Generated on: Sat Jan 10 01:07:53 PM AWST 2026
 5: 
 6: ### 1. OPERATING SYSTEM DETAILS
 7: --------------------------------
 8: PRETTY_NAME="Ubuntu 24.04.3 LTS"
 9: VERSION_ID="24.04"
10: Kernel: Linux 6.8.0-90-generic
11: Architecture: x86_64
12: 
13: ### 2. PYTHON ENVIRONMENT
14: --------------------------------
15: Python Executable: /home/noggin_admin/scripts/.venv/bin/python3
16: Python Version: Python 3.12.3
17: 
18: ### 3. INSTALLED PACKAGES (pip freeze)
19: --------------------------------
20: blinker==1.9.0
21: certifi==2025.10.5
22: charset-normalizer==3.4.4
23: click==8.3.0
24: Flask==3.1.2
25: Flask-HTTPAuth==4.8.0
26: gunicorn==23.0.0
27: idna==3.11
28: itsdangerous==2.2.0
29: Jinja2==3.1.6
30: markdown-it-py==4.0.0
31: MarkupSafe==3.0.3
32: mdurl==0.1.2
33: mypy==1.18.2
34: mypy_extensions==1.1.0
35: numpy==2.3.4
36: packaging==25.0
37: pandas==2.3.3
38: pathspec==0.12.1
39: psycopg2==2.9.11
40: psycopg2-binary==2.9.11
41: Pygments==2.19.2
42: python-dateutil==2.9.0.post0
43: pytz==2025.2
44: requests==2.32.5
45: rich==14.2.0
46: six==1.17.0
47: tabulate==0.9.0
48: typesplainer==0.2.6
49: typing_extensions==4.15.0
50: tzdata==2025.2
51: urllib3==2.5.0
52: Werkzeug==3.1.3
53: 
54: ### 4. RELEVANT ENVIRONMENT VARIABLES
55: --------------------------------
56: BUNDLED_DEBUGPY_PATH=/home/noggin_admin/.vscode-server/extensions/ms-python.debugpy-2025.16.0-linux-x64/bundled/libs/debugpy
57: LANG=en_US.UTF-8
58: PATH=/home/noggin_admin/scripts/.venv/bin:/home/noggin_admin/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand:/home/noggin_admin/.vscode-server/data/User/globalStorage/github.copilot-chat/copilotCli:/home/noggin_admin/.vscode-server/bin/994fd12f8d3a5aa16f17d42c041e5809167e845a/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/noggin_admin/.vscode-server/extensions/ms-python.debugpy-2025.16.0-linux-x64/bundled/scripts/noConfigScripts
59: PYTHON_BASIC_REPL=1
60: PYTHONSTARTUP=/home/noggin_admin/.vscode-server/data/User/workspaceStorage/c4823f721e933cca932dc797f76bc9fd-1/ms-python.python/pythonrc.py
61: SHELL=/usr/bin/zsh
62: VIRTUAL_ENV_DISABLE_PROMPT=1
63: VIRTUAL_ENV=/home/noggin_admin/scripts/.venv
64: VSCODE_PYTHON_AUTOACTIVATE_GUARD=1
65: VSCODE_SHELL_ENV_REPORTING=PATH,VIRTUAL_ENV,HOME,SHELL,PWD
66: 
67: ### 5. HARDWARE RESOURCE OVERVIEW
68: --------------------------------
69: Total Memory: 3.7Gi
70: Disk Space (/): 48G Total, 33G Free
</file>

<file path="web/templates/hashes.html">
 1: {% extends "base.html" %}
 2: 
 3: {% block title %}Hash Management - Noggin Processor{% endblock %}
 4: 
 5: {% block content %}
 6: <div class="card">
 7:     <h2>Entity Hash Statistics</h2>
 8:     
 9:     <table>
10:         <thead>
11:             <tr>
12:                 <th>Entity Type</th>
13:                 <th>Known</th>
14:                 <th>Unknown</th>
15:                 <th>Coverage</th>
16:                 <th>Actions</th>
17:             </tr>
18:         </thead>
19:         <tbody>
20:             {% for object_type in ['vehicle', 'trailer', 'department', 'team'] %}
21:             {% set known = stats.get(object_type, {}).get('known', 0) %}
22:             {% set unknown = stats.get(object_type, {}).get('unknown', 0) %}
23:             {% set total = known + unknown %}
24:             {% set coverage = (known / total * 100) if total > 0 else 0 %}
25:             <tr>
26:                 <td><strong>{{ object_type.capitalize() }}</strong></td>
27:                 <td>{{ known }}</td>
28:                 <td>{{ unknown }}</td>
29:                 <td>
30:                     {% if coverage == 100 %}
31:                         <span class="badge success">{{ "%.1f"|format(coverage) }}%</span>
32:                     {% elif coverage > 90 %}
33:                         <span class="badge warning">{{ "%.1f"|format(coverage) }}%</span>
34:                     {% else %}
35:                         <span class="badge danger">{{ "%.1f"|format(coverage) }}%</span>
36:                     {% endif %}
37:                 </td>
38:                 <td>
39:                     {% if unknown > 0 %}
40:                         <a href="#" class="btn">Export Unknown</a>
41:                     {% else %}
42:                         <span class="badge success"> Complete</span>
43:                     {% endif %}
44:                 </td>
45:             </tr>
46:             {% endfor %}
47:         </tbody>
48:     </table>
49: </div>
50: 
51: <div class="card">
52:     <h2>Unknown Hashes Requiring Resolution</h2>
53:     {% if unknown_counts %}
54:         <p>The following entity types have unresolved hashes:</p>
55:         <ul>
56:             {% for item in unknown_counts %}
57:             <li><strong>{{ item.object_type.capitalize() }}:</strong> {{ item.count }} unknown hashes</li>
58:             {% endfor %}
59:         </ul>
60:         <p>Use the CLI tool to export and resolve: <code>python manage_hashes.py export-unknown [type] --gui</code></p>
61:     {% else %}
62:         <p class="badge success">All hashes resolved!</p>
63:     {% endif %}
64: </div>
65: {% endblock %}
</file>

<file path="web/templates/inspection_detail.html">
  1: {% extends "base.html" %}
  2: 
  3: {% block title %}Inspection {{ inspection.lcd_inspection_id }} - Noggin Processor{% endblock %}
  4: 
  5: {% block content %}
  6: <div class="card">
  7:     <h2>Inspection Details: {{ inspection.lcd_inspection_id or 'N/A' }}</h2>
  8:     
  9:     <table>
 10:         <tr>
 11:             <th>LCD Inspection ID</th>
 12:             <td>{{ inspection.lcd_inspection_id or 'N/A' }}</td>
 13:         </tr>
 14:         <tr>
 15:             <th>Inspection Date</th>
 16:             <td>{{ inspection.inspection_date.strftime('%Y-%m-%d %H:%M') if inspection.inspection_date else 'N/A' }}</td>
 17:         </tr>
 18:         <tr>
 19:             <th>Vehicle</th>
 20:             <td>{{ inspection.vehicle or 'N/A' }}</td>
 21:         </tr>
 22:         <tr>
 23:             <th>Trailer</th>
 24:             <td>{{ inspection.trailer or 'N/A' }}</td>
 25:         </tr>
 26:         <tr>
 27:             <th>Department</th>
 28:             <td>{{ inspection.department or 'N/A' }}</td>
 29:         </tr>
 30:         <tr>
 31:             <th>Team</th>
 32:             <td>{{ inspection.team or 'N/A' }}</td>
 33:         </tr>
 34:         <tr>
 35:             <th>Load Compliance</th>
 36:             <td>{{ inspection.load_compliance or 'N/A' }}</td>
 37:         </tr>
 38:         <tr>
 39:             <th>Processing Status</th>
 40:             <td>
 41:                 {% set status_class = {
 42:                     'complete': 'success',
 43:                     'pending': 'info',
 44:                     'failed': 'danger',
 45:                     'partial': 'warning'
 46:                 }.get(inspection.processing_status, 'secondary') %}
 47:                 <span class="badge {{ status_class }}">{{ inspection.processing_status }}</span>
 48:             </td>
 49:         </tr>
 50:         <tr>
 51:             <th>Retry Count</th>
 52:             <td>{{ inspection.retry_count or 0 }}</td>
 53:         </tr>
 54:     </table>
 55: </div>
 56: 
 57: <div class="card">
 58:     <h2>Attachments ({{ attachments|length }})</h2>
 59:     <table>
 60:         <thead>
 61:             <tr>
 62:                 <th>Filename</th>
 63:                 <th>Status</th>
 64:                 <th>Size</th>
 65:                 <th>Duration</th>
 66:                 <th>Validation</th>
 67:             </tr>
 68:         </thead>
 69:         <tbody>
 70:             {% for att in attachments %}
 71:             <tr>
 72:                 <td>{{ att.filename }}</td>
 73:                 <td>
 74:                     {% set status_class = {
 75:                         'downloaded': 'success',
 76:                         'pending': 'info',
 77:                         'failed': 'danger'
 78:                     }.get(att.attachment_status, 'secondary') %}
 79:                     <span class="badge {{ status_class }}">{{ att.attachment_status }}</span>
 80:                 </td>
 81:                 <td>{{ "%.2f"|format(att.file_size_bytes / 1024 / 1024) }} MB</td>
 82:                 <td>{{ "%.1f"|format(att.download_duration_seconds) }}s</td>
 83:                 <td>
 84:                     {% if att.attachment_validation_status == 'valid' %}
 85:                         <span class="badge success">Valid</span>
 86:                     {% else %}
 87:                         <span class="badge warning">{{ att.attachment_validation_status or 'N/A' }}</span>
 88:                     {% endif %}
 89:                 </td>
 90:             </tr>
 91:             {% endfor %}
 92:         </tbody>
 93:     </table>
 94: </div>
 95: 
 96: {% if errors %}
 97: <div class="card">
 98:     <h2>Recent Errors</h2>
 99:     <table>
100:         <thead>
101:             <tr>
102:                 <th>Type</th>
103:                 <th>Message</th>
104:                 <th>Time</th>
105:             </tr>
106:         </thead>
107:         <tbody>
108:             {% for error in errors %}
109:             <tr>
110:                 <td><span class="badge danger">{{ error.error_type }}</span></td>
111:                 <td>{{ error.error_message[:100] }}</td>
112:                 <td>{{ error.created_at.strftime('%Y-%m-%d %H:%M') }}</td>
113:             </tr>
114:             {% endfor %}
115:         </tbody>
116:     </table>
117: </div>
118: {% endif %}
119: 
120: <a href="{{ url_for('inspections') }}" class="btn"> Back to Inspections</a>
121: {% endblock %}
</file>

<file path="web/templates/service_status.html">
 1: {% extends "base.html" %}
 2: 
 3: {% block title %}Service Status - Noggin Processor{% endblock %}
 4: 
 5: {% block content %}
 6: <div class="card">
 7:     <h2>Service Status</h2>
 8:     
 9:     <div class="stats-grid">
10:         <div class="stat-card {% if service_active %}success{% else %}danger{% endif %}">
11:             <div class="label">Processor Service</div>
12:             <div class="number">{{ ' Running' if service_active else ' Stopped' }}</div>
13:         </div>
14:     </div>
15: </div>
16: 
17: <div class="card">
18:     <h2>Recent Logs</h2>
19:     <pre style="background: #2c3e50; color: #ecf0f1; padding: 1rem; border-radius: 4px; overflow-x: auto; max-height: 600px; overflow-y: auto;">{{ recent_logs }}</pre>
20: </div>
21: 
22: <div class="card">
23:     <h2>Service Actions</h2>
24:     <p>Use command line to manage the service:</p>
25:     <ul>
26:         <li><code>./manage_service.sh status</code> - Check status</li>
27:         <li><code>./manage_service.sh restart</code> - Restart service</li>
28:         <li><code>./manage_service.sh logs</code> - View logs</li>
29:     </ul>
30: </div>
31: {% endblock %}
</file>

<file path="web/get_server_config.sh">
 1: #!/bin/bash
 2: 
 3: echo "=== WEB SERVER DETECTION ==="
 4: echo ""
 5: 
 6: # Check ports
 7: echo "1. Ports in use:"
 8: sudo ss -tlnp | grep -E ':(80|443|5050|8080|10000)' | awk '{print $4, $5, $7}'
 9: echo ""
10: 
11: # Check Apache
12: echo "2. Apache:"
13: if command -v apache2 &> /dev/null; then
14:     echo "   Installed: $(apache2 -v | head -1)"
15:     systemctl is-active apache2 && echo "   Status: RUNNING" || echo "   Status: STOPPED"
16: else
17:     echo "   Not installed"
18: fi
19: echo ""
20: 
21: # Check Nginx
22: echo "3. Nginx:"
23: if command -v nginx &> /dev/null; then
24:     echo "   Installed: $(nginx -v 2>&1)"
25:     systemctl is-active nginx && echo "   Status: RUNNING" || echo "   Status: STOPPED"
26: else
27:     echo "   Not installed"
28: fi
29: echo ""
30: 
31: # Check Webmin
32: echo "4. Webmin:"
33: if systemctl list-units --full --all | grep -q webmin; then
34:     systemctl is-active webmin && echo "   Status: RUNNING on port 10000" || echo "   Status: STOPPED"
35:     echo "   Access: https://$(hostname -I | awk '{print $1}'):10000"
36: else
37:     echo "   Not found as systemd service"
38: fi
39: echo ""
40: 
41: # Check pgAdmin
42: echo "5. pgAdmin:"
43: if systemctl list-units --full --all | grep -q pgadmin; then
44:     systemctl is-active pgadmin4 && echo "   Status: RUNNING" || echo "   Status: STOPPED"
45: elif pgrep -f pgadmin > /dev/null; then
46:     echo "   Process found (not systemd service)"
47: else
48:     echo "   Not running"
49: fi
50: echo ""
51: 
52: # Check what's actually listening
53: echo "6. Active web listeners:"
54: sudo lsof -i -P -n | grep LISTEN | grep -E ':(80|443|5050|8080|10000)' | awk '{print $1, $9}'
</file>

<file path="archive_monthly_sftp.py">
  1: """
  2: Monthly Archive Script for SFTP Processed Files
  3: 
  4: Compresses processed CSV files from previous month into tar.gz archive.
  5: Intended to run on the first day of each month via cron.
  6: 
  7: Cron entry example (runs at 2 AM on 1st of each month):
  8: 0 2 1 * * /path/to/venv/bin/python /path/to/archive_monthly_sftp.py
  9: 
 10: Usage:
 11:     python archive_monthly_sftp.py                    # Archive previous month
 12:     python archive_monthly_sftp.py --month 2024-12   # Archive specific month
 13:     python archive_monthly_sftp.py --dry-run         # Preview without changes
 14: """
 15: 
 16: from __future__ import annotations
 17: import argparse
 18: import gzip
 19: import logging
 20: import shutil
 21: import sys
 22: import tarfile
 23: from datetime import datetime, timedelta
 24: from pathlib import Path
 25: from typing import List, Optional, Tuple
 26: 
 27: logger: logging.Logger = logging.getLogger(__name__)
 28: 
 29: 
 30: def setup_logging(log_path: Optional[Path] = None) -> None:
 31:     """Configure basic logging for standalone execution"""
 32:     log_format = '%(asctime)s | %(levelname)-8s | %(message)s'
 33:     
 34:     handlers = [logging.StreamHandler(sys.stdout)]
 35:     
 36:     if log_path:
 37:         log_path.mkdir(parents=True, exist_ok=True)
 38:         log_file = log_path / f"archive_monthly_{datetime.now().strftime('%Y%m%d')}.log"
 39:         handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
 40:     
 41:     logging.basicConfig(
 42:         level=logging.INFO,
 43:         format=log_format,
 44:         handlers=handlers
 45:     )
 46: 
 47: 
 48: def get_previous_month(reference_date: Optional[datetime] = None) -> Tuple[int, int]:
 49:     """Get year and month for previous month"""
 50:     if reference_date is None:
 51:         reference_date = datetime.now()
 52:     
 53:     first_of_current = reference_date.replace(day=1)
 54:     last_of_previous = first_of_current - timedelta(days=1)
 55:     
 56:     return last_of_previous.year, last_of_previous.month
 57: 
 58: 
 59: def parse_month_arg(month_str: str) -> Tuple[int, int]:
 60:     """Parse YYYY-MM string to year, month tuple"""
 61:     try:
 62:         parts = month_str.split('-')
 63:         if len(parts) != 2:
 64:             raise ValueError("Expected YYYY-MM format")
 65:         
 66:         year = int(parts[0])
 67:         month = int(parts[1])
 68:         
 69:         if not (1 <= month <= 12):
 70:             raise ValueError("Month must be between 1 and 12")
 71:         if not (2000 <= year <= 2100):
 72:             raise ValueError("Year must be between 2000 and 2100")
 73:         
 74:         return year, month
 75:         
 76:     except Exception as e:
 77:         raise ValueError(f"Invalid month format '{month_str}': {e}")
 78: 
 79: 
 80: def find_files_for_month(processed_dir: Path, year: int, month: int) -> List[Path]:
 81:     """
 82:     Find all CSV files in processed directory for specified month
 83:     
 84:     Files are named: {ABBREV}_{YYYY-MM-DD}_{HHMMSS}_{uuid}.csv
 85:     """
 86:     month_prefix = f"{year}-{month:02d}-"
 87:     matching_files = []
 88:     
 89:     for csv_file in processed_dir.glob('*.csv'):
 90:         # Extract date portion from filename (second part after first underscore)
 91:         parts = csv_file.stem.split('_')
 92:         if len(parts) >= 2:
 93:             date_part = parts[1]
 94:             if date_part.startswith(month_prefix):
 95:                 matching_files.append(csv_file)
 96:     
 97:     # Sort by filename for consistent archive ordering
 98:     matching_files.sort(key=lambda p: p.name)
 99:     
100:     return matching_files
101: 
102: 
103: def create_monthly_archive(
104:     files: List[Path],
105:     archive_dir: Path,
106:     year: int,
107:     month: int,
108:     dry_run: bool = False
109: ) -> Optional[Path]:
110:     """
111:     Create compressed tar archive of files for specified month
112:     
113:     Archive naming: YYYY-MM.tar.gz
114:     """
115:     if not files:
116:         logger.info(f"No files found for {year}-{month:02d}")
117:         return None
118:     
119:     archive_name = f"{year}-{month:02d}.tar.gz"
120:     archive_path = archive_dir / archive_name
121:     
122:     if archive_path.exists():
123:         logger.warning(f"Archive already exists: {archive_path}")
124:         logger.warning("Skipping to avoid overwriting existing archive")
125:         return None
126:     
127:     total_size = sum(f.stat().st_size for f in files)
128:     total_size_mb = total_size / (1024 * 1024)
129:     
130:     logger.info(f"Creating archive: {archive_name}")
131:     logger.info(f"Files to archive: {len(files)}")
132:     logger.info(f"Total size before compression: {total_size_mb:.2f} MB")
133:     
134:     if dry_run:
135:         logger.info("[DRY RUN] Would create archive with following files:")
136:         for f in files[:10]:
137:             logger.info(f"  - {f.name}")
138:         if len(files) > 10:
139:             logger.info(f"  ... and {len(files) - 10} more files")
140:         return None
141:     
142:     archive_dir.mkdir(parents=True, exist_ok=True)
143:     
144:     try:
145:         with tarfile.open(archive_path, 'w:gz') as tar:
146:             for file_path in files:
147:                 tar.add(file_path, arcname=file_path.name)
148:                 logger.debug(f"Added to archive: {file_path.name}")
149:         
150:         archive_size = archive_path.stat().st_size
151:         archive_size_mb = archive_size / (1024 * 1024)
152:         compression_ratio = (1 - archive_size / total_size) * 100 if total_size > 0 else 0
153:         
154:         logger.info(f"Archive created: {archive_path}")
155:         logger.info(f"Archive size: {archive_size_mb:.2f} MB")
156:         logger.info(f"Compression ratio: {compression_ratio:.1f}%")
157:         
158:         return archive_path
159:         
160:     except Exception as e:
161:         logger.error(f"Failed to create archive: {e}")
162:         if archive_path.exists():
163:             archive_path.unlink()
164:         raise
165: 
166: 
167: def delete_archived_files(files: List[Path], dry_run: bool = False) -> int:
168:     """Delete files that have been archived"""
169:     if dry_run:
170:         logger.info(f"[DRY RUN] Would delete {len(files)} files")
171:         return 0
172:     
173:     deleted = 0
174:     for file_path in files:
175:         try:
176:             file_path.unlink()
177:             deleted += 1
178:             logger.debug(f"Deleted: {file_path.name}")
179:         except Exception as e:
180:             logger.error(f"Failed to delete {file_path.name}: {e}")
181:     
182:     logger.info(f"Deleted {deleted} archived files")
183:     return deleted
184: 
185: 
186: def cleanup_old_archives(archive_dir: Path, retention_months: int = 24,
187:                          dry_run: bool = False) -> int:
188:     """
189:     Remove archives older than retention period
190:     
191:     Default: Keep 24 months (2 years) of archives
192:     """
193:     if retention_months <= 0:
194:         return 0
195:     
196:     cutoff_date = datetime.now() - timedelta(days=retention_months * 30)
197:     deleted = 0
198:     
199:     for archive_file in archive_dir.glob('*.tar.gz'):
200:         try:
201:             # Parse year-month from filename
202:             stem = archive_file.stem.replace('.tar', '')
203:             year, month = map(int, stem.split('-'))
204:             archive_date = datetime(year, month, 1)
205:             
206:             if archive_date < cutoff_date:
207:                 if dry_run:
208:                     logger.info(f"[DRY RUN] Would delete old archive: {archive_file.name}")
209:                 else:
210:                     archive_file.unlink()
211:                     logger.info(f"Deleted old archive: {archive_file.name}")
212:                 deleted += 1
213:                 
214:         except (ValueError, IndexError):
215:             logger.warning(f"Could not parse archive date from: {archive_file.name}")
216:             continue
217:     
218:     if deleted > 0:
219:         logger.info(f"Cleaned up {deleted} archives older than {retention_months} months")
220:     
221:     return deleted
222: 
223: 
224: def run_monthly_archive(
225:     processed_dir: Path,
226:     archive_dir: Path,
227:     year: Optional[int] = None,
228:     month: Optional[int] = None,
229:     dry_run: bool = False,
230:     delete_after_archive: bool = True,
231:     retention_months: int = 24
232: ) -> dict:
233:     """
234:     Main archive function
235:     
236:     Args:
237:         processed_dir: Directory containing processed CSV files
238:         archive_dir: Directory for compressed archives
239:         year: Target year (default: previous month's year)
240:         month: Target month (default: previous month)
241:         dry_run: Preview without making changes
242:         delete_after_archive: Delete original files after archiving
243:         retention_months: How many months of archives to keep
244:         
245:     Returns:
246:         Summary dictionary with operation results
247:     """
248:     summary = {
249:         'year': year,
250:         'month': month,
251:         'files_found': 0,
252:         'archive_created': False,
253:         'archive_path': None,
254:         'files_deleted': 0,
255:         'old_archives_cleaned': 0,
256:         'status': 'unknown'
257:     }
258:     
259:     if year is None or month is None:
260:         year, month = get_previous_month()
261:     
262:     summary['year'] = year
263:     summary['month'] = month
264:     
265:     logger.info("=" * 60)
266:     logger.info("MONTHLY ARCHIVE PROCESS")
267:     logger.info("=" * 60)
268:     logger.info(f"Target month: {year}-{month:02d}")
269:     logger.info(f"Processed directory: {processed_dir}")
270:     logger.info(f"Archive directory: {archive_dir}")
271:     logger.info(f"Dry run: {dry_run}")
272:     
273:     if not processed_dir.exists():
274:         logger.error(f"Processed directory does not exist: {processed_dir}")
275:         summary['status'] = 'error'
276:         return summary
277:     
278:     files = find_files_for_month(processed_dir, year, month)
279:     summary['files_found'] = len(files)
280:     
281:     logger.info(f"Files found for {year}-{month:02d}: {len(files)}")
282:     
283:     if not files:
284:         summary['status'] = 'no_files'
285:         return summary
286:     
287:     archive_path = create_monthly_archive(files, archive_dir, year, month, dry_run)
288:     
289:     if archive_path:
290:         summary['archive_created'] = True
291:         summary['archive_path'] = str(archive_path)
292:         
293:         if delete_after_archive:
294:             deleted = delete_archived_files(files, dry_run)
295:             summary['files_deleted'] = deleted
296:     
297:     cleaned = cleanup_old_archives(archive_dir, retention_months, dry_run)
298:     summary['old_archives_cleaned'] = cleaned
299:     
300:     summary['status'] = 'success' if not dry_run else 'dry_run'
301:     
302:     logger.info("=" * 60)
303:     logger.info("ARCHIVE SUMMARY")
304:     logger.info("=" * 60)
305:     logger.info(f"Month archived: {year}-{month:02d}")
306:     logger.info(f"Files found: {summary['files_found']}")
307:     logger.info(f"Archive created: {summary['archive_created']}")
308:     logger.info(f"Files deleted: {summary['files_deleted']}")
309:     logger.info(f"Old archives cleaned: {summary['old_archives_cleaned']}")
310:     logger.info("=" * 60)
311:     
312:     return summary
313: 
314: 
315: def main() -> int:
316:     """Command line entry point"""
317:     parser = argparse.ArgumentParser(
318:         description='Archive processed SFTP files by month',
319:         formatter_class=argparse.RawDescriptionHelpFormatter,
320:         epilog="""
321: Examples:
322:   %(prog)s                          Archive previous month
323:   %(prog)s --month 2024-12          Archive December 2024
324:   %(prog)s --dry-run                Preview without changes
325:   %(prog)s --no-delete              Archive but keep original files
326:         """
327:     )
328:     
329:     parser.add_argument(
330:         '--month',
331:         type=str,
332:         help='Month to archive in YYYY-MM format (default: previous month)'
333:     )
334:     
335:     parser.add_argument(
336:         '--dry-run',
337:         action='store_true',
338:         help='Preview actions without making changes'
339:     )
340:     
341:     parser.add_argument(
342:         '--no-delete',
343:         action='store_true',
344:         help='Keep original files after archiving'
345:     )
346:     
347:     parser.add_argument(
348:         '--processed-dir',
349:         type=str,
350:         default='/mnt/data/noggin/sftp/processed',
351:         help='Directory containing processed files'
352:     )
353:     
354:     parser.add_argument(
355:         '--archive-dir',
356:         type=str,
357:         default='/mnt/data/noggin/sftp/monthly_archives',
358:         help='Directory for compressed archives'
359:     )
360:     
361:     parser.add_argument(
362:         '--log-dir',
363:         type=str,
364:         default='/mnt/data/noggin/log',
365:         help='Directory for log files'
366:     )
367:     
368:     parser.add_argument(
369:         '--retention-months',
370:         type=int,
371:         default=24,
372:         help='Number of months of archives to retain (default: 24)'
373:     )
374:     
375:     args = parser.parse_args()
376:     
377:     setup_logging(Path(args.log_dir) if args.log_dir else None)
378:     
379:     year = None
380:     month = None
381:     
382:     if args.month:
383:         try:
384:             year, month = parse_month_arg(args.month)
385:         except ValueError as e:
386:             logger.error(str(e))
387:             return 1
388:     
389:     try:
390:         result = run_monthly_archive(
391:             processed_dir=Path(args.processed_dir),
392:             archive_dir=Path(args.archive_dir),
393:             year=year,
394:             month=month,
395:             dry_run=args.dry_run,
396:             delete_after_archive=not args.no_delete,
397:             retention_months=args.retention_months
398:         )
399:         
400:         if result['status'] in ('success', 'dry_run', 'no_files'):
401:             return 0
402:         else:
403:             return 1
404:             
405:     except Exception as e:
406:         logger.error(f"Archive failed: {e}", exc_info=True)
407:         return 1
408: 
409: 
410: if __name__ == "__main__":
411:     sys.exit(main())
</file>

<file path="manage_service.sh">
 1: #!/bin/bash
 2: 
 3: SERVICE_NAME="noggin-processor"
 4: SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
 5: 
 6: case "$1" in
 7:     start)
 8:         echo "Starting $SERVICE_NAME service..."
 9:         sudo systemctl start $SERVICE_NAME
10:         sudo systemctl status $SERVICE_NAME --no-pager
11:         ;;
12:     stop)
13:         echo "Stopping $SERVICE_NAME service..."
14:         sudo systemctl stop $SERVICE_NAME
15:         sudo systemctl status $SERVICE_NAME --no-pager
16:         ;;
17:     restart)
18:         echo "Restarting $SERVICE_NAME service..."
19:         sudo systemctl restart $SERVICE_NAME
20:         sudo systemctl status $SERVICE_NAME --no-pager
21:         ;;
22:     status)
23:         sudo systemctl status $SERVICE_NAME --no-pager
24:         ;;
25:     enable)
26:         echo "Enabling $SERVICE_NAME to start on boot..."
27:         sudo systemctl enable $SERVICE_NAME
28:         sudo systemctl status $SERVICE_NAME --no-pager
29:         ;;
30:     disable)
31:         echo "Disabling $SERVICE_NAME from starting on boot..."
32:         sudo systemctl disable $SERVICE_NAME
33:         ;;
34:     logs)
35:         echo "Showing recent logs for $SERVICE_NAME..."
36:         sudo journalctl -u $SERVICE_NAME -n 50 --no-pager
37:         ;;
38:     follow)
39:         echo "Following logs for $SERVICE_NAME (Ctrl+C to exit)..."
40:         sudo journalctl -u $SERVICE_NAME -f
41:         ;;
42:     reload)
43:         echo "Reloading systemd daemon..."
44:         sudo systemctl daemon-reload
45:         echo "Daemon reloaded"
46:         ;;
47:     *)
48:         echo "Usage: $0 {start|stop|restart|status|enable|disable|logs|follow|reload}"
49:         echo ""
50:         echo "Commands:"
51:         echo "  start   - Start the service"
52:         echo "  stop    - Stop the service"
53:         echo "  restart - Restart the service"
54:         echo "  status  - Show service status"
55:         echo "  enable  - Enable service to start on boot"
56:         echo "  disable - Disable service from starting on boot"
57:         echo "  logs    - Show recent logs"
58:         echo "  follow  - Follow logs in real-time"
59:         echo "  reload  - Reload systemd daemon (after editing service file)"
60:         exit 1
61:         ;;
62: esac
63: 
64: exit 0
</file>

<file path="test_connection.py">
 1: import requests
 2: import json
 3: 
 4: # --- CONFIGURATION TO TEST ---
 5: # Copy these EXACTLY from your .ini file or Bruno
 6: TIP_TO_TEST = "e5135e2c69fa0933c2611a4c5b653765e2c1cdf6d0764b39db6ef1ecbbeb9af6"
 7: BASE_URL = "https://services.apse2.elasticnoggin.com"
 8: ENDPOINT = f"/rest/object/loadComplianceCheckDriverLoader/{TIP_TO_TEST}"
 9: 
10: # https://services.apse2.elasticnoggin.com/rest/object/loadComplianceCheckDriverLoader/12c02349d9d18e78095961cb3baa77eaa864fa25bd43bf8af7b0b113aacbd15e
11: 
12: 
13: # Check your config.ini for this value. It usually starts with a number.
14: NAMESPACE_ID = "6649a25a06337e51a87b77a7d83e58f522795452456649b8e019574837f8674"
15: 
16: # Paste your NEW token here (ensure no spaces at start/end)
17: TOKEN = "ZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKSVV6STFOaUo5LmV5SnpaWE56YVc5dVZHOXJaVzRpT2lKallqVm1NemRrWkRkak1XVXhPV1ZrTldNeU1EWmhZbVptTnprMVlXUmxaVEprTUdNM09ERmhOVFE0WkRCaFlXTTFNR1V6TXpRMlpqRXpaamd6TjJFMUlpd2ljMlZ6YzJsdmJrbGtJam9pTjJRek5tVTVNak16TUdZME5tSTFZVEF6TURkbU5HRXpZVFl3T0dZek5UQTJaVEZtWXpFNFpEUmpNR1ZtTldRME5UQXdZVGhqTVdRMk5EZzFaak5qTVNJc0ltNWhiV1Z6Y0dGalpTSTZJalkyTkRsaE1qVmhNRFl6TXpkbE5URmhPRGRpTnpkaE4yUTRNMlUxT0dZMU1qSTNPVFUwTlRJME5UWTJORGxpT0dVd01UazFOelE0TXpkbU9EWTNOQ0lzSW1WNGNDSTZNak01T1RNNE5qVTFNeXdpWTNWemRHOXRVR0Y1Ykc5aFpDSTZleUoxYzJWeVZHbHdJam9pWlRGa05XRm1OakJpWkdJM01UQmxaRFkyWVRaaE1HTXpPREF4WTJKa05ERXhabU5pTURJNE9UQmlNR1kwTjJJMk9HUXhORGd3WW1ZM1pHWTFabVE1T0NKOWZRLmlnX1hBX2JZLS1pTE9NeWJ3cjRQQ3l1YUhDVFVrS2pwV3E4aUlPTE1iMDg" 
18: # (If your ini includes "Bearer " inside the value, include it here. 
19: #  If the ini only has the hash, put "Bearer " before it below)
20: 
21: def test_request():
22:     url = BASE_URL + ENDPOINT
23:     
24:     # Construct headers exactly like the processor does
25:     headers = {
26:         "Accept": "application/json",
27:         "Content-Type": "application/json",
28:         "User-Agent": "NogginLCDProcessor/1.0 (Internal Integration)",
29:         "en-namespace": NAMESPACE_ID,
30:         "Authorization": TOKEN if TOKEN.startswith("Bearer") else f"Bearer {TOKEN}"
31:     }
32: 
33:     print(f"Testing URL: {url}")
34:     print("Sending headers:")
35:     print(json.dumps(headers, indent=2, default=str))
36:     print("-" * 40)
37: 
38:     try:
39:         response = requests.get(url, headers=headers, timeout=30)
40:         
41:         print(f"Response Status: {response.status_code}")
42:         
43:         if response.status_code == 200:
44:             print("SUCCESS! The credentials and URL are correct.")
45:             print("Preview:", response.text[:200])
46:         else:
47:             print("FAILED.")
48:             print("Response Body:", response.text)
49:             
50:             # Check for specific 500 causes
51:             if response.status_code == 500:
52:                 print("\n[!] DIAGNOSIS FOR 500 ERROR:")
53:                 print("1. Namespace Mismatch: Does the token belong to the same env as 'en-namespace'?")
54:                 print("2. Permissions: Does the user for this token have 'View' rights on 'Load Compliance Check'?")
55:                 print("3. Invisible Config Chars: Check your .ini file for trailing spaces after the token.")
56: 
57:     except Exception as e:
58:         print(f"Exception occurred: {e}")
59: 
60: if __name__ == "__main__":
61:     test_request()
62: 
63: # import psycopg2
64: # conn = psycopg2.connect(
65: #     # host="GS-SV-011",
66: #     # host="192.168.0.236",
67: #     host="localhost",
68: #     database="noggin_db",
69: #     user="noggin_app",
70: #     password="GoodKingCoat16"
71: # )
</file>

<file path="test_sftp_downloader.py">
  1: """
  2: Test Script for SFTP TIP Downloader
  3: 
  4: Tests the file parsing and object type detection without requiring
  5: an actual SFTP connection. Useful for development and verification.
  6: 
  7: Usage:
  8:     python test_sftp_downloader.py
  9:     python test_sftp_downloader.py --csv-file /path/to/test.csv
 10: """
 11: 
 12: from __future__ import annotations
 13: import argparse
 14: import csv
 15: import logging
 16: import sys
 17: import tempfile
 18: from pathlib import Path
 19: from typing import Dict, List, Any
 20: 
 21: logging.basicConfig(
 22:     level=logging.DEBUG,
 23:     format='%(asctime)s | %(levelname)-8s | %(message)s',
 24:     datefmt='%H:%M:%S'
 25: )
 26: logger = logging.getLogger(__name__)
 27: 
 28: 
 29: # Import functions from main module (with fallback for testing)
 30: try:
 31:     from sftp_download_tips import (
 32:         OBJECT_TYPE_SIGNATURES,
 33:         detect_object_type,
 34:         extract_tips_from_csv,
 35:         find_column_index,
 36:         parse_date
 37:     )
 38: except ImportError:
 39:     logger.warning("Could not import from sftp_download_tips, using inline definitions")
 40:     
 41:     OBJECT_TYPE_SIGNATURES = {
 42:         'couplingId': {'abbreviation': 'CCC', 'full_name': 'Coupling Compliance Check', 'id_prefix': 'C - '},
 43:         'forkliftPrestartInspectionId': {'abbreviation': 'FPI', 'full_name': 'Forklift Prestart Inspection', 'id_prefix': 'FL - Inspection - '},
 44:         'lcsInspectionId': {'abbreviation': 'LCS', 'full_name': 'Load Compliance Check Supervisor/Manager', 'id_prefix': 'LCS - '},
 45:         'lcdInspectionId': {'abbreviation': 'LCC', 'full_name': 'Load Compliance Check Driver/Loader', 'id_prefix': 'LCD - '},
 46:         'siteObservationId': {'abbreviation': 'SO', 'full_name': 'Site Observations', 'id_prefix': 'SO - '},
 47:         'trailerAuditId': {'abbreviation': 'TA', 'full_name': 'Trailer Audits', 'id_prefix': 'TA - '}
 48:     }
 49: 
 50: 
 51: def create_test_csv(object_type: str, num_rows: int = 5) -> Path:
 52:     """Create a test CSV file for specified object type"""
 53:     signatures = {
 54:         'CCC': {
 55:             'headers': ['nogginId', 'couplingId', 'personCompleting', 'date', 'team', 'vehicleId'],
 56:             'id_prefix': 'C - ',
 57:             'id_num_format': '012510'
 58:         },
 59:         'FPI': {
 60:             'headers': [' ', 'goldstarAsset', 'preStartStatus', 'assetType', 'assetId', 'assetName', 
 61:                        'personsCompleting', 'team', 'forkliftPrestartInspectionId', 'date'],
 62:             'id_prefix': 'FL - Inspection - ',
 63:             'id_num_format': '00477'
 64:         },
 65:         'LCS': {
 66:             'headers': ['nogginId', 'lcsInspectionId', 'trailer', 'trailer2', 'trailer3', 'trailerId',
 67:                        'trailerId2', 'trailerId3', 'jobNumber', 'customerClient', 'runNumber',
 68:                        'driverLoaderName', 'vehicleId', 'vehicle', 'goldstarOrContactorList',
 69:                        'contractorName', 'whichDepartmentDoesTheLoadBelongTo', 'team', 'inspectedBy', 'date'],
 70:             'id_prefix': 'LCS - ',
 71:             'id_num_format': '000004'
 72:         },
 73:         'LCC': {
 74:             'headers': ['nogginId', 'lcdInspectionId', 'date', 'inspectedBy', 'vehicle', 'vehicleId'],
 75:             'id_prefix': 'LCD - ',
 76:             'id_num_format': '047985'
 77:         },
 78:         'SO': {
 79:             'headers': ['nogginId', 'siteObservationId', 'date', 'siteManager', 'department', 'personInvolved'],
 80:             'id_prefix': 'SO - ',
 81:             'id_num_format': '00057'
 82:         },
 83:         'TA': {
 84:             'headers': [' ', 'trailerAuditId', 'team', 'date', 'inspectedBy', 'regularDriver'],
 85:             'id_prefix': 'TA - ',
 86:             'id_num_format': '00003'
 87:         }
 88:     }
 89:     
 90:     if object_type not in signatures:
 91:         raise ValueError(f"Unknown object type: {object_type}")
 92:     
 93:     sig = signatures[object_type]
 94:     
 95:     temp_file = tempfile.NamedTemporaryFile(
 96:         mode='w', suffix='.csv', delete=False, newline='', encoding='utf-8'
 97:     )
 98:     
 99:     writer = csv.writer(temp_file)
100:     writer.writerow(sig['headers'])
101:     
102:     # Generate test data rows
103:     import hashlib
104:     for i in range(num_rows):
105:         tip = hashlib.sha256(f"test_tip_{object_type}_{i}".encode()).hexdigest()
106:         inspection_id = f"{sig['id_prefix']}{int(sig['id_num_format']) + i:06d}"
107:         date = f"{15 + i}-Jun-25"
108:         
109:         # Build row based on header count
110:         row = [tip]
111:         for j, header in enumerate(sig['headers'][1:], start=1):
112:             if 'Id' in header and header.lower() != 'nogginid':
113:                 if header == sig['headers'][1] if object_type in ['FPI', 'TA'] else False:
114:                     row.append(f"hash_{j}")
115:                 elif 'Inspection' in header or header in ['couplingId', 'lcsInspectionId', 'lcdInspectionId', 
116:                                                           'siteObservationId', 'trailerAuditId']:
117:                     row.append(inspection_id)
118:                 else:
119:                     row.append(f"ID_{j}")
120:             elif header.lower() == 'date':
121:                 row.append(date)
122:             elif header == ' ':
123:                 continue  # First column is already TIP
124:             else:
125:                 row.append(f"value_{j}")
126:         
127:         # Adjust row if first header is blank (TIP is in first column)
128:         if sig['headers'][0] == ' ':
129:             writer.writerow(row)
130:         else:
131:             writer.writerow(row)
132:     
133:     temp_file.close()
134:     return Path(temp_file.name)
135: 
136: 
137: def test_object_type_detection():
138:     """Test object type detection for all known types"""
139:     logger.info("=" * 60)
140:     logger.info("TEST: Object Type Detection")
141:     logger.info("=" * 60)
142:     
143:     results = []
144:     
145:     for abbrev in ['CCC', 'FPI', 'LCS', 'LCC', 'SO', 'TA']:
146:         try:
147:             csv_path = create_test_csv(abbrev, num_rows=3)
148:             
149:             # Read headers to show what we're testing
150:             with open(csv_path, 'r', encoding='utf-8') as f:
151:                 reader = csv.reader(f)
152:                 headers = next(reader)
153:             
154:             logger.info(f"\nTesting {abbrev}:")
155:             logger.info(f"  Headers: {headers[:5]}...")
156:             
157:             # Test detection
158:             try:
159:                 from sftp_download_tips import detect_object_type
160:                 id_column, metadata = detect_object_type(csv_path)
161:                 detected = metadata['abbreviation']
162:             except ImportError:
163:                 # Fallback detection
164:                 clean_headers = [h.strip() for h in headers]
165:                 detected = None
166:                 for id_col, meta in OBJECT_TYPE_SIGNATURES.items():
167:                     if id_col in clean_headers:
168:                         detected = meta['abbreviation']
169:                         id_column = id_col
170:                         break
171:             
172:             if detected == abbrev:
173:                 logger.info(f"  Result: PASS - Detected {detected} via column '{id_column}'")
174:                 results.append((abbrev, True, detected))
175:             else:
176:                 logger.error(f"  Result: FAIL - Expected {abbrev}, got {detected}")
177:                 results.append((abbrev, False, detected))
178:             
179:             csv_path.unlink()
180:             
181:         except Exception as e:
182:             logger.error(f"  Result: ERROR - {e}")
183:             results.append((abbrev, False, str(e)))
184:     
185:     passed = sum(1 for _, success, _ in results if success)
186:     logger.info(f"\nObject Type Detection: {passed}/{len(results)} tests passed")
187:     
188:     return all(success for _, success, _ in results)
189: 
190: 
191: def test_tip_extraction():
192:     """Test TIP extraction from CSV files"""
193:     logger.info("=" * 60)
194:     logger.info("TEST: TIP Extraction")
195:     logger.info("=" * 60)
196:     
197:     for abbrev in ['CCC', 'LCS']:
198:         try:
199:             csv_path = create_test_csv(abbrev, num_rows=5)
200:             
201:             logger.info(f"\nTesting extraction from {abbrev} CSV:")
202:             
203:             # Find ID column
204:             with open(csv_path, 'r', encoding='utf-8') as f:
205:                 reader = csv.reader(f)
206:                 headers = [h.strip() for h in next(reader)]
207:             
208:             id_column = None
209:             metadata = None
210:             for col, meta in OBJECT_TYPE_SIGNATURES.items():
211:                 if col in headers:
212:                     id_column = col
213:                     metadata = meta
214:                     break
215:             
216:             if id_column is None:
217:                 logger.error("  Could not find ID column")
218:                 continue
219:             
220:             # Extract TIPs
221:             try:
222:                 from sftp_download_tips import extract_tips_from_csv
223:                 tips = extract_tips_from_csv(csv_path, id_column, metadata)
224:             except ImportError:
225:                 # Manual extraction for testing
226:                 tips = []
227:                 with open(csv_path, 'r', encoding='utf-8') as f:
228:                     reader = csv.reader(f)
229:                     headers = [h.strip() for h in next(reader)]
230:                     
231:                     id_idx = headers.index(id_column) if id_column in headers else -1
232:                     date_idx = headers.index('date') if 'date' in [h.lower() for h in headers] else -1
233:                     
234:                     for row in reader:
235:                         if row and row[0].strip():
236:                             tips.append({
237:                                 'tip': row[0].strip(),
238:                                 'inspection_id': row[id_idx] if id_idx >= 0 else None,
239:                                 'inspection_date': row[date_idx] if date_idx >= 0 else None
240:                             })
241:             
242:             logger.info(f"  Extracted {len(tips)} TIPs")
243:             
244:             if tips:
245:                 logger.info(f"  First TIP:")
246:                 logger.info(f"    tip: {tips[0]['tip'][:32]}...")
247:                 logger.info(f"    inspection_id: {tips[0].get('inspection_id')}")
248:                 logger.info(f"    inspection_date: {tips[0].get('inspection_date')}")
249:             
250:             csv_path.unlink()
251:             
252:         except Exception as e:
253:             logger.error(f"  Error: {e}")
254:             import traceback
255:             traceback.print_exc()
256: 
257: 
258: def test_date_parsing():
259:     """Test date parsing for various formats"""
260:     logger.info("=" * 60)
261:     logger.info("TEST: Date Parsing")
262:     logger.info("=" * 60)
263:     
264:     test_dates = [
265:         ('16-Jun-25', '2025-06-16'),
266:         ('4-Jun-24', '2024-06-04'),
267:         ('20-Jun-25', '2025-06-20'),
268:         ('3-Oct-24', '2024-10-03'),
269:         ('2025-01-15', '2025-01-15'),
270:         ('15/06/2025', '2025-06-15'),
271:         ('invalid', None),
272:     ]
273:     
274:     try:
275:         from sftp_download_tips import parse_date
276:     except ImportError:
277:         from datetime import datetime
278:         def parse_date(date_str):
279:             formats = ['%d-%b-%y', '%d-%b-%Y', '%d/%m/%Y', '%d/%m/%y', '%Y-%m-%d']
280:             for fmt in formats:
281:                 try:
282:                     return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d')
283:                 except ValueError:
284:                     continue
285:             return None
286:     
287:     passed = 0
288:     for input_date, expected in test_dates:
289:         result = parse_date(input_date)
290:         status = "PASS" if result == expected else "FAIL"
291:         if result == expected:
292:             passed += 1
293:         logger.info(f"  {input_date:15} -> {result or 'None':12} (expected: {expected or 'None':12}) [{status}]")
294:     
295:     logger.info(f"\nDate Parsing: {passed}/{len(test_dates)} tests passed")
296: 
297: 
298: def test_with_real_file(csv_path: str):
299:     """Test with a real CSV file"""
300:     logger.info("=" * 60)
301:     logger.info(f"TEST: Real File - {csv_path}")
302:     logger.info("=" * 60)
303:     
304:     path = Path(csv_path)
305:     if not path.exists():
306:         logger.error(f"File not found: {csv_path}")
307:         return
308:     
309:     # Read and display headers
310:     with open(path, 'r', encoding='utf-8-sig') as f:
311:         reader = csv.reader(f)
312:         headers = [h.strip() for h in next(reader)]
313:     
314:     logger.info(f"Headers ({len(headers)} columns):")
315:     for i, h in enumerate(headers[:10]):
316:         logger.info(f"  [{i}] {h}")
317:     if len(headers) > 10:
318:         logger.info(f"  ... and {len(headers) - 10} more")
319:     
320:     # Detect object type
321:     id_column = None
322:     metadata = None
323:     for col, meta in OBJECT_TYPE_SIGNATURES.items():
324:         if col in headers:
325:             id_column = col
326:             metadata = meta
327:             logger.info(f"\nDetected object type: {meta['abbreviation']} ({meta['full_name']})")
328:             logger.info(f"ID column: {col}")
329:             break
330:     
331:     if id_column is None:
332:         logger.warning("Could not detect object type from headers")
333:         return
334:     
335:     # Count rows and show sample
336:     with open(path, 'r', encoding='utf-8-sig') as f:
337:         reader = csv.reader(f)
338:         next(reader)  # Skip header
339:         
340:         tip_idx = 0
341:         id_idx = headers.index(id_column) if id_column in headers else -1
342:         date_idx = -1
343:         for i, h in enumerate(headers):
344:             if h.lower() == 'date':
345:                 date_idx = i
346:                 break
347:         
348:         row_count = 0
349:         sample_rows = []
350:         
351:         for row in reader:
352:             if row and row[tip_idx].strip():
353:                 row_count += 1
354:                 if len(sample_rows) < 3:
355:                     sample_rows.append({
356:                         'tip': row[tip_idx][:32] + '...',
357:                         'id': row[id_idx] if id_idx >= 0 and len(row) > id_idx else None,
358:                         'date': row[date_idx] if date_idx >= 0 and len(row) > date_idx else None
359:                     })
360:     
361:     logger.info(f"\nTotal data rows: {row_count}")
362:     logger.info("\nSample rows:")
363:     for i, sample in enumerate(sample_rows, 1):
364:         logger.info(f"  Row {i}: tip={sample['tip']}, id={sample['id']}, date={sample['date']}")
365: 
366: 
367: def main():
368:     parser = argparse.ArgumentParser(description='Test SFTP TIP Downloader functions')
369:     parser.add_argument('--csv-file', type=str, help='Path to real CSV file to test')
370:     parser.add_argument('--skip-synthetic', action='store_true', help='Skip synthetic tests')
371:     
372:     args = parser.parse_args()
373:     
374:     logger.info("SFTP TIP Downloader Test Suite")
375:     logger.info("=" * 60)
376:     
377:     if not args.skip_synthetic:
378:         test_object_type_detection()
379:         print()
380:         
381:         test_date_parsing()
382:         print()
383:         
384:         test_tip_extraction()
385:         print()
386:     
387:     if args.csv_file:
388:         test_with_real_file(args.csv_file)
389:     
390:     logger.info("\nTest suite complete")
391: 
392: 
393: if __name__ == "__main__":
394:     main()
</file>

<file path="test_systemd_service.sh">
 1: #!/bin/bash
 2: 
 3: echo "Testing Noggin Processor Systemd Service"
 4: echo "========================================="
 5: echo ""
 6: 
 7: echo "1. Checking if service file exists..."
 8: if [ -f /etc/systemd/system/noggin-processor.service ]; then
 9:     echo "    Service file found"
10: else
11:     echo "    Service file not found at /etc/systemd/system/noggin-processor.service"
12:     exit 1
13: fi
14: 
15: echo ""
16: echo "2. Reloading systemd daemon..."
17: sudo systemctl daemon-reload
18: echo "    Daemon reloaded"
19: 
20: echo ""
21: echo "3. Checking service status..."
22: sudo systemctl status noggin-processor --no-pager
23: 
24: echo ""
25: echo "4. Starting service..."
26: sudo systemctl start noggin-processor
27: sleep 3
28: 
29: echo ""
30: echo "5. Checking if service is active..."
31: if sudo systemctl is-active --quiet noggin-processor; then
32:     echo "    Service is active"
33: else
34:     echo "    Service failed to start"
35:     echo ""
36:     echo "   Recent logs:"
37:     sudo journalctl -u noggin-processor -n 20 --no-pager
38:     exit 1
39: fi
40: 
41: echo ""
42: echo "6. Showing recent logs (last 20 lines)..."
43: sudo journalctl -u noggin-processor -n 20 --no-pager
44: 
45: echo ""
46: echo "7. Service test complete!"
47: echo ""
48: echo "Useful commands:"
49: echo "  ./manage_service.sh status  - Check service status"
50: echo "  ./manage_service.sh logs    - View recent logs"
51: echo "  ./manage_service.sh follow  - Follow logs in real-time"
52: echo "  ./manage_service.sh stop    - Stop the service"
53: echo "  python service_dashboard.py - View processing dashboard"
</file>

<file path="config/sftp_config.ini">
 1: # SFTP Configuration for Noggin TIP Downloader
 2: 
 3: [sftp]
 4: hostname = ssh.noggin-sftp.goldstartransport.com.au
 5: port = 18765
 6: username = u824-zdigcggtoza6
 7: private_key_path = /home/noggin_admin/scripts/.ssh/noggin/noggin-openssh.pem
 8: host_key_fingerprint = ssh-ed25519 255 Tc8ZNyPlk1EGa6u/DPp7UsJR1lhaw4rxb8IP1IWOCVM
 9: remote_directory = /home/customer/sftp/tip
10: connection_timeout = 30
11: 
12: [paths]
13: incoming_directory = /mnt/data/noggin/etl/sftp/incoming
14: processed_directory = /mnt/data/noggin/etl/sftp/processed
15: quarantine_directory = /mnt/data/noggin/etl/sftp/quarantine
16: monthly_archive_directory = /mnt/data/noggin/etl/sftp/monthly_archives
17: tip_audit_directory = /mnt/data/noggin/etl/sftp/tip_audit
18: 
19: [processing]
20: write_audit_csv = true # Write audit CSV file as fallback (in addition to database)
21: insert_to_database = true # insert tip into db
22: delete_from_sftp_after_archive = false
23: 
24: [archiving]
25: archive_retention_days = 90
26: compress_after_days = 30
27: 
28: [logging]
29: main_log_pattern = sftp_downloader_{date}.log
30: error_log_pattern = sftp_downloader_errors_{date}.log
31: warning_log_pattern = sftp_downloader_warnings_{date}.log
</file>

<file path="processors/field_processor.py">
  1: """
  2: Field Processor Module
  3: 
  4: Handles config-driven field extraction from API responses and database operations.
  5: Uses field mappings from object-type-specific config files to:
  6: - Extract values from API JSON response
  7: - Resolve hash fields to human-readable values
  8: - Insert/update noggin_data records dynamically
  9: """
 10: 
 11: from __future__ import annotations
 12: import json
 13: import logging
 14: from datetime import datetime
 15: from typing import Optional, Dict, Any, Tuple, List
 16: 
 17: logger: logging.Logger = logging.getLogger(__name__)
 18: 
 19: 
 20: class FieldProcessor:
 21:     """
 22:     Processes API response fields based on config-driven mappings
 23:     
 24:     Field mapping format in config:
 25:         api_field = db_column:field_type[:hash_type]
 26:         
 27:     Field types:
 28:         - string: Direct string value
 29:         - datetime: Parse as ISO datetime
 30:         - bool: Boolean value
 31:         - int: Integer value
 32:         - float: Float value
 33:         - hash: Hash value that needs resolution (requires hash_type)
 34:         - json: Store as JSON
 35:     """
 36:     
 37:     def __init__(self, config: 'ConfigLoader', hash_manager: 'HashManager') -> None:
 38:         self.config = config
 39:         self.hash_manager = hash_manager
 40:         
 41:         # Load object type info
 42:         obj_config = config.get_object_type_config()
 43:         self.object_type: str = obj_config['object_type']
 44:         self.abbreviation: str = obj_config['abbreviation']
 45:         self.id_field: str = obj_config['id_field'].split(':')[0]  # e.g., "lcdInspectionId"
 46:         
 47:         # Parse field mappings
 48:         self.field_mappings: Dict[str, Tuple[str, str, Optional[str]]] = config.get_field_mappings()
 49:         
 50:         logger.info(f"FieldProcessor initialised for {self.abbreviation} with {len(self.field_mappings)} field mappings")
 51:     
 52:     def extract_inspection_id(self, response_data: Dict[str, Any]) -> Optional[str]:
 53:         """Extract the inspection ID from response"""
 54:         return response_data.get(self.id_field)
 55:     
 56:     def extract_date(self, response_data: Dict[str, Any]) -> Tuple[Optional[str], Optional[datetime]]:
 57:         """
 58:         Extract date string and parsed datetime from response
 59:         
 60:         Returns:
 61:             Tuple of (date_string, parsed_datetime)
 62:         """
 63:         date_str: Optional[str] = response_data.get('date')
 64:         parsed_date: Optional[datetime] = None
 65:         
 66:         if date_str:
 67:             try:
 68:                 parsed_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 69:             except (ValueError, AttributeError):
 70:                 logger.warning(f"Could not parse date: {date_str}")
 71:         
 72:         return date_str, parsed_date
 73:     
 74:     def process_field(self, api_field: str, value: Any, tip_value: str, 
 75:                      inspection_id: str) -> Tuple[Any, Optional[str]]:
 76:         """
 77:         Process a single field value based on its type
 78:         
 79:         Args:
 80:             api_field: Field name in API response
 81:             value: Raw value from API
 82:             tip_value: TIP for hash lookup logging
 83:             inspection_id: Inspection ID for hash lookup logging
 84:             
 85:         Returns:
 86:             Tuple of (processed_value, resolved_hash_value or None)
 87:         """
 88:         if api_field not in self.field_mappings:
 89:             return value, None
 90:         
 91:         db_column, field_type, hash_type = self.field_mappings[api_field]
 92:         
 93:         if value is None:
 94:             return None, None
 95:         
 96:         if field_type == 'string':
 97:             return str(value) if value else None, None
 98:         
 99:         elif field_type == 'datetime':
100:             if isinstance(value, str):
101:                 try:
102:                     return datetime.fromisoformat(value.replace('Z', '+00:00')), None
103:                 except (ValueError, AttributeError):
104:                     return None, None
105:             return value, None
106:         
107:         elif field_type == 'bool':
108:             if isinstance(value, bool):
109:                 return value, None
110:             if isinstance(value, str):
111:                 return value.lower() in ('true', 'yes', '1'), None
112:             return bool(value), None
113:         
114:         elif field_type == 'int':
115:             try:
116:                 return int(value), None
117:             except (ValueError, TypeError):
118:                 return None, None
119:         
120:         elif field_type == 'float':
121:             try:
122:                 return float(value), None
123:             except (ValueError, TypeError):
124:                 return None, None
125:         
126:         elif field_type == 'hash':
127:             # Hash field - resolve to human-readable value
128:             if not value or not hash_type:
129:                 return value, None
130:             
131:             hash_value: str = str(value)
132:             resolved: str = self.hash_manager.lookup_hash(
133:                 hash_type, hash_value, tip_value, inspection_id
134:             )
135:             self.hash_manager.update_lookup_type_if_unknown(hash_value, hash_type)
136:             
137:             return hash_value, resolved
138:         
139:         elif field_type == 'json':
140:             if isinstance(value, (dict, list)):
141:                 return json.dumps(value), None
142:             return str(value), None
143:         
144:         else:
145:             return value, None
146:     
147:     def extract_all_fields(self, response_data: Dict[str, Any], 
148:                           tip_value: str) -> Dict[str, Any]:
149:         """
150:         Extract all mapped fields from API response
151:         
152:         Returns:
153:             Dictionary with:
154:             - All mapped fields with processed values
155:             - Hash fields have both raw (_hash) and resolved values
156:             - has_unknown_hashes: bool indicating if any hashes unresolved
157:         """
158:         inspection_id = self.extract_inspection_id(response_data) or 'unknown'
159:         result: Dict[str, Any] = {
160:             'tip': tip_value,
161:             'object_type': self.object_type,
162:             'inspection_id': inspection_id,
163:         }
164:         
165:         # Track unknown hashes
166:         unknown_hashes: List[str] = []
167:         
168:         # Process date separately (always extract)
169:         date_str, parsed_date = self.extract_date(response_data)
170:         result['inspection_date'] = parsed_date
171:         result['date_str'] = date_str
172:         
173:         # Process all mapped fields
174:         for api_field, (db_column, field_type, hash_type) in self.field_mappings.items():
175:             value = response_data.get(api_field)
176:             
177:             processed_value, resolved_value = self.process_field(
178:                 api_field, value, tip_value, inspection_id
179:             )
180:             
181:             if field_type == 'hash':
182:                 # Store both hash and resolved value
183:                 result[f"{db_column}"] = processed_value  # The hash
184:                 resolved_column = db_column.replace('_hash', '')
185:                 result[resolved_column] = resolved_value
186:                 
187:                 # Check if unresolved
188:                 if resolved_value and resolved_value.startswith('Unknown'):
189:                     unknown_hashes.append(api_field)
190:             else:
191:                 result[db_column] = processed_value
192:         
193:         result['has_unknown_hashes'] = len(unknown_hashes) > 0
194:         result['unknown_hash_fields'] = unknown_hashes
195:         
196:         return result
197:     
198:     def extract_meta_fields(self, response_data: Dict[str, Any]) -> Dict[str, Any]:
199:         """Extract $meta fields from API response"""
200:         meta: Dict[str, Any] = response_data.get('$meta', {})
201:         
202:         result: Dict[str, Any] = {
203:             'api_meta_raw': json.dumps(meta),
204:             'api_payload_raw': json.dumps(response_data),
205:             'raw_json': json.dumps(response_data),
206:         }
207:         
208:         # Parse meta dates
209:         if meta.get('createdDate'):
210:             try:
211:                 result['api_meta_created_date'] = datetime.fromisoformat(
212:                     meta['createdDate'].replace('Z', '+00:00')
213:                 )
214:             except (ValueError, AttributeError):
215:                 result['api_meta_created_date'] = None
216:         
217:         if meta.get('modifiedDate'):
218:             try:
219:                 result['api_meta_modified_date'] = datetime.fromisoformat(
220:                     meta['modifiedDate'].replace('Z', '+00:00')
221:                 )
222:             except (ValueError, AttributeError):
223:                 result['api_meta_modified_date'] = None
224:         
225:         # Other meta fields
226:         result['api_meta_security'] = meta.get('security')
227:         result['api_meta_type'] = meta.get('type')
228:         result['api_meta_tip'] = meta.get('tip')
229:         result['api_meta_sid'] = meta.get('sid')
230:         result['api_meta_branch'] = meta.get('branch')
231:         result['api_meta_parent'] = meta.get('parent')
232:         result['api_meta_errors'] = json.dumps(meta.get('errors', []))
233:         
234:         return result
235: 
236: 
237: class DatabaseRecordManager:
238:     """Manages database record operations for processed inspections"""
239:     
240:     # Core columns that exist for all object types
241:     CORE_COLUMNS = [
242:         'tip', 'object_type', 'inspection_date', 'processing_status',
243:         'has_unknown_hashes', 'total_attachments', 'csv_imported_at',
244:         'api_meta_created_date', 'api_meta_modified_date',
245:         'api_meta_security', 'api_meta_type', 'api_meta_tip',
246:         'api_meta_sid', 'api_meta_branch', 'api_meta_parent',
247:         'api_meta_errors', 'api_meta_raw', 'api_payload_raw', 'raw_json'
248:     ]
249:     
250:     def __init__(self, db_manager: 'DatabaseConnectionManager', 
251:                  field_processor: FieldProcessor) -> None:
252:         self.db_manager = db_manager
253:         self.field_processor = field_processor
254:         
255:         # Build list of columns we'll use from field mappings
256:         self.mapped_columns: List[str] = []
257:         for api_field, (db_column, field_type, hash_type) in field_processor.field_mappings.items():
258:             self.mapped_columns.append(db_column)
259:             if field_type == 'hash':
260:                 # Also add the resolved column
261:                 resolved_col = db_column.replace('_hash', '')
262:                 if resolved_col != db_column:
263:                     self.mapped_columns.append(resolved_col)
264:     
265:     def insert_or_update_record(self, response_data: Dict[str, Any], 
266:                                 tip_value: str) -> None:
267:         """Insert or update noggin_data record with API response"""
268:         
269:         # Extract all fields
270:         fields = self.field_processor.extract_all_fields(response_data, tip_value)
271:         meta_fields = self.field_processor.extract_meta_fields(response_data)
272:         
273:         # Merge fields
274:         all_fields = {**fields, **meta_fields}
275:         all_fields['processing_status'] = 'api_success'
276:         all_fields['total_attachments'] = len(response_data.get('attachments', []))
277:         
278:         # Build dynamic INSERT query
279:         columns = []
280:         values = []
281:         update_clauses = []
282:         
283:         for col in self.CORE_COLUMNS:
284:             if col in all_fields:
285:                 columns.append(col)
286:                 values.append(all_fields[col])
287:                 if col != 'tip':  # Don't update primary key
288:                     update_clauses.append(f"{col} = EXCLUDED.{col}")
289:         
290:         for col in self.mapped_columns:
291:             if col in all_fields and col not in columns:
292:                 columns.append(col)
293:                 values.append(all_fields[col])
294:                 update_clauses.append(f"{col} = EXCLUDED.{col}")
295:         
296:         # Add inspection_id column (varies by object type)
297:         inspection_id = fields.get('inspection_id')
298:         id_column = self._get_id_column()
299:         if id_column and id_column not in columns:
300:             columns.append(id_column)
301:             values.append(inspection_id)
302:             update_clauses.append(f"{id_column} = EXCLUDED.{id_column}")
303:         
304:         update_clauses.append("updated_at = CURRENT_TIMESTAMP")
305:         
306:         placeholders = ', '.join(['%s'] * len(values))
307:         column_str = ', '.join(columns)
308:         update_str = ', '.join(update_clauses)
309:         
310:         query = f"""
311:             INSERT INTO noggin_data ({column_str})
312:             VALUES ({placeholders})
313:             ON CONFLICT (tip) DO UPDATE SET {update_str}
314:         """
315:         
316:         try:
317:             self.db_manager.execute_update(query, tuple(values))
318:             logger.debug(f"Inserted/updated noggin_data record for TIP {tip_value}")
319:         except Exception as e:
320:             logger.error(f"Failed to insert/update record for TIP {tip_value}: {e}")
321:             raise
322:     
323:     def _get_id_column(self) -> str:
324:         """Get the inspection ID column name for this object type"""
325:         # Map abbreviation to ID column
326:         id_columns = {
327:             'LCD': 'lcd_inspection_id',
328:             'LCS': 'lcs_inspection_id',
329:             'CCC': 'coupling_id',
330:             'FPI': 'forklift_inspection_id',
331:             'SO': 'site_observation_id',
332:             'TA': 'trailer_audit_id',
333:         }
334:         return id_columns.get(self.field_processor.abbreviation, 'inspection_id')
335:     
336:     def update_processing_status(self, tip_value: str, status: str,
337:                                  error_message: Optional[str] = None) -> None:
338:         """Update processing status for a TIP"""
339:         if error_message:
340:             self.db_manager.execute_update(
341:                 """
342:                 UPDATE noggin_data 
343:                 SET processing_status = %s, last_error_message = %s, updated_at = CURRENT_TIMESTAMP
344:                 WHERE tip = %s
345:                 """,
346:                 (status, error_message, tip_value)
347:             )
348:         else:
349:             self.db_manager.execute_update(
350:                 """
351:                 UPDATE noggin_data 
352:                 SET processing_status = %s, updated_at = CURRENT_TIMESTAMP
353:                 WHERE tip = %s
354:                 """,
355:                 (status, tip_value)
356:             )
357:     
358:     def update_attachment_counts(self, tip_value: str, total: int, 
359:                                  completed: int, all_complete: bool) -> None:
360:         """Update attachment counts for a TIP"""
361:         self.db_manager.execute_update(
362:             """
363:             UPDATE noggin_data 
364:             SET total_attachments = %s, 
365:                 completed_attachment_count = %s,
366:                 all_attachments_complete = %s,
367:                 updated_at = CURRENT_TIMESTAMP
368:             WHERE tip = %s
369:             """,
370:             (total, completed, all_complete, tip_value)
371:         )
372:     
373:     def record_processing_error(self, tip_value: str, error_type: str,
374:                                error_message: str, error_details: Optional[Dict] = None) -> None:
375:         """Record a processing error"""
376:         self.db_manager.execute_update(
377:             """
378:             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
379:             VALUES (%s, %s, %s, %s)
380:             """,
381:             (tip_value, error_type, error_message, json.dumps(error_details or {}))
382:         )
383:     
384:     def get_tips_to_process(self, object_type: str, limit: int = 10) -> List[Dict[str, Any]]:
385:         """Get TIPs eligible for processing"""
386:         return self.db_manager.execute_query_dict(
387:             """
388:             SELECT tip, retry_count, processing_status
389:             FROM noggin_data
390:             WHERE object_type = %s
391:               AND processing_status IN ('pending', 'api_error', 'partial', 'failed')
392:               AND (next_retry_at IS NULL OR next_retry_at <= CURRENT_TIMESTAMP)
393:               AND permanently_failed = FALSE
394:             ORDER BY 
395:                 CASE processing_status
396:                     WHEN 'pending' THEN 1
397:                     WHEN 'partial' THEN 2
398:                     WHEN 'api_error' THEN 3
399:                     WHEN 'failed' THEN 4
400:                 END,
401:                 csv_imported_at ASC
402:             LIMIT %s
403:             """,
404:             (object_type, limit)
405:         )
406:     
407:     def mark_permanently_failed(self, tip_value: str, reason: str) -> None:
408:         """Mark a TIP as permanently failed"""
409:         self.db_manager.execute_update(
410:             """
411:             UPDATE noggin_data 
412:             SET permanently_failed = TRUE,
413:                 processing_status = 'permanently_failed',
414:                 last_error_message = %s,
415:                 updated_at = CURRENT_TIMESTAMP
416:             WHERE tip = %s
417:             """,
418:             (reason, tip_value)
419:         )
420:     
421:     def update_retry_info(self, tip_value: str, retry_count: int, 
422:                          next_retry_at: datetime) -> None:
423:         """Update retry information for a TIP"""
424:         self.db_manager.execute_update(
425:             """
426:             UPDATE noggin_data 
427:             SET retry_count = %s,
428:                 next_retry_at = %s,
429:                 last_retry_at = CURRENT_TIMESTAMP,
430:                 updated_at = CURRENT_TIMESTAMP
431:             WHERE tip = %s
432:             """,
433:             (retry_count, next_retry_at, tip_value)
434:         )
</file>

<file path="web/templates/base.html">
  1: <!DOCTYPE html>
  2: <html lang="en">
  3: <head>
  4:     <meta charset="UTF-8">
  5:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
  6:     <title>{% block title %}Noggin Processor{% endblock %}</title>
  7:     <style>
  8:         * {
  9:             margin: 0;
 10:             padding: 0;
 11:             box-sizing: border-box;
 12:         }
 13:         
 14:         body {
 15:             font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
 16:             background: #f5f5f5;
 17:             color: #333;
 18:         }
 19:         
 20:         .header {
 21:             background: #2c3e50;
 22:             color: white;
 23:             padding: 1rem 2rem;
 24:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
 25:         }
 26:         
 27:         .header h1 {
 28:             font-size: 1.5rem;
 29:             font-weight: 500;
 30:         }
 31:         
 32:         .nav {
 33:             background: #34495e;
 34:             padding: 0.5rem 2rem;
 35:         }
 36:         
 37:         .nav a {
 38:             color: white;
 39:             text-decoration: none;
 40:             padding: 0.5rem 1rem;
 41:             display: inline-block;
 42:             transition: background 0.2s;
 43:         }
 44:         
 45:         .nav a:hover {
 46:             background: #2c3e50;
 47:         }
 48:         
 49:         .nav a.active {
 50:             background: #2c3e50;
 51:             border-bottom: 2px solid #3498db;
 52:         }
 53:         
 54:         .container {
 55:             max-width: 1400px;
 56:             margin: 2rem auto;
 57:             padding: 0 2rem;
 58:         }
 59:         
 60:         .card {
 61:             background: white;
 62:             border-radius: 8px;
 63:             padding: 1.5rem;
 64:             margin-bottom: 1.5rem;
 65:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
 66:         }
 67:         
 68:         .card h2 {
 69:             margin-bottom: 1rem;
 70:             color: #2c3e50;
 71:             font-size: 1.3rem;
 72:         }
 73:         
 74:         .stats-grid {
 75:             display: grid;
 76:             grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
 77:             gap: 1rem;
 78:             margin-bottom: 2rem;
 79:         }
 80:         
 81:         .stat-card {
 82:             background: white;
 83:             padding: 1.5rem;
 84:             border-radius: 8px;
 85:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
 86:             text-align: center;
 87:         }
 88:         
 89:         .stat-card .number {
 90:             font-size: 2.5rem;
 91:             font-weight: bold;
 92:             color: #3498db;
 93:             margin: 0.5rem 0;
 94:         }
 95:         
 96:         .stat-card .label {
 97:             color: #7f8c8d;
 98:             font-size: 0.9rem;
 99:             text-transform: uppercase;
100:             letter-spacing: 0.5px;
101:         }
102:         
103:         .stat-card.success .number { color: #27ae60; }
104:         .stat-card.warning .number { color: #f39c12; }
105:         .stat-card.danger .number { color: #e74c3c; }
106:         .stat-card.info .number { color: #3498db; }
107:         .stat-card.secondary .number { color: #95a5a6; }
108:         
109:         table {
110:             width: 100%;
111:             border-collapse: collapse;
112:         }
113:         
114:         table th {
115:             background: #ecf0f1;
116:             padding: 0.75rem;
117:             text-align: left;
118:             font-weight: 600;
119:             color: #2c3e50;
120:             border-bottom: 2px solid #bdc3c7;
121:         }
122:         
123:         table td {
124:             padding: 0.75rem;
125:             border-bottom: 1px solid #ecf0f1;
126:         }
127:         
128:         table tr:hover {
129:             background: #f8f9fa;
130:         }
131:         
132:         .badge {
133:             display: inline-block;
134:             padding: 0.25rem 0.75rem;
135:             border-radius: 12px;
136:             font-size: 0.85rem;
137:             font-weight: 500;
138:         }
139:         
140:         .badge.success { background: #d4edda; color: #155724; }
141:         .badge.warning { background: #fff3cd; color: #856404; }
142:         .badge.danger { background: #f8d7da; color: #721c24; }
143:         .badge.info { background: #d1ecf1; color: #0c5460; }
144:         .badge.secondary { background: #e2e3e5; color: #383d41; }
145:         
146:         .btn {
147:             display: inline-block;
148:             padding: 0.5rem 1rem;
149:             background: #3498db;
150:             color: white;
151:             text-decoration: none;
152:             border-radius: 4px;
153:             border: none;
154:             cursor: pointer;
155:             transition: background 0.2s;
156:             font-size: 0.9rem;
157:         }
158:         
159:         .btn:hover {
160:             background: #2980b9;
161:         }
162:         
163:         .btn.danger {
164:             background: #e74c3c;
165:         }
166:         
167:         .btn.danger:hover {
168:             background: #c0392b;
169:         }
170:         
171:         .btn.success {
172:             background: #27ae60;
173:         }
174:         
175:         .btn.success:hover {
176:             background: #229954;
177:         }
178:         
179:         .pagination {
180:             display: flex;
181:             justify-content: center;
182:             gap: 0.5rem;
183:             margin-top: 2rem;
184:         }
185:         
186:         .pagination a {
187:             padding: 0.5rem 1rem;
188:             background: white;
189:             border: 1px solid #ddd;
190:             text-decoration: none;
191:             color: #333;
192:             border-radius: 4px;
193:         }
194:         
195:         .pagination a.active {
196:             background: #3498db;
197:             color: white;
198:             border-color: #3498db;
199:         }
200:         
201:         .filter-form {
202:             display: flex;
203:             gap: 1rem;
204:             margin-bottom: 1.5rem;
205:         }
206:         
207:         .filter-form input,
208:         .filter-form select {
209:             padding: 0.5rem;
210:             border: 1px solid #ddd;
211:             border-radius: 4px;
212:             font-size: 1rem;
213:         }
214:         
215:         .filter-form input {
216:             flex: 1;
217:         }
218:     </style>
219:     {% block extra_css %}{% endblock %}
220: </head>
221: <body>
222:     <div class="header">
223:         <h1> Noggin Data Processor</h1>
224:     </div>
225:     
226:     <nav class="nav">
227:         <a href="{{ url_for('index') }}" class="{% if request.endpoint == 'index' %}active{% endif %}">Dashboard</a>
228:         <a href="{{ url_for('inspections') }}" class="{% if request.endpoint == 'inspections' %}active{% endif %}">Inspections</a>
229:         <a href="{{ url_for('hashes') }}" class="{% if request.endpoint == 'hashes' %}active{% endif %}">Hashes</a>
230:         <a href="{{ url_for('service_status') }}" class="{% if request.endpoint == 'service_status' %}active{% endif %}">Service Status</a>
231:     </nav>
232:     
233:     <div class="container">
234:         {% block content %}{% endblock %}
235:     </div>
236:     
237:     {% block extra_js %}{% endblock %}
238: </body>
239: </html>
</file>

<file path="web/templates/dashboard.html">
 1: {% extends "base.html" %}
 2: 
 3: {% block title %}Dashboard - Noggin Processor{% endblock %}
 4: 
 5: {% block content %}
 6: <div class="stats-grid">
 7:     {% set status_mapping = {
 8:         'complete': ('success', ''),
 9:         'pending': ('info', ''),
10:         'failed': ('danger', ''),
11:         'partial': ('warning', ''),
12:         'api_failed': ('danger', ''),
13:         'interrupted': ('warning', '')
14:     } %}
15:     
16:     {% for stat in stats %}
17:         {% set status_class, icon = status_mapping.get(stat.processing_status, ('secondary', '')) %}
18:         <div class="stat-card {{ status_class }}">
19:             <div class="label">{{ icon }} {{ stat.processing_status|upper }}</div>
20:             <div class="number">{{ stat.count }}</div>
21:         </div>
22:     {% endfor %}
23: </div>
24: 
25: <div class="card">
26:     <h2>Today's Activity</h2>
27:     <div class="stats-grid">
28:         <div class="stat-card">
29:             <div class="label">Total Processed</div>
30:             <div class="number">{{ today_stats.total_today or 0 }}</div>
31:         </div>
32:         <div class="stat-card success">
33:             <div class="label">Completed</div>
34:             <div class="number">{{ today_stats.completed_today or 0 }}</div>
35:         </div>
36:         <div class="stat-card">
37:             <div class="label">Success Rate</div>
38:             <div class="number">
39:                 {% if today_stats.total_today and today_stats.total_today > 0 %}
40:                     {{ "%.1f"|format((today_stats.completed_today / today_stats.total_today) * 100) }}%
41:                 {% else %}
42:                     0%
43:                 {% endif %}
44:             </div>
45:         </div>
46:     </div>
47: </div>
48: 
49: <div class="card">
50:     <h2>Recent Activity</h2>
51:     <table>
52:         <thead>
53:             <tr>
54:                 <th>LCD Inspection ID</th>
55:                 <th>Date</th>
56:                 <th>Status</th>
57:                 <th>Attachments</th>
58:                 <th>Updated</th>
59:                 <th>Actions</th>
60:             </tr>
61:         </thead>
62:         <tbody>
63:             {% for item in recent %}
64:             <tr>
65:                 <td>{{ item.noggin_reference or 'N/A' }}</td>
66:                 <td>{{ item.inspection_date.strftime('%Y-%m-%d') if item.inspection_date else 'N/A' }}</td>
67:                 <td>
68:                     {% set status_class = {
69:                         'complete': 'success',
70:                         'pending': 'info',
71:                         'failed': 'danger',
72:                         'partial': 'warning'
73:                     }.get(item.processing_status, 'secondary') %}
74:                     <span class="badge {{ status_class }}">{{ item.processing_status }}</span>
75:                 </td>
76:                 <td>{{ item.completed_attachment_count or 0 }} / {{ item.total_attachments or 0 }}</td>
77:                 <td>{{ item.updated_at.strftime('%Y-%m-%d %H:%M') if item.updated_at else 'N/A' }}</td>
78:                 <td>
79:                     <a href="{{ url_for('inspection_detail', tip=item.tip) }}" class="btn">View</a>
80:                 </td>
81:             </tr>
82:             {% endfor %}
83:         </tbody>
84:     </table>
85: </div>
86: {% endblock %}
</file>

<file path="web/templates/inspections.html">
 1: {% extends "base.html" %}
 2: 
 3: {% block title %}Inspections - Noggin Processor{% endblock %}
 4: 
 5: {% block content %}
 6: <div class="card">
 7:     <h2>Inspections</h2>
 8:     
 9:     <form class="filter-form" method="get">
10:         <input type="text" name="search" placeholder="Search LCD ID, vehicle, trailer..." value="{{ search }}">
11:         <select name="status">
12:             <option value="">All Statuses</option>
13:             <option value="complete" {% if status == 'complete' %}selected{% endif %}>Complete</option>
14:             <option value="pending" {% if status == 'pending' %}selected{% endif %}>Pending</option>
15:             <option value="failed" {% if status == 'failed' %}selected{% endif %}>Failed</option>
16:             <option value="partial" {% if status == 'partial' %}selected{% endif %}>Partial</option>
17:         </select>
18:         <button type="submit" class="btn">Filter</button>
19:     </form>
20:     
21:     <p>Showing {{ inspections|length }} of {{ total }} inspections</p>
22:     
23:     <table>
24:         <thead>
25:             <tr>
26:                 <th>LCD ID</th>
27:                 <th>Date</th>
28:                 <th>Vehicle</th>
29:                 <th>Trailer</th>
30:                 <th>Department</th>
31:                 <th>Status</th>
32:                 <th>Attachments</th>
33:                 <th>Actions</th>
34:             </tr>
35:         </thead>
36:         <tbody>
37:             {% for inspection in inspections %}
38:             <tr>
39:                 <td>{{ inspection.noggin_reference or 'N/A' }}</td>
40:                 <td>{{ inspection.inspection_date.strftime('%Y-%m-%d') if inspection.inspection_date else 'N/A' }}</td>
41:                 <td>{{ inspection.vehicle or 'N/A' }}</td>
42:                 <td>{{ inspection.trailer or 'N/A' }}</td>
43:                 <td>{{ inspection.department or 'N/A' }}</td>
44:                 <td>
45:                     {% set status_class = {
46:                         'complete': 'success',
47:                         'pending': 'info',
48:                         'failed': 'danger',
49:                         'partial': 'warning'
50:                     }.get(inspection.processing_status, 'secondary') %}
51:                     <span class="badge {{ status_class }}">{{ inspection.processing_status }}</span>
52:                 </td>
53:                 <td>{{ inspection.completed_attachment_count or 0 }} / {{ inspection.total_attachments or 0 }}</td>
54:                 <td>
55:                     <a href="{{ url_for('inspection_detail', tip=inspection.tip) }}" class="btn">View</a>
56:                 </td>
57:             </tr>
58:             {% endfor %}
59:         </tbody>
60:     </table>
61:     
62:     {% if total_pages > 1 %}
63:     <div class="pagination">
64:         {% if page > 1 %}
65:             <a href="?page={{ page - 1 }}&status={{ status }}&search={{ search }}"> Previous</a>
66:         {% endif %}
67:         
68:         {% for p in range(1, total_pages + 1) %}
69:             {% if p == page %}
70:                 <a href="#" class="active">{{ p }}</a>
71:             {% elif p <= 3 or p > total_pages - 3 or (p >= page - 2 and p <= page + 2) %}
72:                 <a href="?page={{ p }}&status={{ status }}&search={{ search }}">{{ p }}</a>
73:             {% elif p == 4 or p == total_pages - 3 %}
74:                 <span>...</span>
75:             {% endif %}
76:         {% endfor %}
77:         
78:         {% if page < total_pages %}
79:             <a href="?page={{ page + 1 }}&status={{ status }}&search={{ search }}">Next </a>
80:         {% endif %}
81:     </div>
82:     {% endif %}
83: </div>
84: {% endblock %}
</file>

<file path="web/web_config.md">
  1: For Your Noggin Web Interface
  2: You have three options:
  3: Option 1: Use Existing Web Server (Recommended)
  4: If you have Apache or Nginx already running:
  5: bash# Check which one you have
  6: systemctl status apache2
  7: # or
  8: systemctl status nginx
  9: Advantages:
 10: 
 11: Already configured and running
 12: Can host multiple applications
 13: Better performance for production
 14: 
 15: Option 2: Use Flask's Built-in Server (Development Only)
 16: bash# Simple, but not for production
 17: python app.py
 18: # Runs on http://localhost:5000
 19: Advantages:
 20: 
 21: Quick setup
 22: No web server configuration needed
 23: 
 24: Disadvantages:
 25: 
 26: Not secure for production
 27: Single-threaded
 28: No SSL by default
 29: 
 30: Option 3: Use Gunicorn + Nginx (Production Recommended)
 31: bashpip install gunicorn
 32: gunicorn -w 4 -b 0.0.0.0:5000 app:app
 33: Advantages:
 34: 
 35: Production-ready
 36: Multiple workers
 37: Better performance
 38: 
 39: 
 40: Quick Detection Commands
 41: Run these to get instant answers:
 42: bash# What's on port 80?
 43: sudo lsof -i :80 | awk 'NR>1 {print $1}' | uniq
 44: 
 45: # What's on port 443?
 46: sudo lsof -i :443 | awk 'NR>1 {print $1}' | uniq
 47: 
 48: # Show me everything web-related
 49: sudo netstat -tlnp | grep -E ':(80|443|5050|8080|10000)' | \
 50:     awk '{print $4 " -> " $7}' | \
 51:     sed 's/.*:/Port /' | \
 52:     sed 's/\// -> /'
 53: ```
 54: 
 55: ---
 56: 
 57: ## Expected Output Examples
 58: 
 59: ### If Apache is Running:
 60: ```
 61: Port 80 -> apache2
 62: Port 443 -> apache2
 63: ```
 64: 
 65: ### If Nginx is Running:
 66: ```
 67: Port 80 -> nginx
 68: Port 443 -> nginx
 69: ```
 70: 
 71: ### If Webmin Only:
 72: ```
 73: Port 10000 -> perl (webmin)
 74: 
 75: Next Steps
 76: Once you run the detection commands and tell me what you find, I can help you:
 77: 
 78: Configure existing web server to host Noggin interface
 79: Set up new Flask application on appropriate port
 80: Integrate with Apache/Nginx using reverse proxy
 81: Secure with SSL (Let's Encrypt)
 82: Set up authentication for web access
 83: 
 84: Run the detection script above and share the output, then we'll proceed with the best approach for your setup!RetryPGPorts in use:
 85: 0.0.0.0:10000 0.0.0.0:* 
 86: [::]:10000 [::]:* 
 87: *:80 *:*Apache:
 88: Installed: Server version: Apache/2.4.58 (Ubuntu)
 89: active
 90: Status: RUNNINGNginx:
 91: Not installedWebmin:
 92: active
 93: Status: RUNNING on port 10000
 94: Access: https://192.168.0.236:10000pgAdmin:
 95: Not runningActive web listeners:
 96: perl *:10000
 97: perl *:10000
 98: apache2 *:80
 99: apache2 *:80
100: apache2 *:80can we setup port 443? i've tried to setup a certificate with webminPerfect! You have Apache running on port 80 and Webmin on port 10000. Let's set up SSL (port 443) and create a proper web interface for your Noggin system.
101: 
102: Stage 12: Web Interface with SSL
103: Step 1: Enable SSL on Apache
104: bash# Enable SSL module
105: sudo a2enmod ssl
106: 
107: # Enable headers module (for security)
108: sudo a2enmod headers
109: 
110: # Enable proxy modules (for Flask/Python apps)
111: sudo a2enmod proxy
112: sudo a2enmod proxy_http
113: 
114: # Restart Apache
115: sudo systemctl restart apache2
116: 
117: # Verify port 443 is listening
118: sudo ss -tlnp | grep :443
119: 
120: Step 2: Generate Self-Signed Certificate (For Testing)
121: bash# Create directory for certificates
122: sudo mkdir -p /etc/apache2/ssl
123: 
124: # Generate self-signed certificate (valid for 1 year)
125: sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/apache2/ssl/noggin.key -out /etc/apache2/ssl/noggin.crt -subj "/C=AU/ST=WA/L=Perth/O=GoldstarTransport/OU=IT/CN=gs-sv-011.internal.goldstargroup.au"
126: 
127: # Set proper permissions
128: sudo chmod 600 /etc/apache2/ssl/noggin.key
129: sudo chmod 644 /etc/apache2/ssl/noggin.crt
130: Note: For production, use Let's Encrypt (we'll set that up after testing)
131: 
132: Step 3: Create Flask Web Application
133: Create /home/noggin_admin/scripts/web/app.py:
134: python
135: from flask import Flask, render_template, jsonify, request, redirect, url_for, flash
136: from flask_httpauth import HTTPBasicAuth
137: from werkzeug.security import generate_password_hash, check_password_hash
138: from datetime import datetime, timedelta
139: from pathlib import Path
140: import sys
141: 
142: # Add parent directory to path
143: sys.path.insert(0, str(Path(__file__).parent.parent))
144: 
145: from common import ConfigLoader, DatabaseConnectionManager, HashManager
146: 
147: app = Flask(__name__)
148: app.secret_key = 'your-secret-key-change-this'  # CHANGE THIS!
149: 
150: auth = HTTPBasicAuth()
151: 
152: # Initialize
153: config = ConfigLoader(
154:     '../config/base_config.ini',
155:     '../config/load_compliance_check_driver_loader_config.ini'
156: )
157: db_manager = DatabaseConnectionManager(config)
158: hash_manager = HashManager(config, db_manager)
159: 
160: # Users (in production, use database)
161: users = {
162:     "admin": generate_password_hash("admin123")  # CHANGE THIS!
163: }
164: 
165: @auth.verify_password
166: def verify_password(username, password):
167:     if username in users and check_password_hash(users.get(username), password):
168:         return username
169: 
170: @app.route('/')
171: @auth.login_required
172: def index():
173:     """Dashboard home page"""
174:     try:
175:         # Get statistics
176:         stats_query = """
177:             SELECT 
178:                 processing_status,
179:                 COUNT(*) as count
180:             FROM noggin_data
181:             GROUP BY processing_status
182:         """
183:         stats = db_manager.execute_query_dict(stats_query)
184:         
185:         # Today's activity
186:         today_query = """
187:             SELECT 
188:                 COUNT(*) as total_today,
189:                 SUM(CASE WHEN processing_status = 'complete' THEN 1 ELSE 0 END) as completed_today
190:             FROM noggin_data
191:             WHERE updated_at >= CURRENT_DATE
192:         """
193:         today_stats = db_manager.execute_query_dict(today_query)[0]
194:         
195:         # Recent activity
196:         recent_query = """
197:             SELECT 
198:                 tip,
199:                 lcd_inspection_id,
200:                 inspection_date,
201:                 processing_status,
202:                 total_attachments,
203:                 completed_attachment_count,
204:                 updated_at
205:             FROM noggin_data
206:             ORDER BY updated_at DESC
207:             LIMIT 20
208:         """
209:         recent = db_manager.execute_query_dict(recent_query)
210:         
211:         return render_template(
212:             'dashboard.html',
213:             stats=stats,
214:             today_stats=today_stats,
215:             recent=recent,
216:             current_time=datetime.now()
217:         )
218:     except Exception as e:
219:         return f"Error: {e}", 500
220: 
221: @app.route('/api/stats')
222: @auth.login_required
223: def api_stats():
224:     """API endpoint for statistics"""
225:     try:
226:         stats_query = """
227:             SELECT 
228:                 processing_status,
229:                 COUNT(*) as count
230:             FROM noggin_data
231:             GROUP BY processing_status
232:         """
233:         stats = db_manager.execute_query_dict(stats_query)
234:         
235:         return jsonify({
236:             'success': True,
237:             'stats': stats,
238:             'timestamp': datetime.now().isoformat()
239:         })
240:     except Exception as e:
241:         return jsonify({
242:             'success': False,
243:             'error': str(e)
244:         }), 500
245: 
246: @app.route('/inspections')
247: @auth.login_required
248: def inspections():
249:     """List all inspections"""
250:     page = request.args.get('page', 1, type=int)
251:     per_page = 50
252:     offset = (page - 1) * per_page
253:     
254:     # Get filter parameters
255:     status = request.args.get('status', '')
256:     search = request.args.get('search', '')
257:     
258:     # Build query
259:     where_clauses = []
260:     params = []
261:     
262:     if status:
263:         where_clauses.append("processing_status = %s")
264:         params.append(status)
265:     
266:     if search:
267:         where_clauses.append("(lcd_inspection_id ILIKE %s OR vehicle ILIKE %s OR trailer ILIKE %s)")
268:         params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
269:     
270:     where_sql = " WHERE " + " AND ".join(where_clauses) if where_clauses else ""
271:     
272:     # Get total count
273:     count_query = f"SELECT COUNT(*) as total FROM noggin_data{where_sql}"
274:     total = db_manager.execute_query_dict(count_query, tuple(params))[0]['total']
275:     
276:     # Get inspections
277:     params.extend([per_page, offset])
278:     inspections_query = f"""
279:         SELECT 
280:             tip,
281:             lcd_inspection_id,
282:             inspection_date,
283:             vehicle,
284:             trailer,
285:             department,
286:             team,
287:             processing_status,
288:             total_attachments,
289:             completed_attachment_count,
290:             retry_count,
291:             updated_at
292:         FROM noggin_data
293:         {where_sql}
294:         ORDER BY updated_at DESC
295:         LIMIT %s OFFSET %s
296:     """
297:     inspections_list = db_manager.execute_query_dict(inspections_query, tuple(params))
298:     
299:     total_pages = (total + per_page - 1) // per_page
300:     
301:     return render_template(
302:         'inspections.html',
303:         inspections=inspections_list,
304:         page=page,
305:         total_pages=total_pages,
306:         total=total,
307:         status=status,
308:         search=search
309:     )
310: 
311: @app.route('/inspection/<tip>')
312: @auth.login_required
313: def inspection_detail(tip):
314:     """Inspection detail page"""
315:     try:
316:         # Get inspection
317:         inspection_query = """
318:             SELECT * FROM noggin_data WHERE tip = %s
319:         """
320:         inspection = db_manager.execute_query_dict(inspection_query, (tip,))
321:         
322:         if not inspection:
323:             return "Inspection not found", 404
324:         
325:         inspection = inspection[0]
326:         
327:         # Get attachments
328:         attachments_query = """
329:             SELECT 
330:                 filename,
331:                 file_path,
332:                 attachment_status,
333:                 file_size_bytes,
334:                 download_duration_seconds,
335:                 attachment_validation_status
336:             FROM attachments
337:             WHERE record_tip = %s
338:             ORDER BY attachment_sequence
339:         """
340:         attachments = db_manager.execute_query_dict(attachments_query, (tip,))
341:         
342:         # Get errors
343:         errors_query = """
344:             SELECT 
345:                 error_type,
346:                 error_message,
347:                 created_at
348:             FROM processing_errors
349:             WHERE tip = %s
350:             ORDER BY created_at DESC
351:             LIMIT 10
352:         """
353:         errors = db_manager.execute_query_dict(errors_query, (tip,))
354:         
355:         return render_template(
356:             'inspection_detail.html',
357:             inspection=inspection,
358:             attachments=attachments,
359:             errors=errors
360:         )
361:     except Exception as e:
362:         return f"Error: {e}", 500
363: 
364: @app.route('/hashes')
365: @auth.login_required
366: def hashes():
367:     """Hash management page"""
368:     try:
369:         stats = hash_manager.get_hash_statistics()
370:         
371:         # Get unknown hashes count by type
372:         unknown_query = """
373:             SELECT 
374:                 object_type,
375:                 COUNT(*) as count
376:             FROM unknown_hashes
377:             WHERE resolved_at IS NULL
378:             GROUP BY object_type
379:         """
380:         unknown_counts = db_manager.execute_query_dict(unknown_query)
381:         
382:         return render_template(
383:             'hashes.html',
384:             stats=stats,
385:             unknown_counts=unknown_counts
386:         )
387:     except Exception as e:
388:         return f"Error: {e}", 500
389: 
390: @app.route('/service-status')
391: @auth.login_required
392: def service_status():
393:     """Service status page"""
394:     import subprocess
395:     
396:     try:
397:         # Check service status
398:         result = subprocess.run(
399:             ['systemctl', 'is-active', 'noggin-processor'],
400:             capture_output=True,
401:             text=True
402:         )
403:         service_active = result.stdout.strip() == 'active'
404:         
405:         # Get recent logs
406:         log_result = subprocess.run(
407:             ['journalctl', '-u', 'noggin-processor', '-n', '50', '--no-pager'],
408:             capture_output=True,
409:             text=True
410:         )
411:         recent_logs = log_result.stdout
412:         
413:         return render_template(
414:             'service_status.html',
415:             service_active=service_active,
416:             recent_logs=recent_logs
417:         )
418:     except Exception as e:
419:         return f"Error: {e}", 500
420: 
421: if __name__ == '__main__':
422:     app.run(host='0.0.0.0', port=5000, debug=True)
423: 
424: Step 4: Create Templates
425: Create directory structure:
426: bash
427: mkdir -p ~/scripts/web/templates
428: mkdir -p ~/scripts/web/static/css
429: Create ~/scripts/web/templates/base.html:
430: html
431: <!DOCTYPE html>
432: <html lang="en">
433: <head>
434:     <meta charset="UTF-8">
435:     <meta name="viewport" content="width=device-width, initial-scale=1.0">
436:     <title>{% block title %}Noggin Processor{% endblock %}</title>
437:     <style>
438:         * {
439:             margin: 0;
440:             padding: 0;
441:             box-sizing: border-box;
442:         }
443:         
444:         body {
445:             font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
446:             background: #f5f5f5;
447:             color: #333;
448:         }
449:         
450:         .header {
451:             background: #2c3e50;
452:             color: white;
453:             padding: 1rem 2rem;
454:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
455:         }
456:         
457:         .header h1 {
458:             font-size: 1.5rem;
459:             font-weight: 500;
460:         }
461:         
462:         .nav {
463:             background: #34495e;
464:             padding: 0.5rem 2rem;
465:         }
466:         
467:         .nav a {
468:             color: white;
469:             text-decoration: none;
470:             padding: 0.5rem 1rem;
471:             display: inline-block;
472:             transition: background 0.2s;
473:         }
474:         
475:         .nav a:hover {
476:             background: #2c3e50;
477:         }
478:         
479:         .nav a.active {
480:             background: #2c3e50;
481:             border-bottom: 2px solid #3498db;
482:         }
483:         
484:         .container {
485:             max-width: 1400px;
486:             margin: 2rem auto;
487:             padding: 0 2rem;
488:         }
489:         
490:         .card {
491:             background: white;
492:             border-radius: 8px;
493:             padding: 1.5rem;
494:             margin-bottom: 1.5rem;
495:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
496:         }
497:         
498:         .card h2 {
499:             margin-bottom: 1rem;
500:             color: #2c3e50;
501:             font-size: 1.3rem;
502:         }
503:         
504:         .stats-grid {
505:             display: grid;
506:             grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
507:             gap: 1rem;
508:             margin-bottom: 2rem;
509:         }
510:         
511:         .stat-card {
512:             background: white;
513:             padding: 1.5rem;
514:             border-radius: 8px;
515:             box-shadow: 0 2px 4px rgba(0,0,0,0.1);
516:             text-align: center;
517:         }
518:         
519:         .stat-card .number {
520:             font-size: 2.5rem;
521:             font-weight: bold;
522:             color: #3498db;
523:             margin: 0.5rem 0;
524:         }
525:         
526:         .stat-card .label {
527:             color: #7f8c8d;
528:             font-size: 0.9rem;
529:             text-transform: uppercase;
530:             letter-spacing: 0.5px;
531:         }
532:         
533:         .stat-card.success .number { color: #27ae60; }
534:         .stat-card.warning .number { color: #f39c12; }
535:         .stat-card.danger .number { color: #e74c3c; }
536:         
537:         table {
538:             width: 100%;
539:             border-collapse: collapse;
540:         }
541:         
542:         table th {
543:             background: #ecf0f1;
544:             padding: 0.75rem;
545:             text-align: left;
546:             font-weight: 600;
547:             color: #2c3e50;
548:             border-bottom: 2px solid #bdc3c7;
549:         }
550:         
551:         table td {
552:             padding: 0.75rem;
553:             border-bottom: 1px solid #ecf0f1;
554:         }
555:         
556:         table tr:hover {
557:             background: #f8f9fa;
558:         }
559:         
560:         .badge {
561:             display: inline-block;
562:             padding: 0.25rem 0.75rem;
563:             border-radius: 12px;
564:             font-size: 0.85rem;
565:             font-weight: 500;
566:         }
567:         
568:         .badge.success { background: #d4edda; color: #155724; }
569:         .badge.warning { background: #fff3cd; color: #856404; }
570:         .badge.danger { background: #f8d7da; color: #721c24; }
571:         .badge.info { background: #d1ecf1; color: #0c5460; }
572:         .badge.secondary { background: #e2e3e5; color: #383d41; }
573:         
574:         .btn {
575:             display: inline-block;
576:             padding: 0.5rem 1rem;
577:             background: #3498db;
578:             color: white;
579:             text-decoration: none;
580:             border-radius: 4px;
581:             border: none;
582:             cursor: pointer;
583:             transition: background 0.2s;
584:         }
585:         
586:         .btn:hover {
587:             background: #2980b9;
588:         }
589:         
590:         .btn.danger {
591:             background: #e74c3c;
592:         }
593:         
594:         .btn.danger:hover {
595:             background: #c0392b;
596:         }
597:         
598:         .pagination {
599:             display: flex;
600:             justify-content: center;
601:             gap: 0.5rem;
602:             margin-top: 2rem;
603:         }
604:         
605:         .pagination a {
606:             padding: 0.5rem 1rem;
607:             background: white;
608:             border: 1px solid #ddd;
609:             text-decoration: none;
610:             color: #333;
611:             border-radius: 4px;
612:         }
613:         
614:         .pagination a.active {
615:             background: #3498db;
616:             color: white;
617:             border-color: #3498db;
618:         }
619:         
620:         .filter-form {
621:             display: flex;
622:             gap: 1rem;
623:             margin-bottom: 1.5rem;
624:         }
625:         
626:         .filter-form input,
627:         .filter-form select {
628:             padding: 0.5rem;
629:             border: 1px solid #ddd;
630:             border-radius: 4px;
631:             font-size: 1rem;
632:         }
633:         
634:         .filter-form input {
635:             flex: 1;
636:         }
637:     </style>
638:     {% block extra_css %}{% endblock %}
639: </head>
640: <body>
641:     <div class="header">
642:         <h1> Noggin Data Processor</h1>
643:     </div>
644:     
645:     <nav class="nav">
646:         <a href="{{ url_for('index') }}" class="{% if request.endpoint == 'index' %}active{% endif %}">Dashboard</a>
647:         <a href="{{ url_for('inspections') }}" class="{% if request.endpoint == 'inspections' %}active{% endif %}">Inspections</a>
648:         <a href="{{ url_for('hashes') }}" class="{% if request.endpoint == 'hashes' %}active{% endif %}">Hashes</a>
649:         <a href="{{ url_for('service_status') }}" class="{% if request.endpoint == 'service_status' %}active{% endif %}">Service Status</a>
650:     </nav>
651:     
652:     <div class="container">
653:         {% block content %}{% endblock %}
654:     </div>
655:     
656:     {% block extra_js %}{% endblock %}
657: </body>
658: </html>
659: Create ~/scripts/web/templates/dashboard.html:
660: html{% extends "base.html" %}
661: 
662: {% block title %}Dashboard - Noggin Processor{% endblock %}
663: 
664: {% block content %}
665: <div class="stats-grid">
666:     {% set status_mapping = {
667:         'complete': ('success', ''),
668:         'pending': ('info', ''),
669:         'failed': ('danger', ''),
670:         'partial': ('warning', ''),
671:         'api_failed': ('danger', ''),
672:         'interrupted': ('warning', '')
673:     } %}
674:     
675:     {% for stat in stats %}
676:         {% set status_class, icon = status_mapping.get(stat.processing_status, ('secondary', '')) %}
677:         <div class="stat-card {{ status_class }}">
678:             <div class="label">{{ icon }} {{ stat.processing_status|upper }}</div>
679:             <div class="number">{{ stat.count }}</div>
680:         </div>
681:     {% endfor %}
682: </div>
683: 
684: <div class="card">
685:     <h2>Today's Activity</h2>
686:     <div class="stats-grid">
687:         <div class="stat-card">
688:             <div class="label">Total Processed</div>
689:             <div class="number">{{ today_stats.total_today or 0 }}</div>
690:         </div>
691:         <div class="stat-card success">
692:             <div class="label">Completed</div>
693:             <div class="number">{{ today_stats.completed_today or 0 }}</div>
694:         </div>
695:         <div class="stat-card">
696:             <div class="label">Success Rate</div>
697:             <div class="number">
698:                 {% if today_stats.total_today and today_stats.total_today > 0 %}
699:                     {{ "%.1f"|format((today_stats.completed_today / today_stats.total_today) * 100) }}%
700:                 {% else %}
701:                     0%
702:                 {% endif %}
703:             </div>
704:         </div>
705:     </div>
706: </div>
707: 
708: <div class="card">
709:     <h2>Recent Activity</h2>
710:     <table>
711:         <thead>
712:             <tr>
713:                 <th>LCD Inspection ID</th>
714:                 <th>Date</th>
715:                 <th>Status</th>
716:                 <th>Attachments</th>
717:                 <th>Updated</th>
718:                 <th>Actions</th>
719:             </tr>
720:         </thead>
721:         <tbody>
722:             {% for item in recent %}
723:             <tr>
724:                 <td>{{ item.lcd_inspection_id or 'N/A' }}</td>
725:                 <td>{{ item.inspection_date.strftime('%Y-%m-%d') if item.inspection_date else 'N/A' }}</td>
726:                 <td>
727:                     {% set status_class = {
728:                         'complete': 'success',
729:                         'pending': 'info',
730:                         'failed': 'danger',
731:                         'partial': 'warning'
732:                     }.get(item.processing_status, 'secondary') %}
733:                     <span class="badge {{ status_class }}">{{ item.processing_status }}</span>
734:                 </td>
735:                 <td>{{ item.completed_attachment_count or 0 }} / {{ item.total_attachments or 0 }}</td>
736:                 <td>{{ item.updated_at.strftime('%Y-%m-%d %H:%M') if item.updated_at else 'N/A' }}</td>
737:                 <td>
738:                     <a href="{{ url_for('inspection_detail', tip=item.tip) }}" class="btn">View</a>
739:                 </td>
740:             </tr>
741:             {% endfor %}
742:         </tbody>
743:     </table>
744: </div>
745: {% endblock %}
746: 
747: Step 5: Install Flask Dependencies
748: bash
749: cd ~/scripts
750: source .venv/bin/activate
751: 
752: pip install flask flask-httpauth gunicorn
753: 
754: Step 6: Configure Apache Virtual Host with SSL
755: Create /etc/apache2/sites-available/noggin-ssl.conf:
756: apache
757: <VirtualHost *:443>
758:     ServerName gs-sv-011.internal.goldstargroup.au
759:     ServerAlias 192.168.0.236
760: 
761:     # SSL Configuration
762:     SSLEngine on
763:     SSLCertificateFile /etc/apache2/ssl/noggin.crt
764:     SSLCertificateKeyFile /etc/apache2/ssl/noggin.key
765: 
766:     # Security headers
767:     Header always set Strict-Transport-Security "max-age=31536000; includeSubDomains"
768:     Header always set X-Frame-Options "SAMEORIGIN"
769:     Header always set X-Content-Type-Options "nosniff"
770: 
771:     # Proxy to Flask app
772:     ProxyPreserveHost On
773:     ProxyPass / http://127.0.0.1:5000/
774:     ProxyPassReverse / http://127.0.0.1:5000/
775: 
776:     # Logs
777:     ErrorLog ${APACHE_LOG_DIR}/noggin-error.log
778:     CustomLog ${APACHE_LOG_DIR}/noggin-access.log combined
779: </VirtualHost>
780: 
781: # Redirect HTTP to HTTPS
782: <VirtualHost *:80>
783:     ServerName gs-sv-011.internal.goldstargroup.au
784:     ServerAlias 192.168.0.236
785:     
786:     Redirect permanent / https://192.168.0.236/
787: </VirtualHost>
788: Enable the site:
789: bash
790: sudo a2ensite noggin-ssl
791: sudo systemctl reload apache2
792: 
793: Step 7: Create Systemd Service for Flask App
794: Create /etc/systemd/system/noggin-web.service:
795: ini
796: [Unit]
797: Description=Noggin Web Interface
798: After=network.target postgresql.service
799: 
800: [Service]
801: Type=notify
802: User=noggin_admin
803: Group=noggin_admin
804: WorkingDirectory=/home/noggin_admin/scripts/web
805: Environment="PATH=/home/noggin_admin/scripts/.venv/bin"
806: ExecStart=/home/noggin_admin/scripts/.venv/bin/gunicorn \
807:     --workers 4 \
808:     --bind 127.0.0.1:5000 \
809:     --timeout 120 \
810:     --access-logfile /mnt/data/noggin/log/web-access.log \
811:     --error-logfile /mnt/data/noggin/log/web-error.log \
812:     app:app
813: 
814: Restart=always
815: RestartSec=10
816: 
817: [Install]
818: WantedBy=multi-user.target
819: Enable and start:
820: bash
821: sudo systemctl daemon-reload
822: sudo systemctl enable noggin-web
823: sudo systemctl start noggin-web
824: sudo systemctl status noggin-web
825: 
826: Step 8: Test the Setup
827: bash# Check if Flask app is running
828: curl http://127.0.0.1:5000
829: 
830: # Check if Apache is proxying correctly
831: curl -k https://192.168.0.236/
832: 
833: # Check port 443 is listening
834: sudo ss -tlnp | grep :443
835: 
836: Step 9: Access Web Interface
837: Open browser:
838: 
839: HTTPS: https://192.168.0.236/
840: HTTP: http://192.168.0.236/ (redirects to HTTPS)
841: 
842: Login:
843: 
844: Username: admin
845: Password: admin123 (CHANGE THIS!)
846: 
847: 
848: You should now see your Noggin dashboard! Let me know if you hit any issues and we'll troubleshoot together.
849: Would you like me to continue with the remaining templates (inspections.html, inspection_detail.html, etc.) in the next message?
</file>

<file path="noggin_continuous_processor_modular.py">
  1: """
  2: Noggin Continuous Processor (Modular Version)
  3: 
  4: Runs all object type processors in a round-robin fashion with configurable intervals.
  5: Includes CSV import, hash resolution, and SFTP download cycles.
  6: 
  7: Uses the new modular processors package instead of subprocess calls.
  8: """
  9: 
 10: from __future__ import annotations
 11: import sys
 12: import time
 13: import signal
 14: import logging
 15: from datetime import datetime
 16: from pathlib import Path
 17: from typing import Dict, Any, Optional, List
 18: 
 19: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, CSVImporter, HashManager
 20: from common.object_types import get_all_object_types, load_object_types
 21: 
 22: logger: logging.Logger = logging.getLogger(__name__)
 23: 
 24: shutdown_requested: bool = False
 25: 
 26: 
 27: def signal_handler(signum: int, frame: Any) -> None:
 28:     """Handle shutdown signals gracefully"""
 29:     global shutdown_requested
 30:     logger.info(f"Received signal {signum}. Initiating graceful shutdown...")
 31:     shutdown_requested = True
 32: 
 33: 
 34: # Map abbreviations to config files
 35: OBJECT_TYPE_CONFIGS = {
 36:     'LCD': 'config/load_compliance_check_driver_loader_config.ini',
 37:     'LCS': 'config/load_compliance_check_supervisor_manager_config.ini',
 38:     'CCC': 'config/coupling_compliance_check_config.ini',
 39:     'FPI': 'config/forklift_prestart_inspection_config.ini',
 40:     'SO': 'config/site_observations_config.ini',
 41:     'TA': 'config/trailer_audits_config.ini',
 42: }
 43: 
 44: 
 45: class ContinuousProcessor:
 46:     """Manages continuous processing of all object types"""
 47:     
 48:     def __init__(self, base_config_path: str = 'config/base_config.ini') -> None:
 49:         self.base_config_path = base_config_path
 50:         
 51:         self.config = ConfigLoader(base_config_path)
 52:         
 53:         self.logger_manager = LoggerManager(self.config, script_name='noggin_continuous')
 54:         self.logger_manager.configure_application_logger()
 55:         
 56:         self.db_manager = DatabaseConnectionManager(self.config)
 57:         
 58:         self.hash_manager = HashManager(self.config, self.db_manager)
 59:         
 60:         self.sleep_interval = self.config.getint('continuous', 'sleep_between_cycles', fallback=60)
 61:         self.tips_per_type = self.config.getint('continuous', 'tips_per_type_per_cycle', fallback=10)
 62:         self.csv_import_frequency = self.config.getint('continuous', 'csv_import_every_n_cycles', fallback=5)
 63:         self.sftp_download_frequency = self.config.getint('continuous', 'sftp_download_every_n_cycles', fallback=6)
 64:         self.sftp_enabled = self.config.getboolean('sftp', 'enabled', fallback=False)
 65:         
 66:         self.enabled_types = self._get_enabled_types()
 67:         
 68:         self.cycle_count = 0
 69:         self.stats: Dict[str, Dict[str, int]] = {
 70:             abbrev: {'processed': 0, 'errors': 0}
 71:             for abbrev in self.enabled_types
 72:         }
 73:         
 74:         logger.info(f"ContinuousProcessor initialised with {len(self.enabled_types)} object types")
 75:         logger.info(f"Enabled types: {', '.join(self.enabled_types)}")
 76:     
 77:     def _get_enabled_types(self) -> List[str]:
 78:         """Get list of enabled object types from config or defaults"""
 79:         enabled_str = self.config.get('continuous', 'enabled_object_types', fallback='')
 80:         
 81:         if enabled_str:
 82:             return [t.strip().upper() for t in enabled_str.split(',') if t.strip()]
 83:         
 84:         enabled = []
 85:         for abbrev, config_path in OBJECT_TYPE_CONFIGS.items():
 86:             if Path(config_path).exists():
 87:                 enabled.append(abbrev)
 88:         
 89:         return enabled
 90:     
 91:     def run(self) -> None:
 92:         """Main continuous processing loop"""
 93:         global shutdown_requested
 94:         
 95:         signal.signal(signal.SIGINT, signal_handler)
 96:         signal.signal(signal.SIGTERM, signal_handler)
 97:         
 98:         logger.info("=" * 80)
 99:         logger.info("NOGGIN CONTINUOUS PROCESSOR STARTED")
100:         logger.info(f"Sleep interval: {self.sleep_interval} seconds")
101:         logger.info(f"TIPs per type per cycle: {self.tips_per_type}")
102:         logger.info(f"CSV import every {self.csv_import_frequency} cycles")
103:         if self.sftp_enabled:
104:             logger.info(f"SFTP download every {self.sftp_download_frequency} cycles")
105:         logger.info("=" * 80)
106:         
107:         while not shutdown_requested:
108:             try:
109:                 self.cycle_count += 1
110:                 cycle_start = datetime.now()
111:                 
112:                 logger.info(f"\n{'='*60}")
113:                 logger.info(f"CYCLE {self.cycle_count} - {cycle_start.strftime('%Y-%m-%d %H:%M:%S')}")
114:                 logger.info(f"{'='*60}")
115:                 
116:                 if self.sftp_enabled and self.cycle_count % self.sftp_download_frequency == 0:
117:                     self._run_sftp_download()
118:                 
119:                 if self.cycle_count % self.csv_import_frequency == 0:
120:                     self._run_csv_import()
121:                 
122:                 for abbrev in self.enabled_types:
123:                     if shutdown_requested:
124:                         break
125:                     
126:                     self._process_object_type(abbrev)
127:                 
128:                 cycle_duration = (datetime.now() - cycle_start).total_seconds()
129:                 logger.info(f"Cycle {self.cycle_count} completed in {cycle_duration:.1f} seconds")
130:                 
131:                 if not shutdown_requested:
132:                     logger.info(f"Sleeping {self.sleep_interval} seconds before next cycle...")
133:                     self._interruptible_sleep(self.sleep_interval)
134:                 
135:             except Exception as e:
136:                 logger.error(f"Error in cycle {self.cycle_count}: {e}", exc_info=True)
137:                 if not shutdown_requested:
138:                     self._interruptible_sleep(30)
139:         
140:         self._log_final_summary()
141:     
142:     def _process_object_type(self, abbrev: str) -> int:
143:         """
144:         Process TIPs for a single object type
145:         
146:         Returns:
147:             Number of TIPs processed
148:         """
149:         config_path = OBJECT_TYPE_CONFIGS.get(abbrev)
150:         
151:         if not config_path or not Path(config_path).exists():
152:             logger.warning(f"Config not found for {abbrev}: {config_path}")
153:             return 0
154:         
155:         try:
156:             from processors import ObjectProcessor
157:             
158:             processor = ObjectProcessor(
159:                 base_config_path=self.base_config_path,
160:                 specific_config_path=config_path
161:             )
162:             
163:             processed = processor.run(
164:                 from_database=True,
165:                 batch_size=self.tips_per_type
166:             )
167:             
168:             self.stats[abbrev]['processed'] += processed
169:             
170:             if processed > 0:
171:                 logger.info(f"{abbrev}: Processed {processed} TIPs")
172:             else:
173:                 logger.debug(f"{abbrev}: No TIPs to process")
174:             
175:             return processed
176:             
177:         except Exception as e:
178:             logger.error(f"Error processing {abbrev}: {e}", exc_info=True)
179:             self.stats[abbrev]['errors'] += 1
180:             return 0
181:     
182:     def _run_csv_import(self) -> None:
183:         """Run CSV import for all object types"""
184:         logger.info("Running CSV import cycle...")
185:         
186:         try:
187:             csv_importer = CSVImporter(self.config, self.db_manager)
188:             
189:             summary = csv_importer.scan_and_import()
190:             
191:             if summary['files_processed'] > 0:
192:                 logger.info(
193:                     f"CSV import: {summary['files_processed']} files, "
194:                     f"{summary['total_imported']} imported, "
195:                     f"{summary['total_duplicates']} duplicates"
196:                 )
197:             
198:         except Exception as e:
199:             logger.error(f"CSV import cycle failed: {e}", exc_info=True)
200:     
201:     def _run_sftp_download(self) -> None:
202:         """Run SFTP download cycle"""
203:         logger.info("Running SFTP download cycle...")
204:         
205:         try:
206:             sftp_script = Path(__file__).parent / 'sftp_download_tips.py'
207:             
208:             if not sftp_script.exists():
209:                 logger.warning(f"SFTP script not found: {sftp_script}")
210:                 return
211:             
212:             import subprocess
213:             
214:             result = subprocess.run(
215:                 [sys.executable, str(sftp_script)],
216:                 capture_output=True,
217:                 text=True,
218:                 timeout=600
219:             )
220:             
221:             if result.returncode == 0:
222:                 logger.info("SFTP download completed successfully")
223:             else:
224:                 logger.error(f"SFTP download failed: {result.stderr[:500] if result.stderr else 'No error output'}")
225:                 
226:         except subprocess.TimeoutExpired:
227:             logger.error("SFTP download timed out after 600 seconds")
228:         except Exception as e:
229:             logger.error(f"SFTP download failed: {e}", exc_info=True)
230:     
231:     def _interruptible_sleep(self, seconds: int) -> None:
232:         """Sleep that can be interrupted by shutdown signal"""
233:         global shutdown_requested
234:         
235:         for _ in range(seconds):
236:             if shutdown_requested:
237:                 return
238:             time.sleep(1)
239:     
240:     def _log_final_summary(self) -> None:
241:         """Log final processing summary"""
242:         logger.info("\n" + "=" * 80)
243:         logger.info("CONTINUOUS PROCESSOR SHUTDOWN")
244:         logger.info("=" * 80)
245:         logger.info(f"Total cycles completed: {self.cycle_count}")
246:         logger.info("")
247:         logger.info("Processing summary by object type:")
248:         
249:         total_processed = 0
250:         total_errors = 0
251:         
252:         for abbrev, stats in self.stats.items():
253:             logger.info(f"  {abbrev}: {stats['processed']} processed, {stats['errors']} errors")
254:             total_processed += stats['processed']
255:             total_errors += stats['errors']
256:         
257:         logger.info("")
258:         logger.info(f"Total TIPs processed: {total_processed}")
259:         logger.info(f"Total errors: {total_errors}")
260:         logger.info("=" * 80)
261:         
262:         try:
263:             self.db_manager.close_all()
264:         except Exception as e:
265:             logger.error(f"Error closing database connections: {e}")
266: 
267: 
268: def main() -> int:
269:     """Main entry point"""
270:     import argparse
271:     
272:     parser = argparse.ArgumentParser(
273:         description='Noggin Continuous Processor (Modular Version)'
274:     )
275:     parser.add_argument(
276:         '--config',
277:         default='config/base_config.ini',
278:         help='Path to base config file'
279:     )
280:     parser.add_argument(
281:         '--once',
282:         action='store_true',
283:         help='Run one cycle and exit'
284:     )
285:     
286:     args = parser.parse_args()
287:     
288:     try:
289:         processor = ContinuousProcessor(args.config)
290:         
291:         if args.once:
292:             processor.cycle_count = 1
293:             for abbrev in processor.enabled_types:
294:                 processor._process_object_type(abbrev)
295:             processor._log_final_summary()
296:         else:
297:             processor.run()
298:         
299:         return 0
300:         
301:     except KeyboardInterrupt:
302:         logger.info("Interrupted by user")
303:         return 0
304:     except Exception as e:
305:         logger.error(f"Fatal error: {e}", exc_info=True)
306:         return 1
307: 
308: 
309: if __name__ == "__main__":
310:     sys.exit(main())
</file>

<file path="noggin_processor_unified.py">
  1: """
  2: Unified Noggin Processor
  3: 
  4: Processes any object type based on command line argument.
  5: This is the recommended script for the continuous processor and manual invocations.
  6: 
  7: Usage:
  8:     python noggin_processor_unified.py LCD                    # Process LCD from default CSV
  9:     python noggin_processor_unified.py CCC --csv tips.csv     # Process CCC from specific CSV
 10:     python noggin_processor_unified.py FPI --database         # Process FPI from database queue
 11:     python noggin_processor_unified.py TA --tip ABC123...     # Process single TA TIP
 12:     
 13: Supported object types:
 14:     LCD - Load Compliance Check (Driver/Loader)
 15:     LCS - Load Compliance Check (Supervisor/Manager)
 16:     CCC - Coupling Compliance Check
 17:     FPI - Forklift Prestart Inspection
 18:     SO  - Site Observations
 19:     TA  - Trailer Audits
 20: """
 21: 
 22: import sys
 23: import argparse
 24: import logging
 25: from pathlib import Path
 26: 
 27: from processors import ObjectProcessor
 28: 
 29: logger = logging.getLogger(__name__)
 30: 
 31: CONFIG_FILES = {
 32:     'LCD': 'config/load_compliance_check_driver_loader_config.ini',
 33:     'LCS': 'config/load_compliance_check_supervisor_manager_config.ini',
 34:     'CCC': 'config/coupling_compliance_check_config.ini',
 35:     'FPI': 'config/forklift_prestart_inspection_config.ini',
 36:     'SO': 'config/site_observations_config.ini',
 37:     'TA': 'config/trailer_audits_config.ini',
 38: }
 39: 
 40: OBJECT_TYPE_NAMES = {
 41:     'LCD': 'Load Compliance Check (Driver/Loader)',
 42:     'LCS': 'Load Compliance Check (Supervisor/Manager)',
 43:     'CCC': 'Coupling Compliance Check',
 44:     'FPI': 'Forklift Prestart Inspection',
 45:     'SO': 'Site Observations',
 46:     'TA': 'Trailer Audits',
 47: }
 48: 
 49: 
 50: def main() -> int:
 51:     parser = argparse.ArgumentParser(
 52:         description='Process Noggin inspection records',
 53:         formatter_class=argparse.RawDescriptionHelpFormatter,
 54:         epilog="""
 55: Object types:
 56:   LCD  Load Compliance Check (Driver/Loader)
 57:   LCS  Load Compliance Check (Supervisor/Manager)
 58:   CCC  Coupling Compliance Check
 59:   FPI  Forklift Prestart Inspection
 60:   SO   Site Observations
 61:   TA   Trailer Audits
 62: 
 63: Examples:
 64:   %(prog)s LCD                         Process LCD from default CSV
 65:   %(prog)s CCC --csv ccc_tips.csv      Process CCC from specific CSV
 66:   %(prog)s FPI --database              Process FPI from database queue
 67:   %(prog)s TA --tip abc123def456...    Process single TA TIP
 68:         """
 69:     )
 70:     
 71:     parser.add_argument(
 72:         'object_type',
 73:         choices=list(CONFIG_FILES.keys()),
 74:         help='Object type abbreviation'
 75:     )
 76:     parser.add_argument(
 77:         '--csv',
 78:         help='Path to CSV file containing TIPs'
 79:     )
 80:     parser.add_argument(
 81:         '--database',
 82:         action='store_true',
 83:         help='Process TIPs from database queue instead of CSV'
 84:     )
 85:     parser.add_argument(
 86:         '--batch-size',
 87:         type=int,
 88:         default=10,
 89:         help='Batch size when processing from database (default: 10)'
 90:     )
 91:     parser.add_argument(
 92:         '--tip',
 93:         help='Process a single TIP'
 94:     )
 95:     parser.add_argument(
 96:         '--base-config',
 97:         default='config/base_config.ini',
 98:         help='Path to base config file (default: config/base_config.ini)'
 99:     )
100:     
101:     args = parser.parse_args()
102:     
103:     specific_config = CONFIG_FILES[args.object_type]
104:     
105:     if not Path(specific_config).exists():
106:         logger.error(f"Config file not found: {specific_config}")
107:         print(f"Error: Config file not found: {specific_config}")
108:         return 1
109:     
110:     if not Path(args.base_config).exists():
111:         logger.error(f"Base config file not found: {args.base_config}")
112:         print(f"Error: Base config file not found: {args.base_config}")
113:         return 1
114:     
115:     try:
116:         processor = ObjectProcessor(
117:             base_config_path=args.base_config,
118:             specific_config_path=specific_config
119:         )
120:         
121:         object_type_name = OBJECT_TYPE_NAMES[args.object_type]
122:         logger.info(f"Starting processor for: {object_type_name}")
123:         
124:         if args.tip:
125:             success = processor.process_single(args.tip)
126:             return 0 if success else 1
127:         
128:         processed = processor.run(
129:             csv_file_path=args.csv,
130:             batch_size=args.batch_size,
131:             from_database=args.database
132:         )
133:         
134:         logger.info(f"Processing complete: {processed} TIPs processed for {args.object_type}")
135:         return 0
136:         
137:     except KeyboardInterrupt:
138:         logger.info("Processing interrupted by user")
139:         return 0
140:         
141:     except Exception as e:
142:         logger.error(f"Processing failed: {e}", exc_info=True)
143:         print(f"Error: {e}")
144:         return 1
145: 
146: 
147: def process_object_type(object_type: str, csv_path: str = None, 
148:                        from_database: bool = False,
149:                        batch_size: int = 10) -> int:
150:     """
151:     Programmatic interface for processing an object type
152:     
153:     Args:
154:         object_type: Abbreviation (LCD, CCC, etc.)
155:         csv_path: Path to CSV file (optional)
156:         from_database: Process from database queue
157:         batch_size: Batch size for database processing
158:         
159:     Returns:
160:         Number of TIPs processed
161:     """
162:     if object_type not in CONFIG_FILES:
163:         raise ValueError(f"Unknown object type: {object_type}. Valid: {list(CONFIG_FILES.keys())}")
164:     
165:     processor = ObjectProcessor(
166:         base_config_path='config/base_config.ini',
167:         specific_config_path=CONFIG_FILES[object_type]
168:     )
169:     
170:     return processor.run(
171:         csv_file_path=csv_path,
172:         batch_size=batch_size,
173:         from_database=from_database
174:     )
175: 
176: 
177: if __name__ == "__main__":
178:     sys.exit(main())
</file>

<file path="sftp_download_tips.py">
  1: """
  2: SFTP TIP Downloader for Noggin Data Extraction System
  3: 
  4: Downloads CSV files from SFTP server, identifies object types,
  5: extracts TIPs, and inserts them into the database for processing.
  6: 
  7: Can be run standalone or called from noggin_continuous_processor.py
  8: """
  9: 
 10: from __future__ import annotations
 11: import csv
 12: import logging
 13: import shutil
 14: import sys
 15: from configparser import ConfigParser
 16: from datetime import datetime
 17: from pathlib import Path
 18: from typing import Dict, List, Optional, Tuple, Any
 19: 
 20: import paramiko
 21: 
 22: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager
 23: 
 24: # Try to import shared object types module, fall back to inline if not available
 25: try:
 26:     from common.object_types import (
 27:         detect_object_type_from_headers,
 28:         ObjectTypeConfig,
 29:         find_column_index as shared_find_column_index
 30:     )
 31:     USE_SHARED_OBJECT_TYPES = True
 32: except ImportError:
 33:     USE_SHARED_OBJECT_TYPES = False
 34: 
 35: logger: logging.Logger = logging.getLogger(__name__)
 36: 
 37: # Fallback object type detection signatures (used if shared module not available)
 38: OBJECT_TYPE_SIGNATURES: Dict[str, Dict[str, str]] = {
 39:     'couplingId': {
 40:         'abbreviation': 'CCC',
 41:         'full_name': 'Coupling Compliance Check',
 42:         'id_prefix': 'C - '
 43:     },
 44:     'forkliftPrestartInspectionId': {
 45:         'abbreviation': 'FPI',
 46:         'full_name': 'Forklift Prestart Inspection',
 47:         'id_prefix': 'FL - Inspection - '
 48:     },
 49:     'lcsInspectionId': {
 50:         'abbreviation': 'LCS',
 51:         'full_name': 'Load Compliance Check Supervisor/Manager',
 52:         'id_prefix': 'LCS - '
 53:     },
 54:     'lcdInspectionId': {
 55:         'abbreviation': 'LCC',
 56:         'full_name': 'Load Compliance Check Driver/Loader',
 57:         'id_prefix': 'LCD - '
 58:     },
 59:     'siteObservationId': {
 60:         'abbreviation': 'SO',
 61:         'full_name': 'Site Observations',
 62:         'id_prefix': 'SO - '
 63:     },
 64:     'trailerAuditId': {
 65:         'abbreviation': 'TA',
 66:         'full_name': 'Trailer Audits',
 67:         'id_prefix': 'TA - '
 68:     }
 69: }
 70: 
 71: 
 72: class SFTPDownloaderError(Exception):
 73:     """Base exception for SFTP downloader errors"""
 74:     pass
 75: 
 76: 
 77: class SFTPConnectionError(SFTPDownloaderError):
 78:     """SFTP connection failed"""
 79:     pass
 80: 
 81: 
 82: class ObjectTypeDetectionError(SFTPDownloaderError):
 83:     """Could not identify object type from CSV"""
 84:     pass
 85: 
 86: 
 87: class SFTPLoggerManager:
 88:     """Manages multiple log files for SFTP operations"""
 89:     
 90:     def __init__(self, config: ConfigParser, log_path: Path) -> None:
 91:         self.config = config
 92:         self.log_path = log_path
 93:         self.log_path.mkdir(parents=True, exist_ok=True)
 94:         
 95:         self._error_logger: Optional[logging.Logger] = None
 96:         self._warning_logger: Optional[logging.Logger] = None
 97:     
 98:     def _build_log_filename(self, pattern: str) -> str:
 99:         now = datetime.now()
100:         return pattern.format(date=now.strftime('%Y%m%d'))
101:     
102:     def configure_sftp_loggers(self) -> None:
103:         """Configure separate error and warning loggers"""
104:         formatter = logging.Formatter(
105:             fmt='%(asctime)s | %(levelname)-8s | %(message)s',
106:             datefmt='%Y-%m-%d %H:%M:%S'
107:         )
108:         
109:         error_pattern = self.config.get('logging', 'error_log_pattern', 
110:                                         fallback='sftp_downloader_errors_{date}.log')
111:         error_filename = self._build_log_filename(error_pattern)
112:         error_file = self.log_path / error_filename
113:         
114:         self._error_logger = logging.getLogger('sftp_errors')
115:         self._error_logger.setLevel(logging.ERROR)
116:         self._error_logger.handlers.clear()
117:         
118:         error_handler = logging.FileHandler(error_file, encoding='utf-8')
119:         error_handler.setLevel(logging.ERROR)
120:         error_handler.setFormatter(formatter)
121:         self._error_logger.addHandler(error_handler)
122:         
123:         warning_pattern = self.config.get('logging', 'warning_log_pattern',
124:                                           fallback='sftp_downloader_warnings_{date}.log')
125:         warning_filename = self._build_log_filename(warning_pattern)
126:         warning_file = self.log_path / warning_filename
127:         
128:         self._warning_logger = logging.getLogger('sftp_warnings')
129:         self._warning_logger.setLevel(logging.WARNING)
130:         self._warning_logger.handlers.clear()
131:         
132:         warning_handler = logging.FileHandler(warning_file, encoding='utf-8')
133:         warning_handler.setLevel(logging.WARNING)
134:         warning_handler.setFormatter(formatter)
135:         self._warning_logger.addHandler(warning_handler)
136:         
137:         logger.info(f"Error log: {error_file}")
138:         logger.info(f"Warning log: {warning_file}")
139:     
140:     @property
141:     def error_logger(self) -> logging.Logger:
142:         if self._error_logger is None:
143:             raise RuntimeError("Loggers not configured. Call configure_sftp_loggers() first.")
144:         return self._error_logger
145:     
146:     @property
147:     def warning_logger(self) -> logging.Logger:
148:         if self._warning_logger is None:
149:             raise RuntimeError("Loggers not configured. Call configure_sftp_loggers() first.")
150:         return self._warning_logger
151: 
152: 
153: def load_sftp_config(config_path: str = 'config/sftp_config.ini') -> ConfigParser:
154:     """Load SFTP configuration from INI file"""
155:     config = ConfigParser()
156:     config_file = Path(config_path)
157:     
158:     if not config_file.exists():
159:         raise FileNotFoundError(f"SFTP config not found: {config_path}")
160:     
161:     config.read(config_file)
162:     return config
163: 
164: 
165: def create_directories(config: ConfigParser) -> Dict[str, Path]:
166:     """Create required directories and return paths"""
167:     paths = {
168:         'incoming': Path(config.get('paths', 'incoming_directory')),
169:         'processed': Path(config.get('paths', 'processed_directory')),
170:         'quarantine': Path(config.get('paths', 'quarantine_directory')),
171:         'monthly_archive': Path(config.get('paths', 'monthly_archive_directory')),
172:         'tip_audit': Path(config.get('paths', 'tip_audit_directory'))
173:     }
174:     
175:     for name, path in paths.items():
176:         path.mkdir(parents=True, exist_ok=True)
177:         logger.debug(f"Directory ready: {name} -> {path}")
178:     
179:     return paths
180: 
181: 
182: def connect_sftp(config: ConfigParser) -> Tuple[paramiko.SSHClient, paramiko.SFTPClient]:
183:     """Establish SFTP connection using private key authentication"""
184:     hostname = config.get('sftp', 'hostname')
185:     port = config.getint('sftp', 'port')
186:     username = config.get('sftp', 'username')
187:     key_path = config.get('sftp', 'private_key_path')
188:     timeout = config.getint('sftp', 'connection_timeout', fallback=30)
189:     
190:     logger.info(f"Connecting to SFTP: {hostname}:{port} as {username}")
191:     
192:     if not Path(key_path).exists():
193:         raise SFTPConnectionError(f"Private key not found: {key_path}")
194:     
195:     try:
196:         ssh_client = paramiko.SSHClient()
197:         ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
198:         
199:         private_key = paramiko.RSAKey.from_private_key_file(key_path)
200:         
201:         ssh_client.connect(
202:             hostname=hostname,
203:             port=port,
204:             username=username,
205:             pkey=private_key,
206:             timeout=timeout,
207:             allow_agent=False,
208:             look_for_keys=False
209:         )
210:         
211:         sftp_client = ssh_client.open_sftp()
212:         logger.info("SFTP connection established")
213:         
214:         return ssh_client, sftp_client
215:         
216:     except paramiko.AuthenticationException as e:
217:         raise SFTPConnectionError(f"Authentication failed: {e}")
218:     except paramiko.SSHException as e:
219:         raise SFTPConnectionError(f"SSH error: {e}")
220:     except Exception as e:
221:         raise SFTPConnectionError(f"Connection failed: {e}")
222: 
223: 
224: def list_csv_files(sftp: paramiko.SFTPClient, remote_dir: str) -> List[Tuple[str, int]]:
225:     """
226:     List CSV files on SFTP server sorted by modification time (oldest first)
227:     
228:     Returns list of tuples: (filename, mtime)
229:     """
230:     try:
231:         sftp.chdir(remote_dir)
232:         files = []
233:         
234:         for entry in sftp.listdir_attr():
235:             if entry.filename.endswith('.csv'):
236:                 files.append((entry.filename, entry.st_mtime))
237:         
238:         # Sort by modification time (oldest first for FIFO processing)
239:         files.sort(key=lambda x: x[1])
240:         
241:         logger.info(f"Found {len(files)} CSV files on SFTP server")
242:         return files
243:         
244:     except Exception as e:
245:         raise SFTPDownloaderError(f"Failed to list remote directory: {e}")
246: 
247: 
248: def download_file(sftp: paramiko.SFTPClient, remote_filename: str, 
249:                   local_path: Path) -> Path:
250:     """Download single file from SFTP to local path"""
251:     local_file = local_path / remote_filename
252:     
253:     try:
254:         sftp.get(remote_filename, str(local_file))
255:         logger.debug(f"Downloaded: {remote_filename}")
256:         return local_file
257:         
258:     except Exception as e:
259:         raise SFTPDownloaderError(f"Failed to download {remote_filename}: {e}")
260: 
261: 
262: def detect_object_type(csv_path: Path) -> Tuple[str, Dict[str, str]]:
263:     """
264:     Detect object type by examining CSV headers
265:     
266:     Returns tuple of (id_column_name, object_type_metadata)
267:     Raises ObjectTypeDetectionError if type cannot be determined
268:     """
269:     try:
270:         with open(csv_path, 'r', newline='', encoding='utf-8-sig') as f:
271:             reader = csv.reader(f)
272:             headers = next(reader)
273:         
274:         headers = [h.strip() for h in headers]
275:         
276:         # Use shared module if available
277:         if USE_SHARED_OBJECT_TYPES:
278:             config = detect_object_type_from_headers(headers)
279:             if config:
280:                 metadata = {
281:                     'abbreviation': config.abbreviation,
282:                     'full_name': config.full_name,
283:                     'id_prefix': config.id_prefix
284:                 }
285:                 logger.debug(f"Detected object type: {config.abbreviation} via column '{config.id_column}'")
286:                 return config.id_column, metadata
287:         else:
288:             # Fallback to inline detection
289:             for id_column, metadata in OBJECT_TYPE_SIGNATURES.items():
290:                 if id_column in headers:
291:                     logger.debug(f"Detected object type: {metadata['abbreviation']} via column '{id_column}'")
292:                     return id_column, metadata
293:         
294:         raise ObjectTypeDetectionError(
295:             f"No known ID column found in headers: {headers[:10]}..."
296:         )
297:         
298:     except ObjectTypeDetectionError:
299:         raise
300:     except Exception as e:
301:         raise ObjectTypeDetectionError(f"Failed to read CSV headers: {e}")
302: 
303: 
304: def find_column_index(headers: List[str], column_name: str) -> int:
305:     """
306:     Find column index by name (case-insensitive, handles whitespace)
307:     Returns -1 if not found
308:     """
309:     clean_headers = [h.strip().lower() for h in headers]
310:     target = column_name.strip().lower()
311:     
312:     try:
313:         return clean_headers.index(target)
314:     except ValueError:
315:         return -1
316: 
317: 
318: def extract_tips_from_csv(csv_path: Path, id_column: str, 
319:                           object_type_meta: Dict[str, str]) -> List[Dict[str, Any]]:
320:     """
321:     Extract TIP data from CSV file
322:     
323:     Returns list of dicts with keys: tip, object_type, inspection_id, inspection_date
324:     """
325:     tips = []
326:     
327:     with open(csv_path, 'r', newline='', encoding='utf-8-sig') as f:
328:         reader = csv.reader(f)
329:         headers = [h.strip() for h in next(reader)]
330:         
331:         # First column is always nogginId (TIP), regardless of header name
332:         tip_index = 0
333:         
334:         id_index = find_column_index(headers, id_column)
335:         date_index = find_column_index(headers, 'date')
336:         
337:         if id_index == -1:
338:             logger.warning(f"ID column '{id_column}' not found in headers")
339:             id_index = None
340:         
341:         if date_index == -1:
342:             logger.warning("Date column not found in headers")
343:             date_index = None
344:         
345:         for row_num, row in enumerate(reader, start=2):
346:             if not row or not row[tip_index].strip():
347:                 continue
348:             
349:             tip_value = row[tip_index].strip()
350:             
351:             inspection_id = None
352:             if id_index is not None and len(row) > id_index:
353:                 inspection_id = row[id_index].strip() or None
354:             
355:             inspection_date = None
356:             if date_index is not None and len(row) > date_index:
357:                 date_str = row[date_index].strip()
358:                 if date_str:
359:                     inspection_date = parse_date(date_str)
360:             
361:             tips.append({
362:                 'tip': tip_value,
363:                 'object_type': object_type_meta['full_name'],
364:                 'abbreviation': object_type_meta['abbreviation'],
365:                 'inspection_id': inspection_id,
366:                 'inspection_date': inspection_date,
367:                 'row_number': row_num
368:             })
369:     
370:     logger.info(f"Extracted {len(tips)} TIPs from {csv_path.name}")
371:     return tips
372: 
373: 
374: def parse_date(date_str: str) -> Optional[str]:
375:     """
376:     Parse date string to ISO format (YYYY-MM-DD)
377:     Handles multiple formats commonly found in Noggin exports
378:     """
379:     formats = [
380:         '%d-%b-%y',      # 16-Jun-25
381:         '%d-%b-%Y',      # 16-Jun-2025
382:         '%d/%m/%Y',      # 16/06/2025
383:         '%d/%m/%y',      # 16/06/25
384:         '%Y-%m-%d',      # 2025-06-16
385:         '%d-%m-%Y',      # 16-06-2025
386:         '%d-%m-%y',      # 16-06-25
387:     ]
388:     
389:     for fmt in formats:
390:         try:
391:             parsed = datetime.strptime(date_str, fmt)
392:             return parsed.strftime('%Y-%m-%d')
393:         except ValueError:
394:             continue
395:     
396:     logger.debug(f"Could not parse date: {date_str}")
397:     return None
398: 
399: 
400: def check_existing_tips(db_manager: DatabaseConnectionManager, 
401:                         tips: List[str]) -> set:
402:     """Check which TIPs already exist in database"""
403:     if not tips:
404:         return set()
405:     
406:     placeholders = ', '.join(['%s'] * len(tips))
407:     query = f"SELECT tip FROM noggin_data WHERE tip IN ({placeholders})"
408:     
409:     results = db_manager.execute_query_dict(query, tuple(tips))
410:     return {row['tip'] for row in results}
411: 
412: 
413: def insert_tips_to_database(db_manager: DatabaseConnectionManager,
414:                             tips_data: List[Dict[str, Any]],
415:                             source_file: str,
416:                             warning_logger: logging.Logger) -> Dict[str, int]:
417:     """
418:     Insert new TIPs into database with pending status
419:     
420:     Uses individual inserts with transaction for rollback on failure.
421:     Skips existing TIPs and logs to warning file.
422:     Falls back to basic insert if new columns don't exist yet.
423:     
424:     Returns dict with counts: inserted, duplicates, errors
425:     """
426:     if not tips_data:
427:         return {'inserted': 0, 'duplicates': 0, 'errors': 0}
428:     
429:     tip_values = [t['tip'] for t in tips_data]
430:     existing_tips = check_existing_tips(db_manager, tip_values)
431:     
432:     inserted = 0
433:     duplicates = 0
434:     errors = 0
435:     
436:     # Try full insert first, fall back to basic if columns missing
437:     insert_query_full = """
438:         INSERT INTO noggin_data (
439:             tip, object_type, processing_status,
440:             expected_inspection_id, expected_inspection_date,
441:             csv_imported_at, source_filename
442:         )
443:         VALUES (%s, %s, 'pending', %s, %s, CURRENT_TIMESTAMP, %s)
444:     """
445:     
446:     insert_query_basic = """
447:         INSERT INTO noggin_data (
448:             tip, object_type, processing_status, csv_imported_at
449:         )
450:         VALUES (%s, %s, 'pending', CURRENT_TIMESTAMP)
451:     """
452:     
453:     use_full_insert = True
454:     
455:     for tip_data in tips_data:
456:         tip_value = tip_data['tip']
457:         
458:         if tip_value in existing_tips:
459:             duplicates += 1
460:             warning_logger.warning(
461:                 f"DUPLICATE TIP skipped | "
462:                 f"tip={tip_value[:16]}... | "
463:                 f"object_type={tip_data['abbreviation']} | "
464:                 f"inspection_id={tip_data['inspection_id']} | "
465:                 f"source={source_file}"
466:             )
467:             continue
468:         
469:         try:
470:             if use_full_insert:
471:                 db_manager.execute_update(
472:                     insert_query_full,
473:                     (
474:                         tip_value,
475:                         tip_data['object_type'],
476:                         tip_data['inspection_id'],
477:                         tip_data['inspection_date'],
478:                         source_file
479:                     )
480:                 )
481:             else:
482:                 db_manager.execute_update(
483:                     insert_query_basic,
484:                     (tip_value, tip_data['object_type'])
485:                 )
486:             inserted += 1
487:             logger.debug(f"Inserted TIP: {tip_value[:16]}... ({tip_data['abbreviation']})")
488:             
489:         except Exception as e:
490:             error_str = str(e).lower()
491:             # Check if error is due to missing columns
492:             if 'column' in error_str and ('expected_inspection' in error_str or 'source_filename' in error_str):
493:                 logger.warning("New columns not found, falling back to basic insert")
494:                 use_full_insert = False
495:                 # Retry with basic insert
496:                 try:
497:                     db_manager.execute_update(
498:                         insert_query_basic,
499:                         (tip_value, tip_data['object_type'])
500:                     )
501:                     inserted += 1
502:                     logger.debug(f"Inserted TIP (basic): {tip_value[:16]}... ({tip_data['abbreviation']})")
503:                 except Exception as e2:
504:                     errors += 1
505:                     logger.error(f"Failed to insert TIP {tip_value[:16]}...: {e2}")
506:             else:
507:                 errors += 1
508:                 logger.error(f"Failed to insert TIP {tip_value[:16]}...: {e}")
509:             
510:         except Exception as e:
511:             errors += 1
512:             logger.error(f"Failed to insert TIP {tip_value[:16]}...: {e}")
513:     
514:     logger.info(
515:         f"Database insert complete: {inserted} inserted, "
516:         f"{duplicates} duplicates, {errors} errors"
517:     )
518:     
519:     return {'inserted': inserted, 'duplicates': duplicates, 'errors': errors}
520: 
521: 
522: def write_audit_csv(tips_data: List[Dict[str, Any]], audit_dir: Path,
523:                     source_file: str) -> Path:
524:     """
525:     Write extracted TIPs to audit CSV file as fallback
526:     
527:     Returns path to created audit file
528:     """
529:     timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
530:     audit_filename = f"tips_{timestamp}_{Path(source_file).stem}.csv"
531:     audit_path = audit_dir / audit_filename
532:     
533:     with open(audit_path, 'w', newline='', encoding='utf-8') as f:
534:         writer = csv.writer(f)
535:         writer.writerow(['tip', 'object_type', 'inspection_id', 'date', 'source_file'])
536:         
537:         for tip_data in tips_data:
538:             writer.writerow([
539:                 tip_data['tip'],
540:                 tip_data['abbreviation'],
541:                 tip_data['inspection_id'] or '',
542:                 tip_data['inspection_date'] or '',
543:                 source_file
544:             ])
545:     
546:     logger.info(f"Audit CSV written: {audit_path}")
547:     return audit_path
548: 
549: 
550: def archive_file(source_path: Path, processed_dir: Path, 
551:                  object_type_abbrev: str) -> Path:
552:     """
553:     Move file to processed directory with timestamp prefix
554:     
555:     Filename format: {ABBREV}_{YYYY-MM-DD}_{HHMMSS}_{original_uuid}.csv
556:     """
557:     timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
558:     
559:     # Extract UUID portion from original filename (e.g., "exported-file-3a2c1734-37c7-4569-8859-2d5e17e8fe6e.csv")
560:     original_stem = source_path.stem
561:     if original_stem.startswith('exported-file-'):
562:         uuid_part = original_stem.replace('exported-file-', '')
563:     else:
564:         uuid_part = original_stem
565:     
566:     archive_filename = f"{object_type_abbrev}_{timestamp}_{uuid_part}.csv"
567:     archive_path = processed_dir / archive_filename
568:     
569:     shutil.move(str(source_path), str(archive_path))
570:     logger.debug(f"Archived: {source_path.name} -> {archive_filename}")
571:     
572:     return archive_path
573: 
574: 
575: def quarantine_file(source_path: Path, quarantine_dir: Path, 
576:                     reason: str, error_logger: logging.Logger) -> Path:
577:     """Move unidentified file to quarantine directory"""
578:     timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')
579:     quarantine_filename = f"QUARANTINE_{timestamp}_{source_path.name}"
580:     quarantine_path = quarantine_dir / quarantine_filename
581:     
582:     shutil.move(str(source_path), str(quarantine_path))
583:     
584:     error_logger.error(
585:         f"FILE QUARANTINED | "
586:         f"original={source_path.name} | "
587:         f"reason={reason} | "
588:         f"quarantine_path={quarantine_path}"
589:     )
590:     
591:     logger.warning(f"File quarantined: {source_path.name} - {reason}")
592:     
593:     return quarantine_path
594: 
595: 
596: def delete_remote_file(sftp: paramiko.SFTPClient, filename: str,
597:                        files_to_delete: List[str]) -> None:
598:     """Add file to deletion queue (actual deletion happens after all processing)"""
599:     files_to_delete.append(filename)
600:     logger.debug(f"Queued for deletion: {filename}")
601: 
602: 
603: def execute_remote_deletions(sftp: paramiko.SFTPClient, 
604:                              files_to_delete: List[str]) -> int:
605:     """Delete all queued files from SFTP server"""
606:     deleted = 0
607:     
608:     for filename in files_to_delete:
609:         try:
610:             sftp.remove(filename)
611:             deleted += 1
612:             logger.debug(f"Deleted from SFTP: {filename}")
613:         except Exception as e:
614:             logger.error(f"Failed to delete {filename} from SFTP: {e}")
615:     
616:     if deleted > 0:
617:         logger.info(f"Deleted {deleted} files from SFTP server")
618:     
619:     return deleted
620: 
621: 
622: def process_single_file(
623:     sftp: paramiko.SFTPClient,
624:     remote_filename: str,
625:     paths: Dict[str, Path],
626:     db_manager: Optional[DatabaseConnectionManager],
627:     sftp_logger: SFTPLoggerManager,
628:     config: ConfigParser,
629:     files_to_delete: List[str]
630: ) -> Dict[str, Any]:
631:     """
632:     Process a single CSV file from SFTP
633:     
634:     Returns dict with processing results
635:     """
636:     result = {
637:         'filename': remote_filename,
638:         'status': 'unknown',
639:         'object_type': None,
640:         'tips_found': 0,
641:         'inserted': 0,
642:         'duplicates': 0,
643:         'errors': 0
644:     }
645:     
646:     local_file = None
647:     
648:     try:
649:         local_file = download_file(sftp, remote_filename, paths['incoming'])
650:         
651:         try:
652:             id_column, object_meta = detect_object_type(local_file)
653:             result['object_type'] = object_meta['abbreviation']
654:         except ObjectTypeDetectionError as e:
655:             quarantine_file(local_file, paths['quarantine'], str(e), 
656:                            sftp_logger.error_logger)
657:             result['status'] = 'quarantined'
658:             return result
659:         
660:         tips_data = extract_tips_from_csv(local_file, id_column, object_meta)
661:         result['tips_found'] = len(tips_data)
662:         
663:         if not tips_data:
664:             logger.warning(f"No TIPs found in {remote_filename}")
665:             result['status'] = 'empty'
666:         
667:         write_audit = config.getboolean('processing', 'write_audit_csv', fallback=True)
668:         if write_audit and tips_data:
669:             write_audit_csv(tips_data, paths['tip_audit'], remote_filename)
670:         
671:         insert_to_db = config.getboolean('processing', 'insert_to_database', fallback=True)
672:         if insert_to_db and db_manager and tips_data:
673:             db_result = insert_tips_to_database(
674:                 db_manager, tips_data, remote_filename, 
675:                 sftp_logger.warning_logger
676:             )
677:             result.update(db_result)
678:         
679:         archive_file(local_file, paths['processed'], object_meta['abbreviation'])
680:         local_file = None
681:         
682:         delete_after = config.getboolean('processing', 'delete_from_sftp_after_archive', 
683:                                          fallback=True)
684:         if delete_after:
685:             delete_remote_file(sftp, remote_filename, files_to_delete)
686:         
687:         result['status'] = 'success'
688:         
689:     except Exception as e:
690:         logger.error(f"Error processing {remote_filename}: {e}", exc_info=True)
691:         sftp_logger.error_logger.error(
692:             f"PROCESSING FAILED | file={remote_filename} | error={e}"
693:         )
694:         result['status'] = 'error'
695:         result['errors'] = 1
696:         
697:         if local_file and local_file.exists():
698:             quarantine_file(local_file, paths['quarantine'], 
699:                            f"Processing error: {e}", sftp_logger.error_logger)
700:     
701:     return result
702: 
703: 
704: def run_sftp_download(
705:     sftp_config_path: str = 'config/sftp_config.ini',
706:     base_config: Optional[ConfigLoader] = None,
707:     db_manager: Optional[DatabaseConnectionManager] = None
708: ) -> Dict[str, Any]:
709:     """
710:     Main entry point for SFTP download process
711:     
712:     Can be called standalone or from continuous processor.
713:     
714:     Args:
715:         sftp_config_path: Path to SFTP configuration file
716:         base_config: Optional existing ConfigLoader (for logging paths)
717:         db_manager: Optional existing database connection
718:         
719:     Returns:
720:         Summary dict with processing statistics
721:     """
722:     summary = {
723:         'start_time': datetime.now().isoformat(),
724:         'files_processed': 0,
725:         'total_tips_found': 0,
726:         'total_inserted': 0,
727:         'total_duplicates': 0,
728:         'total_errors': 0,
729:         'files_quarantined': 0,
730:         'files_deleted_from_sftp': 0,
731:         'status': 'unknown'
732:     }
733:     
734:     ssh_client = None
735:     sftp_client = None
736:     own_db_manager = False
737:     
738:     try:
739:         sftp_config = load_sftp_config(sftp_config_path)
740:         paths = create_directories(sftp_config)
741:         
742:         if base_config:
743:             log_path = Path(base_config.get('paths', 'base_log_path'))
744:         else:
745:             log_path = Path('/mnt/data/noggin/log')
746:         
747:         sftp_logger = SFTPLoggerManager(sftp_config, log_path)
748:         sftp_logger.configure_sftp_loggers()
749:         
750:         if db_manager is None:
751:             if base_config is None:
752:                 base_config = ConfigLoader(
753:                     'config/base_config.ini',
754:                     'config/load_compliance_check_driver_loader_config.ini'
755:                 )
756:             db_manager = DatabaseConnectionManager(base_config)
757:             own_db_manager = True
758:         
759:         ssh_client, sftp_client = connect_sftp(sftp_config)
760:         
761:         remote_dir = sftp_config.get('sftp', 'remote_directory')
762:         csv_files = list_csv_files(sftp_client, remote_dir)
763:         
764:         if not csv_files:
765:             logger.info("No CSV files found on SFTP server")
766:             summary['status'] = 'no_files'
767:             return summary
768:         
769:         files_to_delete: List[str] = []
770:         
771:         for filename, mtime in csv_files:
772:             logger.info(f"Processing: {filename}")
773:             
774:             result = process_single_file(
775:                 sftp_client, filename, paths, db_manager,
776:                 sftp_logger, sftp_config, files_to_delete
777:             )
778:             
779:             summary['files_processed'] += 1
780:             summary['total_tips_found'] += result.get('tips_found', 0)
781:             summary['total_inserted'] += result.get('inserted', 0)
782:             summary['total_duplicates'] += result.get('duplicates', 0)
783:             summary['total_errors'] += result.get('errors', 0)
784:             
785:             if result['status'] == 'quarantined':
786:                 summary['files_quarantined'] += 1
787:         
788:         deleted_count = execute_remote_deletions(sftp_client, files_to_delete)
789:         summary['files_deleted_from_sftp'] = deleted_count
790:         
791:         summary['status'] = 'success'
792:         summary['end_time'] = datetime.now().isoformat()
793:         
794:         logger.info("=" * 60)
795:         logger.info("SFTP DOWNLOAD SUMMARY")
796:         logger.info("=" * 60)
797:         logger.info(f"Files processed:     {summary['files_processed']}")
798:         logger.info(f"TIPs found:          {summary['total_tips_found']}")
799:         logger.info(f"TIPs inserted:       {summary['total_inserted']}")
800:         logger.info(f"Duplicates skipped:  {summary['total_duplicates']}")
801:         logger.info(f"Errors:              {summary['total_errors']}")
802:         logger.info(f"Files quarantined:   {summary['files_quarantined']}")
803:         logger.info(f"Deleted from SFTP:   {summary['files_deleted_from_sftp']}")
804:         logger.info("=" * 60)
805:         
806:         return summary
807:         
808:     except SFTPConnectionError as e:
809:         logger.error(f"SFTP connection error: {e}")
810:         summary['status'] = 'connection_error'
811:         summary['error'] = str(e)
812:         return summary
813:         
814:     except Exception as e:
815:         logger.error(f"SFTP download failed: {e}", exc_info=True)
816:         summary['status'] = 'error'
817:         summary['error'] = str(e)
818:         return summary
819:         
820:     finally:
821:         if sftp_client:
822:             sftp_client.close()
823:         if ssh_client:
824:             ssh_client.close()
825:         if own_db_manager and db_manager:
826:             db_manager.close_all()
827:         
828:         logger.info("SFTP connection closed")
829: 
830: 
831: def main() -> int:
832:     """Standalone entry point"""
833:     try:
834:         base_config = ConfigLoader(
835:             'config/base_config.ini',
836:             'config/load_compliance_check_driver_loader_config.ini'
837:         )
838:         
839:         logger_manager = LoggerManager(base_config, script_name='sftp_download_tips')
840:         logger_manager.configure_application_logger()
841:         
842:         logger.info("=" * 80)
843:         logger.info("SFTP TIP DOWNLOADER - STANDALONE MODE")
844:         logger.info("=" * 80)
845:         
846:         result = run_sftp_download(base_config=base_config)
847:         
848:         if result['status'] == 'success':
849:             return 0
850:         else:
851:             return 1
852:             
853:     except KeyboardInterrupt:
854:         logger.warning("Download interrupted by user")
855:         return 1
856:         
857:     except Exception as e:
858:         logger.error(f"Fatal error: {e}", exc_info=True)
859:         return 1
860: 
861: 
862: if __name__ == "__main__":
863:     sys.exit(main())
</file>

<file path="common/config.py">
  1: import configparser
  2: from pathlib import Path
  3: from typing import Any, Optional
  4: import logging
  5: 
  6: logger: logging.Logger = logging.getLogger(__name__)
  7: 
  8: class ConfigurationError(Exception):
  9:     pass
 10: 
 11: class ConfigLoader:
 12:     def __init__(self, base_config_path: str, specific_config_path: Optional[str] = None) -> None:
 13:         self.base_config: configparser.ConfigParser = configparser.ConfigParser()
 14:         self.specific_config: configparser.ConfigParser = configparser.ConfigParser()
 15:         
 16:         if not Path(base_config_path).exists():
 17:             raise ConfigurationError(f"Base config not found: {base_config_path}")
 18:         
 19:         self.base_config.read(base_config_path)
 20:         logger.info(f"Loaded base configuration from {base_config_path}")
 21:         
 22:         if specific_config_path:
 23:             if not Path(specific_config_path).exists():
 24:                 raise ConfigurationError(f"Specific config not found: {specific_config_path}")
 25:             
 26:             self.specific_config.read(specific_config_path)
 27:             logger.info(f"Loaded specific configuration from {specific_config_path}")
 28:         
 29:         self._validate_configuration()
 30:     
 31:     def _validate_configuration(self) -> None:
 32:         required_base_sections: dict[str, list[str]] = {
 33:             'postgresql': ['host', 'port', 'database', 'user', 'password'],
 34:             'paths': ['base_output_path', 'base_log_path', 'input_folder_path'],
 35:             'api': ['base_url', 'media_service_url', 'namespace', 'bearer_token'],
 36:             'processing': ['max_api_retries', 'api_timeout'],
 37:             'retry': ['max_retry_attempts', 'retry_backoff_multiplier']
 38:         }
 39:         
 40:         for section, keys in required_base_sections.items():
 41:             if not self.base_config.has_section(section):
 42:                 raise ConfigurationError(f"Missing required section: [{section}]")
 43:             
 44:             for key in keys:
 45:                 if not self.base_config.has_option(section, key):
 46:                     raise ConfigurationError(f"Missing required key: [{section}] {key}")
 47:         
 48:         logger.info("Configuration validation passed")
 49:     
 50:     def get(self, section: str, key: str, fallback: Optional[str] = None, from_specific: bool = False) -> str:
 51:         config: configparser.ConfigParser = self.specific_config if from_specific else self.base_config
 52:         
 53:         if from_specific and not config.has_option(section, key):
 54:             config = self.base_config
 55:         
 56:         value: str = config.get(section, key, fallback=fallback)
 57:         return value
 58:     
 59:     def getint(self, section: str, key: str, fallback: Optional[int] = None, from_specific: bool = False) -> int:
 60:         config: configparser.ConfigParser = self.specific_config if from_specific else self.base_config
 61:         
 62:         if from_specific and not config.has_option(section, key):
 63:             config = self.base_config
 64:         
 65:         value: int = config.getint(section, key, fallback=fallback)
 66:         return value
 67:     
 68:     def getfloat(self, section: str, key: str, fallback: Optional[float] = None, from_specific: bool = False) -> float:
 69:         config: configparser.ConfigParser = self.specific_config if from_specific else self.base_config
 70:         
 71:         if from_specific and not config.has_option(section, key):
 72:             config = self.base_config
 73:         
 74:         value: float = config.getfloat(section, key, fallback=fallback)
 75:         return value
 76:     
 77:     def getboolean(self, section: str, key: str, fallback: Optional[bool] = None, from_specific: bool = False) -> bool:
 78:         config: configparser.ConfigParser = self.specific_config if from_specific else self.base_config
 79:         
 80:         if from_specific and not config.has_option(section, key):
 81:             config = self.base_config
 82:         
 83:         value: bool = config.getboolean(section, key, fallback=fallback)
 84:         return value
 85:     
 86:     def get_section(self, section: str, from_specific: bool = False) -> dict[str, str]:
 87:         config: configparser.ConfigParser = self.specific_config if from_specific else self.base_config
 88:         
 89:         if not config.has_section(section):
 90:             return {}
 91:         
 92:         return dict(config.items(section))
 93:     
 94:     def get_postgresql_config(self) -> dict[str, Any]:
 95:         return {
 96:             'host': self.get('postgresql', 'host'),
 97:             'port': self.getint('postgresql', 'port'),
 98:             'database': self.get('postgresql', 'database'),
 99:             'user': self.get('postgresql', 'user'),
100:             'password': self.get('postgresql', 'password'),
101:             'schema': self.get('postgresql', 'schema', fallback='noggin_schema'),
102:             'minconn': self.getint('postgresql', 'pool_min_connections', fallback=2),
103:             'maxconn': self.getint('postgresql', 'pool_max_connections', fallback=10)
104:         }
105:     
106:     def get_api_headers(self) -> dict[str, str]:
107:         return {
108:             'en-namespace': self.get('api', 'namespace'),
109:             'authorization': f"Bearer {self.get('api', 'bearer_token')}"
110:         }
111:     
112:     def get_object_type_config(self) -> dict[str, str]:
113:         return {
114:             'endpoint': self.get('api', 'endpoint', from_specific=True),
115:             'object_type': self.get('api', 'object_type', from_specific=True),
116:             'id_column': self.get('object_detection', 'id_column', from_specific=True)
117:         }
118: 
119: if __name__ == "__main__":
120:     try:
121:         config: ConfigLoader = ConfigLoader(
122:             'config/base_config.ini',
123:             'config/load_compliance_check_driver_loader_config.ini'
124:         )
125:         
126:         print("PostgreSQL Config:", config.get_postgresql_config())
127:         print("API Headers:", config.get_api_headers())
128:         print("Object Type Config:", config.get_object_type_config())
129:         
130:     except ConfigurationError as e:
131:         print(f"Configuration error: {e}")
</file>

<file path="common/database.py">
  1: from __future__ import annotations
  2: import psycopg2
  3: from psycopg2 import pool, extras
  4: from typing import Optional, Any, List, Dict, Tuple, Generator, Sequence
  5: import logging
  6: import atexit
  7: from contextlib import contextmanager
  8: 
  9: logger: logging.Logger = logging.getLogger(__name__)
 10: 
 11: class DatabaseConnectionError(Exception):
 12:     """Raised when database connection fails"""
 13:     pass
 14: 
 15: 
 16: class DatabaseConnectionManager:
 17:     """Manages PostgreSQL connection pool with health checks and graceful cleanup"""
 18:     
 19:     def __init__(self, config: 'ConfigLoader') -> None:
 20:         """
 21:         Initialise connection pool
 22:         
 23:         Args:
 24:             config: ConfigLoader instance with PostgreSQL configuration
 25:         """
 26:         self.config: 'ConfigLoader' = config
 27:         self.pool: Optional[pool.ThreadedConnectionPool] = None
 28:         
 29:         pg_config: Dict[str, Any] = config.get_postgresql_config()
 30:         
 31:         try:
 32:             self.pool = psycopg2.pool.ThreadedConnectionPool(
 33:                 minconn=pg_config.get('minconn', 2),
 34:                 maxconn=pg_config.get('maxconn', 10),
 35:                 host=pg_config['host'],
 36:                 port=pg_config['port'],
 37:                 database=pg_config['database'],
 38:                 user=pg_config['user'],
 39:                 password=pg_config['password'],
 40:                 options=f"-c search_path={pg_config.get('schema', 'noggin_schema')},public",
 41:                 keepalives=1,
 42:                 keepalives_idle=60,
 43:                 keepalives_interval=10,
 44:                 keepalives_count=5
 45:             )
 46:             
 47:             logger.info(f"Database connection pool initialised: {pg_config['database']}@{pg_config['host']}")
 48:             logger.info(f"Schema: {pg_config.get('schema', 'noggin_schema')}")
 49:             logger.info(f"Pool size: min={pg_config.get('minconn', 2)}, max={pg_config.get('maxconn', 10)}")
 50:             
 51:             atexit.register(self.close_all)
 52:             
 53:         except psycopg2.Error as e:
 54:             raise DatabaseConnectionError(f"Failed to create connection pool: {e}")
 55:     
 56:     def _health_check(self, conn: psycopg2.extensions.connection) -> bool:
 57:         """
 58:         Perform quick health check on connection
 59:         
 60:         Args:
 61:             conn: Database connection to check
 62:             
 63:         Returns:
 64:             True if connection is healthy, False otherwise
 65:         """
 66:         try:
 67:             with conn.cursor() as cur:
 68:                 cur.execute("SELECT 1")
 69:                 cur.fetchone()
 70:             return True
 71:         except (psycopg2.OperationalError, psycopg2.InterfaceError):
 72:             return False
 73:     
 74:     def get_connection(self) -> psycopg2.extensions.connection:
 75:         """
 76:         Get a healthy connection from the pool
 77:         
 78:         Returns:
 79:             psycopg2 connection object
 80:             
 81:         Raises:
 82:             DatabaseConnectionError if cannot get healthy connection
 83:         """
 84:         if not self.pool:
 85:             raise DatabaseConnectionError("Connection pool not initialised")
 86:         
 87:         try:
 88:             conn: psycopg2.extensions.connection = self.pool.getconn()
 89:             
 90:             if not self._health_check(conn):
 91:                 logger.warning("Stale connection detected, replacing...")
 92:                 self.pool.putconn(conn, close=True)
 93:                 conn = self.pool.getconn()
 94:                 
 95:                 if not self._health_check(conn):
 96:                     raise DatabaseConnectionError("Unable to obtain healthy connection")
 97:             
 98:             return conn
 99:             
100:         except psycopg2.pool.PoolError as e:
101:             raise DatabaseConnectionError(f"Pool error: {e}")
102:     
103:     def return_connection(self, conn: psycopg2.extensions.connection, close_conn: bool = False) -> None:
104:         """
105:         Return connection to pool
106:         
107:         Args:
108:             conn: Connection to return
109:             close_conn: If True, close connection instead of returning to pool
110:         """
111:         if not self.pool:
112:             return
113:         
114:         try:
115:             self.pool.putconn(conn, close=close_conn)
116:         except psycopg2.pool.PoolError as e:
117:             logger.error(f"Error returning connection to pool: {e}")
118:     
119:     @contextmanager
120:     def get_cursor(self, cursor_factory: Optional[Any] = None) -> Generator[psycopg2.extensions.cursor, None, None]:
121:         """
122:         Context manager for getting connection and cursor
123:         
124:         Args:
125:             cursor_factory: Optional cursor factory (e.g., RealDictCursor)
126:         
127:         Yields:
128:             Database cursor
129:             
130:         Usage:
131:             with db_manager.get_cursor() as cur:
132:                 cur.execute("SELECT * FROM table")
133:                 results = cur.fetchall()
134:         """
135:         conn: psycopg2.extensions.connection = self.get_connection()
136:         cursor: psycopg2.extensions.cursor = conn.cursor(cursor_factory=cursor_factory)
137:         try:
138:             yield cursor
139:             conn.commit()
140:         except Exception as e:
141:             conn.rollback()
142:             logger.error(f"Database operation failed: {e}", exc_info=True)
143:             raise
144:         finally:
145:             cursor.close()
146:             self.return_connection(conn)
147:     
148:     def execute_query(self, query: str, params: Optional[Tuple[Any, ...]] = None) -> List[Tuple[Any, ...]]:
149:         """
150:         Execute SELECT query and return results
151:         
152:         Args:
153:             query: SQL query string
154:             params: Query parameters (optional)
155:             
156:         Returns:
157:             List of tuples (query results)
158:         """
159:         with self.get_cursor() as cur:
160:             cur.execute(query, params)
161:             results: List[Tuple[Any, ...]] = cur.fetchall()
162:             return results
163:     
164:     def execute_query_dict(self, query: str, params: Optional[Tuple[Any, ...]] = None) -> List[Dict[str, Any]]:
165:         """
166:         Execute SELECT query and return results as dictionaries
167:         
168:         Args:
169:             query: SQL query string
170:             params: Query parameters (optional)
171:             
172:         Returns:
173:             List of dictionaries (query results)
174:         """
175:         with self.get_cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
176:             cur.execute(query, params)
177:             results: List[Dict[str, Any]] = [dict(row) for row in cur.fetchall()]
178:             return results
179:     
180:     def execute_update(self, query: str, params: Optional[Tuple[Any, ...]] = None) -> int:
181:         """
182:         Execute INSERT/UPDATE/DELETE query
183:         
184:         Args:
185:             query: SQL query string
186:             params: Query parameters (optional)
187:             
188:         Returns:
189:             Number of affected rows
190:         """
191:         with self.get_cursor() as cur:
192:             cur.execute(query, params)
193:             rowcount: int = cur.rowcount
194:             return rowcount
195:     
196:     # def execute_transaction(self, queries: List[Tuple[str, Optional[Tuple[Any, ...]]]]) -> bool:
197:     def execute_transaction(self, queries: Sequence[Tuple[str, Optional[Tuple[Any, ...]]]]) -> bool:
198:         """
199:         Execute multiple queries in a transaction
200:         
201:         Args:
202:             queries: List of (query, params) tuples
203:             
204:         Returns:
205:             True if transaction succeeded, False otherwise
206:         """
207:         conn: psycopg2.extensions.connection = self.get_connection()
208:         try:
209:             with conn.cursor() as cur:
210:                 for query, params in queries:
211:                     cur.execute(query, params)
212:             conn.commit()
213:             logger.debug(f"Transaction committed: {len(queries)} queries")
214:             return True
215:         except Exception as e:
216:             conn.rollback()
217:             logger.error(f"Transaction failed, rolled back: {e}", exc_info=True)
218:             return False
219:         finally:
220:             self.return_connection(conn)
221:     
222:     def close_all(self) -> None:
223:         """Close all connections and clean up pool"""
224:         if self.pool:
225:             try:
226:                 logger.info("Closing all database connections...")
227:                 self.pool.closeall()
228:                 logger.info("All database connections closed")
229:             except Exception as e:
230:                 logger.error(f"Error closing connection pool: {e}")
231:             finally:
232:                 self.pool = None
233: 
234: 
235: if __name__ == "__main__":
236:     from .config import ConfigLoader
237:     
238:     try:
239:         config: ConfigLoader = ConfigLoader(
240:             '../config/base_config.ini',
241:             '../config/load_compliance_check_driver_loader_config.ini'
242:         )
243:         
244:         db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
245:         
246:         version_result: List[Tuple[Any, ...]] = db_manager.execute_query("SELECT version()")
247:         print(" PostgreSQL version:", version_result[0][0])
248:         
249:         tables_result: List[Dict[str, Any]] = db_manager.execute_query_dict(
250:             "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_name"
251:         )
252:         print(f" Found {len(tables_result)} tables:")
253:         for row in tables_result:
254:             print(f"  - {row['table_name']}")
255:         
256:         print("\n Database connection manager working correctly")
257:         
258:     except DatabaseConnectionError as e:
259:         print(f" Database error: {e}")
260:     except Exception as e:
261:         print(f" Error: {e}")
262:         import traceback
263:         traceback.print_exc()
264:     finally:
265:         if 'db_manager' in locals():
266:             db_manager.close_all()
</file>

<file path="common/rate_limiter.py">
  1: from __future__ import annotations
  2: import logging
  3: import time
  4: from typing import Optional, List
  5: from datetime import datetime, timedelta
  6: from enum import Enum
  7: 
  8: logger: logging.Logger = logging.getLogger(__name__)
  9: 
 10: 
 11: class CircuitState(Enum):
 12:     """Circuit breaker states"""
 13:     CLOSED = "closed"      # Normal operation
 14:     OPEN = "open"          # Circuit tripped, refusing requests
 15:     HALF_OPEN = "half_open"  # Testing if service recovered
 16: 
 17: 
 18: class CircuitBreakerError(Exception):
 19:     """Raised when circuit breaker is open"""
 20:     pass
 21: 
 22: 
 23: class CircuitBreaker:
 24:     """
 25:     Circuit breaker for API rate limiting and failure handling
 26:     
 27:     Monitors API request success/failure rate and opens circuit when
 28:     failure rate exceeds threshold, preventing additional load on struggling server.
 29:     """
 30:     
 31:     def __init__(self, config: 'ConfigLoader') -> None:
 32:         """
 33:         Initialise circuit breaker
 34:         
 35:         Args:
 36:             config: ConfigLoader instance
 37:         """
 38:         self.config: 'ConfigLoader' = config
 39:         
 40:         self.failure_threshold: float = config.getfloat('circuit_breaker', 'failure_threshold_percent') / 100
 41:         self.recovery_threshold: float = config.getfloat('circuit_breaker', 'recovery_threshold_percent') / 100
 42:         self.open_duration: int = config.getint('circuit_breaker', 'circuit_open_duration_seconds')
 43:         self.sample_size: int = config.getint('circuit_breaker', 'sample_size')
 44:         
 45:         self.state: CircuitState = CircuitState.CLOSED
 46:         self.failure_count: int = 0
 47:         self.success_count: int = 0
 48:         self.last_failure_time: Optional[datetime] = None
 49:         self.opened_at: Optional[datetime] = None
 50:         
 51:         self.recent_requests: List[bool] = []
 52:         
 53:         logger.info(f"Circuit breaker initialised: failure_threshold={self.failure_threshold*100}%, "
 54:                    f"recovery_threshold={self.recovery_threshold*100}%, "
 55:                    f"open_duration={self.open_duration}s, sample_size={self.sample_size}")
 56:     
 57:     def _calculate_failure_rate(self) -> float:
 58:         """Calculate failure rate from recent requests"""
 59:         if not self.recent_requests:
 60:             return 0.0
 61:         
 62:         failures: int = sum(1 for success in self.recent_requests if not success)
 63:         return failures / len(self.recent_requests)
 64:     
 65:     def _should_attempt_reset(self) -> bool:
 66:         """Check if enough time has passed to attempt circuit reset"""
 67:         if self.opened_at is None:
 68:             return False
 69:         
 70:         time_open: timedelta = datetime.now() - self.opened_at
 71:         return time_open.total_seconds() >= self.open_duration
 72:     
 73:     def before_request(self) -> None:
 74:         """
 75:         Call before making API request
 76:         
 77:         Raises:
 78:             CircuitBreakerError if circuit is open
 79:         """
 80:         if self.state == CircuitState.OPEN:
 81:             if self._should_attempt_reset():
 82:                 self.state = CircuitState.HALF_OPEN
 83:                 logger.info("Circuit breaker entering HALF_OPEN state (testing recovery)")
 84:             else:
 85:                 time_remaining: float = self.open_duration - (datetime.now() - self.opened_at).total_seconds()
 86:                 raise CircuitBreakerError(
 87:                     f"Circuit breaker is OPEN. Server is struggling. "
 88:                     f"Retry in {time_remaining:.0f} seconds."
 89:                 )
 90:     
 91:     def record_success(self) -> None:
 92:         """Record successful API request"""
 93:         self.success_count += 1
 94:         self.recent_requests.append(True)
 95:         
 96:         if len(self.recent_requests) > self.sample_size:
 97:             self.recent_requests.pop(0)
 98:         
 99:         if self.state == CircuitState.HALF_OPEN:
100:             failure_rate: float = self._calculate_failure_rate()
101:             if failure_rate <= self.recovery_threshold:
102:                 self.state = CircuitState.CLOSED
103:                 self.opened_at = None
104:                 logger.info(f"Circuit breaker CLOSED (recovered). Failure rate: {failure_rate*100:.1f}%")
105:         
106:         if self.state == CircuitState.CLOSED:
107:             logger.debug(f"Request successful. Failure rate: {self._calculate_failure_rate()*100:.1f}%")
108:     
109:     def record_failure(self) -> None:
110:         """Record failed API request"""
111:         self.failure_count += 1
112:         self.last_failure_time = datetime.now()
113:         self.recent_requests.append(False)
114:         
115:         if len(self.recent_requests) > self.sample_size:
116:             self.recent_requests.pop(0)
117:         
118:         failure_rate: float = self._calculate_failure_rate()
119:         
120:         if self.state == CircuitState.HALF_OPEN:
121:             self.state = CircuitState.OPEN
122:             self.opened_at = datetime.now()
123:             logger.warning(f"Circuit breaker reopened OPEN (recovery failed). "
124:                           f"Failure rate: {failure_rate*100:.1f}%")
125:         
126:         elif self.state == CircuitState.CLOSED:
127:             if len(self.recent_requests) >= self.sample_size and failure_rate >= self.failure_threshold: 
128:                 self.state = CircuitState.OPEN
129:                 self.opened_at = datetime.now()
130:                 logger.warning(f"Circuit breaker OPEN (failure threshold exceeded). "
131:                               f"Failure rate: {failure_rate*100:.1f}%. "
132:                               f"Pausing requests for {self.open_duration}s")
133:             else:
134:                 logger.debug(f"Request failed. Failure rate: {failure_rate*100:.1f}%")
135:     
136:     def get_state(self) -> CircuitState:
137:         """Get current circuit state"""
138:         return self.state
139:     
140:     def get_statistics(self) -> dict[str, any]:
141:         """Get circuit breaker statistics"""
142:         failure_rate: float = self._calculate_failure_rate()
143:         
144:         return {
145:             'state': self.state.value,
146:             'total_requests': self.success_count + self.failure_count,
147:             'success_count': self.success_count,
148:             'failure_count': self.failure_count,
149:             'failure_rate': round(failure_rate * 100, 2),
150:             'recent_sample_size': len(self.recent_requests),
151:             'opened_at': self.opened_at.isoformat() if self.opened_at else None,
152:             'last_failure': self.last_failure_time.isoformat() if self.last_failure_time else None
153:         }
154:     
155:     def reset(self) -> None:
156:         """Reset circuit breaker to initial state"""
157:         self.state = CircuitState.CLOSED
158:         self.failure_count = 0
159:         self.success_count = 0
160:         self.last_failure_time = None
161:         self.opened_at = None
162:         self.recent_requests = []
163:         logger.info("Circuit breaker reset to CLOSED state")
164: 
165: 
166: if __name__ == "__main__":
167:     from .config import ConfigLoader
168:     from .logger import LoggerManager
169:     
170:     try:
171:         config: ConfigLoader = ConfigLoader(
172:             '../config/base_config.ini',
173:             '../config/load_compliance_check_driver_loader_config.ini'
174:         )
175:         
176:         logger_manager: LoggerManager = LoggerManager(config, script_name='test_circuit_breaker')
177:         logger_manager.configure_application_logger()
178:         
179:         circuit_breaker: CircuitBreaker = CircuitBreaker(config)
180:         
181:         print("\n=== Testing Circuit Breaker ===\n")
182:         
183:         print("1. Simulating successful requests (should stay CLOSED):")
184:         for i in range(5):
185:             circuit_breaker.before_request()
186:             circuit_breaker.record_success()
187:             print(f"   Request {i+1}: SUCCESS - State: {circuit_breaker.get_state().value}")
188:         
189:         print(f"\n2. Simulating failures (should trip to OPEN at 50% failure rate):")
190:         for i in range(10):
191:             try:
192:                 circuit_breaker.before_request()
193:                 if i % 2 == 0:
194:                     circuit_breaker.record_failure()
195:                     print(f"   Request {i+1}: FAILED - State: {circuit_breaker.get_state().value}")
196:                 else:
197:                     circuit_breaker.record_success()
198:                     print(f"   Request {i+1}: SUCCESS - State: {circuit_breaker.get_state().value}")
199:             except CircuitBreakerError as e:
200:                 print(f"   Request {i+1}: BLOCKED - {e}")
201:         
202:         print(f"\n3. Waiting for circuit to enter HALF_OPEN...")
203:         print(f"   (would normally wait {circuit_breaker.open_duration}s)")
204:         circuit_breaker.opened_at = datetime.now() - timedelta(seconds=circuit_breaker.open_duration + 1)
205:         
206:         print(f"\n4. Testing recovery (should close on success):")
207:         try:
208:             circuit_breaker.before_request()
209:             circuit_breaker.record_success()
210:             print(f"   Recovery request: SUCCESS - State: {circuit_breaker.get_state().value}")
211:         except CircuitBreakerError as e:
212:             print(f"   Recovery blocked: {e}")
213:         
214:         print(f"\n5. Statistics:")
215:         stats: dict = circuit_breaker.get_statistics()
216:         for key, value in stats.items():
217:             print(f"   {key}: {value}")
218:         
219:         print("\n Circuit breaker test complete")
220:         
221:     except Exception as e:
222:         print(f" Error: {e}")
223:         import traceback
224:         traceback.print_exc()
</file>

<file path="config/coupling_compliance_check_config.ini">
  1: [object_type]
  2: abbreviation = CCC
  3: full_name = Coupling Compliance Check
  4: 
  5: [api]
  6: endpoint = /rest/object/couplingComplianceCheck/$tip
  7: object_type = Coupling Compliance Check
  8: 
  9: [csv_import]
 10: couplingId = noggin_reference
 11: date = inspection_date
 12: personCompleting = person_completing
 13: team = team
 14: vehicleId = vehicle_id
 15: trailer = trailer
 16: trailerId = trailer_id
 17: 
 18: [fields]
 19: id_field = couplingId:noggin_reference:string
 20: date_field = date:inspection_date:datetime
 21: couplingId = coupling_id:string
 22: date = inspection_date:datetime
 23: personCompleting = person_completing:string
 24: team = team_hash:hash:team
 25: vehicleId = vehicle_id:string
 26: trailerId = trailer_id:string
 27: runNumber = run_number:string
 28: jobNumber = job_number:string
 29: customerClient = customer_client:string
 30: goldstarAsset = goldstar_asset:string
 31: trailer = trailer_hash:hash:trailer
 32: trailer2 = trailer2_hash:hash:trailer
 33: trailerId2 = trailer2_id:string
 34: trailer3 = trailer3_hash:hash:trailer
 35: trailerId3 = trailer3_id:string
 36: checkboxForTrailer2 = checkbox_trailer2:bool
 37: checkboxForTrailer3 = checkbox_trailer3:bool
 38: contactBetweenTheSkidPlateTurntableYT1 = skid_plate_contact_t1:bool
 39: turntableReleaseHandleFullyEngagedAndTheSafetyChaiYT1 = turntable_release_engaged_t1:bool
 40: isTheKingPinFullyEngagedYT1 = king_pin_engaged_t1:bool
 41: hasATugTestBeenPerformedYT1 = tug_test_performed_t1:bool
 42: howManyTugTestsPerformedT1 = tug_tests_count_t1:string
 43: haveTheTrailerLegsBeenRaisedYT1 = trailer_legs_raised_t1:bool
 44: contactBetweenTheSkidPlateTurntableYT2 = skid_plate_contact_t2:bool
 45: isTheTurntableReleaseHandleFullyEngagedAndTheSafetYT2 = turntable_release_engaged_t2:bool
 46: isTheKingPinFullyEngagedYT2 = king_pin_engaged_t2:bool
 47: hasATugTestBeenPerformedYT2 = tug_test_performed_t2:bool
 48: howManyTugTestsPerformedT2 = tug_tests_count_t2:string
 49: isTheRingFeederPinFullyEngagedAndLockedIntoPlaceYT = ring_feeder_pin_engaged_t2:bool
 50: haveTheTrailerLegsBeenRaisedYT2 = trailer_legs_raised_t2:bool
 51: contactBetweenTheSkidPlateTurntableYT3 = skid_plate_contact_t3:bool
 52: turntableReleaseHandleFullyEngagedAndTheSafetyChaiT3 = turntable_release_engaged_t3:bool
 53: isTheKingPinFullyEngagedYT3 = king_pin_engaged_t3:bool
 54: hasATugTestBeenPerformedYT3 = tug_test_performed_t3:bool
 55: howManyTugTestsPerformedT3 = tug_tests_count_t3:string
 56: isTheRingFeederPinFullyEngagedAndLockedIntoPlaceYT3 = ring_feeder_pin_engaged_t3:bool
 57: haveTheTrailerLegsBeenRaisedYT3 = trailer_legs_raised_t3:bool
 58: 
 59: [output]
 60: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
 61: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
 62: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
 63: show_json_payload_in_text_file = true
 64: show_all_fields = false
 65: filename_image_stub = photo
 66: unknown_response_output_text = Unknown
 67: 
 68: [template]
 69: content = 
 70:     ============================================================
 71:     <full_name>
 72:     RECORD GENERATED: <generation_date>
 73:     ============================================================
 74:     
 75:     Coupling ID:           <couplingId>
 76:     Date:                  <date>
 77:     Person Completing:     <personCompleting>
 78:     
 79:     Vehicle ID:            <vehicleId>
 80:     
 81:     Trailer:               <trailer_resolved>
 82:     Trailer ID:            <trailerId>
 83:     
 84:     <if:trailer2>
 85:     Trailer 2:             <trailer2_resolved>
 86:     Trailer 2 ID:          <trailerId2>
 87:     
 88:         <if:trailer3>
 89:     Trailer 3:             <trailer3_resolved>
 90:     Trailer 3 ID:          <trailerId3>
 91:         </if:trailer3>
 92:     </if:trailer2>
 93:     
 94:     <if:jobNumber>
 95:     Job Number:            <jobNumber>
 96:     </if:jobNumber>
 97:     <if:runNumber>
 98:     Run Number:            <runNumber>
 99:     </if:runNumber>
100:     <if:customerClient>
101:     Customer/Client:       <customerClient>
102:     </if:customerClient>
103:     
104:     Team:                  <team_resolved>
105:     
106:     <if:show_all_fields>
107:     ------------------------------------------------------------
108:     TRAILER 1 COUPLING CHECKS
109:     ------------------------------------------------------------
110:     Skid Plate/Turntable Contact:              <contactBetweenTheSkidPlateTurntableYT1>
111:     Turntable Release Handle Engaged:          <turntableReleaseHandleFullyEngagedAndTheSafetyChaiYT1>
112:     King Pin Fully Engaged:                    <isTheKingPinFullyEngagedYT1>
113:     Tug Test Performed:                        <hasATugTestBeenPerformedYT1>
114:     <if:howManyTugTestsPerformedT1>
115:     Number of Tug Tests:                       <howManyTugTestsPerformedT1>
116:     </if:howManyTugTestsPerformedT1>
117:     Trailer Legs Raised:                       <haveTheTrailerLegsBeenRaisedYT1>
118:     
119:     <if:trailer2>
120:     ------------------------------------------------------------
121:     TRAILER 2 COUPLING CHECKS
122:     ------------------------------------------------------------
123:     Skid Plate/Turntable Contact:              <contactBetweenTheSkidPlateTurntableYT2>
124:     Turntable Release Handle Engaged:          <isTheTurntableReleaseHandleFullyEngagedAndTheSafetYT2>
125:     King Pin Fully Engaged:                    <isTheKingPinFullyEngagedYT2>
126:     Tug Test Performed:                        <hasATugTestBeenPerformedYT2>
127:     <if:howManyTugTestsPerformedT2>
128:     Number of Tug Tests:                       <howManyTugTestsPerformedT2>
129:     </if:howManyTugTestsPerformedT2>
130:     Ring Feeder Pin Engaged:                   <isTheRingFeederPinFullyEngagedAndLockedIntoPlaceYT>
131:     Trailer Legs Raised:                       <haveTheTrailerLegsBeenRaisedYT2>
132:     
133:         <if:trailer3>
134:     ------------------------------------------------------------
135:     TRAILER 3 COUPLING CHECKS
136:     ------------------------------------------------------------
137:     Skid Plate/Turntable Contact:              <contactBetweenTheSkidPlateTurntableYT3>
138:     Turntable Release Handle Engaged:          <turntableReleaseHandleFullyEngagedAndTheSafetyChaiT3>
139:     King Pin Fully Engaged:                    <isTheKingPinFullyEngagedYT3>
140:     Tug Test Performed:                        <hasATugTestBeenPerformedYT3>
141:     <if:howManyTugTestsPerformedT3>
142:     Number of Tug Tests:                       <howManyTugTestsPerformedT3>
143:     </if:howManyTugTestsPerformedT3>
144:     Ring Feeder Pin Engaged:                   <isTheRingFeederPinFullyEngagedAndLockedIntoPlaceYT3>
145:     Trailer Legs Raised:                       <haveTheTrailerLegsBeenRaisedYT3>
146:         </if:trailer3>
147:     </if:trailer2>
148:     </if:show_all_fields>
149:     
150:     Attachments:           <attachment_count>
151:     
152:     <if:show_json_payload_in_text_file>
153:     ------------------------------------------------------------
154:     COMPLETE TECHNICAL DATA (JSON FORMAT)
155:     ------------------------------------------------------------
156:     
157:     <json_payload>
158:     </if:show_json_payload_in_text_file>
</file>

<file path="config/forklift_prestart_inspection_config.ini">
  1: [object_type]
  2: abbreviation = FPI
  3: full_name = Forklift Prestart Inspection
  4: 
  5: [api]
  6: endpoint = /rest/object/forkliftPrestartInspection/$tip
  7: object_type = Forklift Prestart Inspection
  8: 
  9: [csv_import]
 10: forkliftPrestartInspectionId = noggin_reference
 11: date = inspection_date
 12: personsCompleting = persons_completing
 13: team = team
 14: goldstarAsset = goldstar_asset
 15: preStartStatus = prestart_status
 16: 
 17: [fields]
 18: id_field = forkliftPrestartInspectionId:noggin_reference:string
 19: date_field = date:inspection_date:datetime
 20: forkliftPrestartInspectionId = forklift_inspection_id:string
 21: date = inspection_date:datetime
 22: goldstarAsset = goldstar_asset:string
 23: preStartStatus = prestart_status:string
 24: assetType = asset_type:string
 25: assetId = asset_id:string
 26: assetName = asset_name:string
 27: personsCompleting = persons_completing:string
 28: team = team_hash:hash:team
 29: hourReadingAtStartOfShift = hour_reading:string
 30: damageCompliant = damage_compliant:bool
 31: damageDefect = damage_defect:bool
 32: damageComments = damage_comments:string
 33: fluidLeaksCompliant = fluid_leaks_compliant:bool
 34: fluidLeaksDefect = fluid_leaks_defect:bool
 35: fluidLeaksComments = fluid_leaks_comments:string
 36: tyresWheelsCompliant = tyres_wheels_compliant:bool
 37: tyreWheelsDefect = tyres_wheels_defect:bool
 38: tyresWheelsComments = tyres_wheels_comments:string
 39: forkTynesCompliant = fork_tynes_compliant:bool
 40: forkTynesDefect = fork_tynes_defect:bool
 41: forkTynesComments = fork_tynes_comments:string
 42: chainsHosesCablesCompliant = chains_hoses_cables_compliant:bool
 43: chainsHosesCablesDefect = chains_hoses_cables_defect:bool
 44: chainsHosesCablesComments = chains_hoses_cables_comments:string
 45: guardsCompliant = guards_compliant:bool
 46: guardsDefect = guards_defect:bool
 47: guardsComments = guards_comments:string
 48: safetyDevicesCompliant = safety_devices_compliant:bool
 49: safetyDevicesDefect = safety_devices_defect:bool
 50: safetyDevicesComments = safety_devices_comments:string
 51: capacityRatingPlateWarningLabelsCompliant = capacity_plate_compliant:bool
 52: capacityRatingPlateWarningLabelsDefect = capacity_plate_defect:bool
 53: capacityRatingPlateWarningLabelsComment = capacity_plate_comments:string
 54: fluidLevelChecksCompliant = fluid_level_compliant:bool
 55: fluidLevelChecksDefect = fluid_level_defect:bool
 56: fluidLevelChecksComments = fluid_level_comments:string
 57: lpgCompliant = lpg_compliant:bool
 58: lpgDefect = lpg_defect:bool
 59: lpgComments = lpg_comments:string
 60: radiatorFanCompliant = radiator_fan_compliant:bool
 61: radiatorFanDefect = radiator_fan_defect:bool
 62: radiatorFanComments = radiator_fan_comments:string
 63: airCleanerCompliant = air_cleaner_compliant:bool
 64: airCleanerDefect = air_cleaner_defect:bool
 65: airCleanerComments = air_cleaner_comments:string
 66: transmissionFluidLevelsCompliant = transmission_fluid_compliant:bool
 67: transmissionFluidLevelsDefect = transmission_fluid_defect:bool
 68: transmissionFluidLevelsComments = transmission_fluid_comments:string
 69: audibleAlarmsCompliant = audible_alarms_compliant:bool
 70: audibleAlarmsDefect = audible_alarms_defect:bool
 71: audibleAlarmsComments = audible_alarms_comments:string
 72: brakesCompliant = brakes_compliant:bool
 73: brakesDefect = brakes_defect:bool
 74: brakesComments = brakes_comments:string
 75: inchingPedalCompliant = inching_pedal_compliant:bool
 76: inchingPedalDefect = inching_pedal_defect:bool
 77: inchingPedalComments = inching_pedal_comments:string
 78: steeringCompliant = steering_compliant:bool
 79: steeringDefect = steering_defect:bool
 80: steeringComments = steering_comments:string
 81: hydraulicControlsCompliant = hydraulic_controls_compliant:bool
 82: hydraulicControlsDefect = hydraulic_controls_defect:bool
 83: hydraulicControlsComments = hydraulic_controls_comments:string
 84: attachmentsCompliant = attachments_compliant:bool
 85: attachmentsDefect = attachments_defect:bool
 86: attachmentComments = attachment_comments:string
 87: interlockSpeedGovernorCompliant = interlock_governor_compliant:bool
 88: interlockSpeedGovernorDefect = interlock_governor_defect:bool
 89: interlockSpeedGovernorComments = interlock_governor_comments:string
 90: comments = general_comments:string
 91: 
 92: [output]
 93: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
 94: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
 95: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
 96: show_json_payload_in_text_file = true
 97: show_all_fields = false
 98: filename_image_stub = photo
 99: unknown_response_output_text = Unknown
100: 
101: [template]
102: content = 
103:     ============================================================
104:     <full_name>
105:     RECORD GENERATED: <generation_date>
106:     ============================================================
107:     
108:     Inspection ID:         <forkliftPrestartInspectionId>
109:     Date:                  <date>
110:     Person Completing:     <personsCompleting>
111:     
112:     Prestart Status:       <preStartStatus>
113:     
114:     <if:goldstarAsset>
115:     Goldstar Asset:        <goldstarAsset>
116:     </if:goldstarAsset>
117:     <if:assetType>
118:     Asset Type:            <assetType>
119:     </if:assetType>
120:     <if:assetId>
121:     Asset ID:              <assetId>
122:     </if:assetId>
123:     <if:assetName>
124:     Asset Name:            <assetName>
125:     </if:assetName>
126:     
127:     Team:                  <team_resolved>
128:     
129:     <if:hourReadingAtStartOfShift>
130:     Hour Reading:          <hourReadingAtStartOfShift>
131:     </if:hourReadingAtStartOfShift>
132:     
133:     <if:show_all_fields>
134:     ------------------------------------------------------------
135:     INSPECTION ITEMS (Compliant / Defect)
136:     ------------------------------------------------------------
137:     Damage:                <damageCompliant> / <damageDefect>
138:     <if:damageComments>
139:       Comment: <damageComments>
140:     </if:damageComments>
141:     
142:     Fluid Leaks:           <fluidLeaksCompliant> / <fluidLeaksDefect>
143:     <if:fluidLeaksComments>
144:       Comment: <fluidLeaksComments>
145:     </if:fluidLeaksComments>
146:     
147:     Tyres/Wheels:          <tyresWheelsCompliant> / <tyreWheelsDefect>
148:     <if:tyresWheelsComments>
149:       Comment: <tyresWheelsComments>
150:     </if:tyresWheelsComments>
151:     
152:     Fork Tynes:            <forkTynesCompliant> / <forkTynesDefect>
153:     <if:forkTynesComments>
154:       Comment: <forkTynesComments>
155:     </if:forkTynesComments>
156:     
157:     Chains/Hoses/Cables:   <chainsHosesCablesCompliant> / <chainsHosesCablesDefect>
158:     <if:chainsHosesCablesComments>
159:       Comment: <chainsHosesCablesComments>
160:     </if:chainsHosesCablesComments>
161:     
162:     Guards:                <guardsCompliant> / <guardsDefect>
163:     <if:guardsComments>
164:       Comment: <guardsComments>
165:     </if:guardsComments>
166:     
167:     Safety Devices:        <safetyDevicesCompliant> / <safetyDevicesDefect>
168:     <if:safetyDevicesComments>
169:       Comment: <safetyDevicesComments>
170:     </if:safetyDevicesComments>
171:     
172:     Capacity Rating Plate: <capacityRatingPlateWarningLabelsCompliant> / <capacityRatingPlateWarningLabelsDefect>
173:     <if:capacityRatingPlateWarningLabelsComment>
174:       Comment: <capacityRatingPlateWarningLabelsComment>
175:     </if:capacityRatingPlateWarningLabelsComment>
176:     
177:     Fluid Level Checks:    <fluidLevelChecksCompliant> / <fluidLevelChecksDefect>
178:     <if:fluidLevelChecksComments>
179:       Comment: <fluidLevelChecksComments>
180:     </if:fluidLevelChecksComments>
181:     
182:     LPG:                   <lpgCompliant> / <lpgDefect>
183:     <if:lpgComments>
184:       Comment: <lpgComments>
185:     </if:lpgComments>
186:     
187:     Radiator/Fan:          <radiatorFanCompliant> / <radiatorFanDefect>
188:     <if:radiatorFanComments>
189:       Comment: <radiatorFanComments>
190:     </if:radiatorFanComments>
191:     
192:     Air Cleaner:           <airCleanerCompliant> / <airCleanerDefect>
193:     <if:airCleanerComments>
194:       Comment: <airCleanerComments>
195:     </if:airCleanerComments>
196:     
197:     Transmission Fluid:    <transmissionFluidLevelsCompliant> / <transmissionFluidLevelsDefect>
198:     <if:transmissionFluidLevelsComments>
199:       Comment: <transmissionFluidLevelsComments>
200:     </if:transmissionFluidLevelsComments>
201:     
202:     Audible Alarms:        <audibleAlarmsCompliant> / <audibleAlarmsDefect>
203:     <if:audibleAlarmsComments>
204:       Comment: <audibleAlarmsComments>
205:     </if:audibleAlarmsComments>
206:     
207:     Brakes:                <brakesCompliant> / <brakesDefect>
208:     <if:brakesComments>
209:       Comment: <brakesComments>
210:     </if:brakesComments>
211:     
212:     Inching Pedal:         <inchingPedalCompliant> / <inchingPedalDefect>
213:     <if:inchingPedalComments>
214:       Comment: <inchingPedalComments>
215:     </if:inchingPedalComments>
216:     
217:     Steering:              <steeringCompliant> / <steeringDefect>
218:     <if:steeringComments>
219:       Comment: <steeringComments>
220:     </if:steeringComments>
221:     
222:     Hydraulic Controls:    <hydraulicControlsCompliant> / <hydraulicControlsDefect>
223:     <if:hydraulicControlsComments>
224:       Comment: <hydraulicControlsComments>
225:     </if:hydraulicControlsComments>
226:     
227:     Attachments:           <attachmentsCompliant> / <attachmentsDefect>
228:     <if:attachmentComments>
229:       Comment: <attachmentComments>
230:     </if:attachmentComments>
231:     
232:     Interlock/Governor:    <interlockSpeedGovernorCompliant> / <interlockSpeedGovernorDefect>
233:     <if:interlockSpeedGovernorComments>
234:       Comment: <interlockSpeedGovernorComments>
235:     </if:interlockSpeedGovernorComments>
236:     
237:     <if:comments>
238:     ------------------------------------------------------------
239:     GENERAL COMMENTS
240:     ------------------------------------------------------------
241:     <comments>
242:     </if:comments>
243:     </if:show_all_fields>
244:     
245:     Attachments:           <attachment_count>
246:     
247:     <if:show_json_payload_in_text_file>
248:     ------------------------------------------------------------
249:     COMPLETE TECHNICAL DATA (JSON FORMAT)
250:     ------------------------------------------------------------
251:     
252:     <json_payload>
253:     </if:show_json_payload_in_text_file>
</file>

<file path="config/load_compliance_check_driver_loader_config.ini">
  1: ; [paths]
  2: ; subdirectory = lcd
  3: ; lookup_table_filename = lookup_table.csv
  4: 
  5: [api]
  6: endpoint = /rest/object/loadComplianceCheckDriverLoader/$tip
  7: object_type = Load Compliance Check (Driver/Loader)
  8: 
  9: [object_detection]
 10: id_column = lcdInspectionId
 11: 
 12: [output]
 13: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
 14: filename_image_stub = photo
 15: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
 16: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
 17: show_all_fields = false
 18: show_compliance_status = true
 19: show_json_payload_in_text_file = true
 20: unknown_response_output_text = Unknown
 21: 
 22: [csv_import]
 23: lcdInspectionId = noggin_reference
 24: date = inspection_date
 25: inspectedBy = inspected_by
 26: driverLoaderName = driver_loader_name
 27: team = team
 28: vehicle = vehicle
 29: vehicleId = vehicle_id
 30: trailer = trailer
 31: trailerId = trailer_id
 32: 
 33: [object_type]
 34: abbreviation = LCD
 35: full_name = Load Compliance Check (Driver/Loader)
 36: 
 37: [fields]
 38: id_field = lcdInspectionId:noggin_reference:string
 39: date_field = date:inspection_date:datetime
 40: lcdInspectionId = lcd_inspection_id:string
 41: date = inspection_date:datetime
 42: inspectedBy = inspected_by:string
 43: vehicle = vehicle_hash:hash:vehicle
 44: vehicleId = vehicle_id:string
 45: trailer = trailer_hash:hash:trailer
 46: trailerId = trailer_id:string
 47: trailer2 = trailer2_hash:hash:trailer
 48: trailerId2 = trailer2_id:string
 49: trailer3 = trailer3_hash:hash:trailer
 50: trailerId3 = trailer3_id:string
 51: totalLoadMassTrailer1 = total_load_mass_trailer1:string
 52: totalLoadMassTrailer2 = total_load_mass_trailer2:string
 53: totalLoadMassTrailer3 = total_load_mass_trailer3:string
 54: jobNumber = job_number:string
 55: runNumber = run_number:string
 56: customerClient = customer_client:string
 57: driver = driver:bool
 58: loader = loader:bool
 59: loaderPhotoAttachment = loader_photo_attachment:string
 60: driverLoaderName = driver_loader_name:string
 61: goldstarOrContactorList = goldstar_or_contractor:string
 62: contractorName = contractor_name:string
 63: whichDepartmentDoesTheLoadBelongTo = department_hash:hash:department
 64: team = team_hash:hash:team
 65: mass = mass:string
 66: isYourLoadCompliantWithTheLoadRestraintGuide2004Ye = compliance_yes:bool
 67: isYourLoadCompliantWithTheLoadRestraintGuide2004No = compliance_no:bool
 68: freeTextWhyIsTheLoadNotCompliant = non_compliance_reason:string
 69: commentsActions = comments_actions:string
 70: straps = straps:bool
 71: noOfStraps = no_of_straps:string
 72: chains = chains:bool
 73: noOfChains = no_of_chains:string
 74: 
 75: [template]
 76: content = 
 77:     ============================================================
 78:     <full_name>
 79:     RECORD GENERATED: <generation_date>
 80:     ============================================================
 81:     
 82:     LCD Inspection ID:     <lcdInspectionId>
 83:     
 84:     Date:                  <date>
 85:     
 86:     Inspected By:          <inspectedBy>
 87:     
 88:     Vehicle:               <vehicle_resolved>
 89:     
 90:     Vehicle ID:            <vehicleId>
 91:     
 92:     Trailer:               <trailer_resolved>
 93:     
 94:     Trailer ID:            <trailerId>
 95:     
 96:     <if:trailer2>
 97:     Trailer 2:             <trailer2_resolved>
 98:     
 99:     Trailer 2 ID:          <trailerId2>
100:     
101:         <if:trailer3>
102:     Trailer 3:             <trailer3_resolved>
103:     
104:     Trailer 3 ID:          <trailerId3>
105:         </if:trailer3>
106:     </if:trailer2>
107:     
108:     <if:jobNumber>
109:     Job Number:            <jobNumber>
110:     </if:jobNumber>
111:     <if:runNumber>
112:     Run Number:            <runNumber>
113:     </if:runNumber>
114:     
115:     Driver/Loader Name:    <driverLoaderName>
116:     
117:     Department:            <department_resolved>
118:     
119:     Team:                  <team_resolved>
120:     
121:     <if:isYourLoadCompliantWithTheLoadRestraintGuide2004Ye>
122:     Load Compliance:       COMPLIANT
123:     </if:isYourLoadCompliantWithTheLoadRestraintGuide2004Ye>
124:     <if:isYourLoadCompliantWithTheLoadRestraintGuide2004No>
125:     Load Compliance:       NOT COMPLIANT
126:     <if:freeTextWhyIsTheLoadNotCompliant>
127:     Reason:                <freeTextWhyIsTheLoadNotCompliant>
128:     </if:freeTextWhyIsTheLoadNotCompliant>
129:     </if:isYourLoadCompliantWithTheLoadRestraintGuide2004No>
130:     
131:     Attachments:           <attachment_count>
132:     
133:     <if:show_json_payload_in_text_file>
134:     ------------------------------------------------------------
135:     COMPLETE TECHNICAL DATA (JSON FORMAT)
136:     ------------------------------------------------------------
137:     
138:     <json_payload>
139:     </if:show_json_payload_in_text_file>
</file>

<file path="config/load_compliance_check_supervisor_manager_config.ini">
  1: [object_type]
  2: abbreviation = LCS
  3: full_name = Load Compliance Check (Supervisor/Manager)
  4: 
  5: [api]
  6: endpoint = /rest/object/loadComplianceCheckSupervisorManager/$tip
  7: object_type = Load Compliance Check (Supervisor/Manager)
  8: 
  9: [csv_import]
 10: lcsInspectionId = noggin_reference
 11: date = inspection_date
 12: inspectedBy = inspected_by
 13: driverLoaderName = driver_loader_name
 14: team = team
 15: vehicle = vehicle
 16: vehicleId = vehicle_id
 17: trailer = trailer
 18: trailerId = trailer_id
 19: 
 20: [fields]
 21: id_field = lcsInspectionId:noggin_reference:string
 22: date_field = date:inspection_date:datetime
 23: lcsInspectionId = lcs_inspection_id:string
 24: date = inspection_date:datetime
 25: inspectedBy = inspected_by:string
 26: vehicle = vehicle_hash:hash:vehicle
 27: vehicleId = vehicle_id:string
 28: trailer = trailer_hash:hash:trailer
 29: trailerId = trailer_id:string
 30: trailer2 = trailer2_hash:hash:trailer
 31: trailerId2 = trailer2_id:string
 32: trailer3 = trailer3_hash:hash:trailer
 33: trailerId3 = trailer3_id:string
 34: jobNumber = job_number:string
 35: runNumber = run_number:string
 36: customerClient = customer_client:string
 37: driverLoaderName = driver_loader_name:string
 38: goldstarOrContactorList = goldstar_or_contractor:string
 39: contractorName = contractor_name:string
 40: whichDepartmentDoesTheLoadBelongTo = department_hash:hash:department
 41: team = team_hash:hash:team
 42: mass = mass:string
 43: vehicleIsAppropriateForTheTaskIEMeetsMassDimension = vehicle_appropriate_yes:bool
 44: vehicleIsAppropriateForTheTaskIEMeetsMassDimensionN = vehicle_appropriate_no:bool
 45: vehicleIsAppropriateForTheTaskIEMeetsMassDimensionNA = vehicle_appropriate_na:bool
 46: vehicleIsAppropriateForTheTaskText = vehicle_appropriate_text:string
 47: loadDistributedCorrectlyOverTheVehicleAxlesY = load_distributed_yes:bool
 48: loadDistributedCorrectlyOverTheVehicleAxlesN = load_distributed_no:bool
 49: loadDistributedCorrectlyOverTheVehicleAxlesNa = load_distributed_na:bool
 50: loadDistributedCorrectlyText = load_distributed_text:string
 51: loadAreEitherSittingOnTimberRubberOrAntiSlipMateri = load_sitting_yes:bool
 52: loadAreEitherSittingOnTimberRubberOrAntiSlipMateriN = load_sitting_no:bool
 53: loadAreEitherSittingOnTimberRubberOrAntiSlipMateriNA = load_sitting_na:bool
 54: loadAreEitherSittingText = load_sitting_text:string
 55: haveGalasCornersBeenAppliedToCoilsY = galas_corners_yes:bool
 56: haveGalasCornersBeenAppliedToCoilsN = galas_corners_no:bool
 57: haveGalasCornersBeenAppliedToCoilsNa = galas_corners_na:bool
 58: haveGalasCornersBeenAppliedToCoilsText = galas_corners_text:string
 59: numberOfLashingsUsedAppropriateForTheLoadY = lashings_appropriate_yes:bool
 60: numberOfLashingsUsedAppropriateForTheLoadN = lashings_appropriate_no:bool
 61: numberOfLashingsUsedAppropriateForTheLoadNa = lashings_appropriate_na:bool
 62: numberOfLashingsText = lashings_text:string
 63: loadsAreNotToBeRestrainedAtLowLashingAngle30Y = low_lashing_angle_yes:bool
 64: loadsAreNotToBeRestrainedAtLowLashingAngle30N = low_lashing_angle_no:bool
 65: loadsAreNotToBeRestrainedAtLowLashingAngle30Na = low_lashing_angle_na:bool
 66: loadsAreNotToBeRestrainedText = low_lashing_angle_text:string
 67: additionalRestraintUsedForItemsThatCanBeDislodgedF = additional_restraint_yes:bool
 68: additionalRestraintUsedForItemsThatCanBeDislodgedFN = additional_restraint_no:bool
 69: additionalRestraintUsedForItemsThatCanBeDislodgedFNA = additional_restraint_na:bool
 70: additionalRestraintUsedForItemsText = additional_restraint_text:string
 71: noLooseItemsEGDunnageChainsStrapsEtcAreLeftOnTheLo = no_loose_items_yes:bool
 72: noLooseItemsEGDunnageChainsStrapsEtcAreLeftOnTheLoN = no_loose_items_no:bool
 73: noLooseItemsEGDunnageChainsStrapsEtcAreLeftOnTheLoNA = no_loose_items_na:bool
 74: noLooseItemsText = no_loose_items_text:string
 75: loadDoesNotExceedHeadboardHeightByMoreThanHalfTheT = headboard_height_yes:bool
 76: loadDoesNotExceedHeadboardHeightByMoreThanHalfTheTN = headboard_height_no:bool
 77: loadDoesNotExceedHeadboardHeightByMoreThanHalfTheTNA = headboard_height_na:bool
 78: loadDoesNotExceedHeadboardText = headboard_height_text:string
 79: loadDoesNotExceedMassOrDimensionOverhangRequiremenY = mass_dimension_yes:bool
 80: loadDoesNotExceedMassOrDimensionOverhangRequiremen = mass_dimension_no:bool
 81: loadDoesNotExceedMassOrDimensionOverhangRequiremenNA = mass_dimension_na:bool
 82: loadDoesNotExceedMassText = mass_dimension_text:string
 83: allPalletsSkidsOrFramesAreInGoodConditionAndAbleToY = pallets_condition_yes:bool
 84: allPalletsSkidsOrFramesAreInGoodConditionAndAbleToN = pallets_condition_no:bool
 85: allPalletsSkidsOrFramesAreInGoodConditionAndAbleToNA = pallets_condition_na:bool
 86: allPalletsSkidsOrFramesText = pallets_condition_text:string
 87: tailgatesSideAndCentrePinsAreSecuredToTheTrailerEGY = tailgates_secured_yes:bool
 88: tailgatesSideAndCentrePinsAreSecuredToTheTrailerEGN = tailgates_secured_no:bool
 89: tailgatesSideAndCentrePinsAreSecuredToTheTrailerEGNA = tailgates_secured_na:bool
 90: tailgatesSideAndCentreText = tailgates_secured_text:string
 91: allLashingsAreAnchoredSecurelyToTheTrailerIEHooksNY = lashings_anchored_yes:bool
 92: allLashingsAreAnchoredSecurelyToTheTrailerIEHooksNN = lashings_anchored_no:bool
 93: allLashingsAreAnchoredSecurelyToTheTrailerIEHooksNA = lashings_anchored_na:bool
 94: allLashingsAreAnchoredText = lashings_anchored_text:string
 95: allLashingsPositionedInLineWithDunnageAndBearersFoY = lashings_positioned_yes:bool
 96: allLashingsPositionedInLineWithDunnageAndBearersFoN = lashings_positioned_no:bool
 97: allLashingsPositionedInLineWithDunnageAndBearersFoNA = lashings_positioned_na:bool
 98: allLashingsPositionedText = lashings_positioned_text:string
 99: noRectangularDunnageOnTheShortEdgeYes = no_rectangular_dunnage_yes:bool
100: noRectangularDunnageOnTheShortEdgeNo = no_rectangular_dunnage_no:bool
101: noRectangularDunnageOnTheShortEdgeNA = no_rectangular_dunnage_na:bool
102: noRectangularDunnageText = no_rectangular_dunnage_text:string
103: appropriateStrapAndOrProductProtectionIsInPlaceEGEY = strap_protection_yes:bool
104: appropriateStrapAndOrProductProtectionIsInPlaceEGEN = strap_protection_no:bool
105: appropriateStrapAndOrProductProtectionIsInPlaceEGENA = strap_protection_na:bool
106: appropriateStrapAndOrProductText = strap_protection_text:string
107: palletJacksAreParkedAndSecuredYes = pallet_jacks_secured_yes:bool
108: palletJacksAreParkedAndSecuredNo = pallet_jacks_secured_no:bool
109: palletJacksAreParkedAndSecuredNA = pallet_jacks_secured_na:bool
110: palletJacksAreParkedText = pallet_jacks_secured_text:string
111: dangerousGoodsLoadsHaveGatesInPlaceYes = dangerous_goods_gates_yes:bool
112: dangerousGoodsLoadsHaveGatesInPlaceNo = dangerous_goods_gates_no:bool
113: dangerousGoodsLoadsHaveGatesInPlaceNA = dangerous_goods_gates_na:bool
114: dangerousGoodsLoadsText = dangerous_goods_gates_text:string
115: restraintEquipmentInGoodWorkingConditionChainsTensY = restraint_equipment_yes:bool
116: restraintEquipmentInGoodWorkingConditionChainsTensN = restraint_equipment_no:bool
117: restraintEquipmentInGoodWorkingConditionChainsTenNA = restraint_equipment_na:bool
118: restraintEquipmentText = restraint_equipment_text:string
119: dunnageIsAlignedWithSufficientClampDownForceToKeepY = dunnage_aligned_yes:bool
120: dunnageIsAlignedWithSufficientClampDownForceToKeepN = dunnage_aligned_no:bool
121: dunnageIsAlignedWithSufficientClampDownForceToKeepNA = dunnage_aligned_na:bool
122: dunnageIsAlignedText = dunnage_aligned_text:string
123: productProtectionIsInPlaceToPreventScratchesAndProY = product_protection_yes:bool
124: productProtectionIsInPlaceToPreventScratchesAndProN = product_protection_no:bool
125: productProtectionIsInPlaceToPreventScratchesAndProNA = product_protection_na:bool
126: productProtectionText = product_protection_text:string
127: toolDuunageBoxesRacksSecuredYes = tool_boxes_secured_yes:bool
128: toolDuunageBoxesRacksSecuredNo = tool_boxes_secured_no:bool
129: toolDuunageBoxesRacksSecuredNA = tool_boxes_secured_na:bool
130: toolDuunageBoxesText = tool_boxes_secured_text:string
131: gluts = gluts:bool
132: noOfGluts = no_of_gluts:string
133: straps = straps:bool
134: noOfStraps = no_of_straps:string
135: webbings = webbings:bool
136: noOfWebbings = no_of_webbings:string
137: chains = chains:bool
138: noOfChains = no_of_chains:string
139: commentsActions = comments_actions:string
140: isTheLoadCompliantWithTheLoadRestraintGuideY = compliance_yes:bool
141: isTheLoadCompliantWithTheLoadRestraintGuideN = compliance_no:bool
142: freeTextWhyIsTheLoadNotCompliant = non_compliance_reason:string
143: 
144: [output]
145: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
146: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
147: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
148: show_json_payload_in_text_file = true
149: show_all_fields = false
150: filename_image_stub = photo
151: unknown_response_output_text = Unknown
152: 
153: [template]
154: content = 
155:     ============================================================
156:     <full_name>
157:     RECORD GENERATED: <generation_date>
158:     ============================================================
159:     
160:     Inspection ID:         <lcsInspectionId>
161:     Date:                  <date>
162:     Inspected By:          <inspectedBy>
163:     
164:     Vehicle:               <vehicle_resolved>
165:     Vehicle ID:            <vehicleId>
166:     
167:     Trailer:               <trailer_resolved>
168:     Trailer ID:            <trailerId>
169:     
170:     <if:trailer2>
171:     Trailer 2:             <trailer2_resolved>
172:     Trailer 2 ID:          <trailerId2>
173:     
174:         <if:trailer3>
175:     Trailer 3:             <trailer3_resolved>
176:     Trailer 3 ID:          <trailerId3>
177:         </if:trailer3>
178:     </if:trailer2>
179:     
180:     <if:jobNumber>
181:     Job Number:            <jobNumber>
182:     </if:jobNumber>
183:     <if:runNumber>
184:     Run Number:            <runNumber>
185:     </if:runNumber>
186:     <if:customerClient>
187:     Customer/Client:       <customerClient>
188:     </if:customerClient>
189:     
190:     Driver/Loader Name:    <driverLoaderName>
191:     Goldstar/Contractor:   <goldstarOrContactorList>
192:     <if:contractorName>
193:     Contractor Name:       <contractorName>
194:     </if:contractorName>
195:     
196:     Department:            <department_resolved>
197:     Team:                  <team_resolved>
198:     
199:     <if:mass>
200:     Mass:                  <mass>
201:     </if:mass>
202:     
203:     <if:show_all_fields>
204:     ------------------------------------------------------------
205:     LOAD COMPLIANCE CHECKS
206:     ------------------------------------------------------------
207:     Vehicle Appropriate for Task:              <vehicleIsAppropriateForTheTaskIEMeetsMassDimension>
208:     <if:vehicleIsAppropriateForTheTaskText>
209:       Comment: <vehicleIsAppropriateForTheTaskText>
210:     </if:vehicleIsAppropriateForTheTaskText>
211:     
212:     Load Distributed Correctly:                <loadDistributedCorrectlyOverTheVehicleAxlesY>
213:     <if:loadDistributedCorrectlyText>
214:       Comment: <loadDistributedCorrectlyText>
215:     </if:loadDistributedCorrectlyText>
216:     
217:     Load on Timber/Rubber/Anti-Slip:           <loadAreEitherSittingOnTimberRubberOrAntiSlipMateri>
218:     <if:loadAreEitherSittingText>
219:       Comment: <loadAreEitherSittingText>
220:     </if:loadAreEitherSittingText>
221:     
222:     Galas Corners Applied to Coils:            <haveGalasCornersBeenAppliedToCoilsY>
223:     <if:haveGalasCornersBeenAppliedToCoilsText>
224:       Comment: <haveGalasCornersBeenAppliedToCoilsText>
225:     </if:haveGalasCornersBeenAppliedToCoilsText>
226:     
227:     Number of Lashings Appropriate:            <numberOfLashingsUsedAppropriateForTheLoadY>
228:     <if:numberOfLashingsText>
229:       Comment: <numberOfLashingsText>
230:     </if:numberOfLashingsText>
231:     
232:     No Low Lashing Angle (<30deg):             <loadsAreNotToBeRestrainedAtLowLashingAngle30Y>
233:     <if:loadsAreNotToBeRestrainedText>
234:       Comment: <loadsAreNotToBeRestrainedText>
235:     </if:loadsAreNotToBeRestrainedText>
236:     
237:     Additional Restraint for Dislodgeable:     <additionalRestraintUsedForItemsThatCanBeDislodgedF>
238:     <if:additionalRestraintUsedForItemsText>
239:       Comment: <additionalRestraintUsedForItemsText>
240:     </if:additionalRestraintUsedForItemsText>
241:     
242:     No Loose Items on Load:                    <noLooseItemsEGDunnageChainsStrapsEtcAreLeftOnTheLo>
243:     <if:noLooseItemsText>
244:       Comment: <noLooseItemsText>
245:     </if:noLooseItemsText>
246:     
247:     Load Not Exceeding Headboard Height:       <loadDoesNotExceedHeadboardHeightByMoreThanHalfTheT>
248:     <if:loadDoesNotExceedHeadboardText>
249:       Comment: <loadDoesNotExceedHeadboardText>
250:     </if:loadDoesNotExceedHeadboardText>
251:     
252:     Load Not Exceeding Mass/Dimension:         <loadDoesNotExceedMassOrDimensionOverhangRequiremenY>
253:     <if:loadDoesNotExceedMassText>
254:       Comment: <loadDoesNotExceedMassText>
255:     </if:loadDoesNotExceedMassText>
256:     
257:     Pallets/Skids/Frames in Good Condition:    <allPalletsSkidsOrFramesAreInGoodConditionAndAbleToY>
258:     <if:allPalletsSkidsOrFramesText>
259:       Comment: <allPalletsSkidsOrFramesText>
260:     </if:allPalletsSkidsOrFramesText>
261:     
262:     Tailgates/Pins Secured:                    <tailgatesSideAndCentrePinsAreSecuredToTheTrailerEGY>
263:     <if:tailgatesSideAndCentreText>
264:       Comment: <tailgatesSideAndCentreText>
265:     </if:tailgatesSideAndCentreText>
266:     
267:     All Lashings Anchored Securely:            <allLashingsAreAnchoredSecurelyToTheTrailerIEHooksNY>
268:     <if:allLashingsAreAnchoredText>
269:       Comment: <allLashingsAreAnchoredText>
270:     </if:allLashingsAreAnchoredText>
271:     
272:     Lashings Positioned with Dunnage:          <allLashingsPositionedInLineWithDunnageAndBearersFoY>
273:     <if:allLashingsPositionedText>
274:       Comment: <allLashingsPositionedText>
275:     </if:allLashingsPositionedText>
276:     
277:     No Rectangular Dunnage on Short Edge:      <noRectangularDunnageOnTheShortEdgeYes>
278:     <if:noRectangularDunnageText>
279:       Comment: <noRectangularDunnageText>
280:     </if:noRectangularDunnageText>
281:     
282:     Strap/Product Protection in Place:         <appropriateStrapAndOrProductProtectionIsInPlaceEGEY>
283:     <if:appropriateStrapAndOrProductText>
284:       Comment: <appropriateStrapAndOrProductText>
285:     </if:appropriateStrapAndOrProductText>
286:     
287:     Pallet Jacks Parked and Secured:           <palletJacksAreParkedAndSecuredYes>
288:     <if:palletJacksAreParkedText>
289:       Comment: <palletJacksAreParkedText>
290:     </if:palletJacksAreParkedText>
291:     
292:     Dangerous Goods Gates in Place:            <dangerousGoodsLoadsHaveGatesInPlaceYes>
293:     <if:dangerousGoodsLoadsText>
294:       Comment: <dangerousGoodsLoadsText>
295:     </if:dangerousGoodsLoadsText>
296:     
297:     Restraint Equipment in Good Condition:     <restraintEquipmentInGoodWorkingConditionChainsTensY>
298:     <if:restraintEquipmentText>
299:       Comment: <restraintEquipmentText>
300:     </if:restraintEquipmentText>
301:     
302:     Dunnage Aligned with Clamp Force:          <dunnageIsAlignedWithSufficientClampDownForceToKeepY>
303:     <if:dunnageIsAlignedText>
304:       Comment: <dunnageIsAlignedText>
305:     </if:dunnageIsAlignedText>
306:     
307:     Product Protection (Scratches/Damage):     <productProtectionIsInPlaceToPreventScratchesAndProY>
308:     <if:productProtectionText>
309:       Comment: <productProtectionText>
310:     </if:productProtectionText>
311:     
312:     Tool/Dunnage Boxes Secured:                <toolDuunageBoxesRacksSecuredYes>
313:     <if:toolDuunageBoxesText>
314:       Comment: <toolDuunageBoxesText>
315:     </if:toolDuunageBoxesText>
316:     
317:     ------------------------------------------------------------
318:     RESTRAINT EQUIPMENT USED
319:     ------------------------------------------------------------
320:     <if:gluts>
321:     Gluts:                 <gluts>
322:     Number of Gluts:       <noOfGluts>
323:     </if:gluts>
324:     <if:straps>
325:     Straps:                <straps>
326:     Number of Straps:      <noOfStraps>
327:     </if:straps>
328:     <if:webbings>
329:     Webbings:              <webbings>
330:     Number of Webbings:    <noOfWebbings>
331:     </if:webbings>
332:     <if:chains>
333:     Chains:                <chains>
334:     Number of Chains:      <noOfChains>
335:     </if:chains>
336:     
337:     <if:commentsActions>
338:     ------------------------------------------------------------
339:     COMMENTS/ACTIONS
340:     ------------------------------------------------------------
341:     <commentsActions>
342:     </if:commentsActions>
343:     </if:show_all_fields>
344:     
345:     Attachments:           <attachment_count>
346:     
347:     <if:show_json_payload_in_text_file>
348:     ------------------------------------------------------------
349:     COMPLETE TECHNICAL DATA (JSON FORMAT)
350:     ------------------------------------------------------------
351:     
352:     <json_payload>
353:     </if:show_json_payload_in_text_file>
</file>

<file path="config/site_observations_config.ini">
  1: [object_type]
  2: abbreviation = SO
  3: full_name = Site Observations
  4: 
  5: [api]
  6: endpoint = /rest/object/siteObservations/$tip
  7: object_type = Site Observations
  8: 
  9: [csv_import]
 10: siteObservationId = noggin_reference
 11: date = inspection_date
 12: siteManager = site_manager
 13: department = department
 14: inspectedBy = inspected_by
 15: personInvolved = person_involved
 16: 
 17: [fields]
 18: id_field = siteObservationId:noggin_reference:string
 19: date_field = date:observation_date:datetime
 20: siteObservationId = site_observation_id:string
 21: date = observation_date:datetime
 22: siteManager = site_manager:string
 23: department = department_hash:hash:department
 24: inspectedBy = inspected_by:string
 25: personInvolved = person_involved:string
 26: vehicleS = vehicles:string
 27: details1 = details_1:string
 28: findings1 = findings_1:string
 29: details2 = details_2:string
 30: findings2 = findings_2:string
 31: details3 = details_3:string
 32: findings3 = findings_3:string
 33: details4 = details_4:string
 34: findings4 = findings_4:string
 35: summary1 = summary_1:string
 36: summary2 = summary_2:string
 37: summary3 = summary_3:string
 38: summary4 = summary_4:string
 39: observation1Checkbox = observation_1_checkbox:bool
 40: observation2Checkbox = observation_2_checkbox:bool
 41: observation3Checkbox = observation_3_checkbox:bool
 42: observation4Checkbox = observation_4_checkbox:bool
 43: 
 44: [output]
 45: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
 46: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
 47: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
 48: show_json_payload_in_text_file = true
 49: show_all_fields = false
 50: filename_image_stub = photo
 51: unknown_response_output_text = Unknown
 52: 
 53: [template]
 54: content = 
 55:     ============================================================
 56:     <full_name>
 57:     RECORD GENERATED: <generation_date>
 58:     ============================================================
 59:     
 60:     Observation ID:        <siteObservationId>
 61:     Date:                  <date>
 62:     Inspected By:          <inspectedBy>
 63:     
 64:     <if:siteManager>
 65:     Site Manager:          <siteManager>
 66:     </if:siteManager>
 67:     
 68:     Department:            <department_resolved>
 69:     
 70:     <if:personInvolved>
 71:     Person(s) Involved:    <personInvolved>
 72:     </if:personInvolved>
 73:     
 74:     <if:vehicleS>
 75:     Vehicle(s):            <vehicleS>
 76:     </if:vehicleS>
 77:     
 78:     <if:show_all_fields>
 79:     <if:observation1Checkbox>
 80:     ------------------------------------------------------------
 81:     OBSERVATION 1
 82:     ------------------------------------------------------------
 83:     <if:details1>
 84:     Details:
 85:     <details1>
 86:     </if:details1>
 87:     
 88:     <if:findings1>
 89:     Findings:
 90:     <findings1>
 91:     </if:findings1>
 92:     
 93:     <if:summary1>
 94:     Summary:
 95:     <summary1>
 96:     </if:summary1>
 97:     </if:observation1Checkbox>
 98:     
 99:     <if:observation2Checkbox>
100:     ------------------------------------------------------------
101:     OBSERVATION 2
102:     ------------------------------------------------------------
103:     <if:details2>
104:     Details:
105:     <details2>
106:     </if:details2>
107:     
108:     <if:findings2>
109:     Findings:
110:     <findings2>
111:     </if:findings2>
112:     
113:     <if:summary2>
114:     Summary:
115:     <summary2>
116:     </if:summary2>
117:     </if:observation2Checkbox>
118:     
119:     <if:observation3Checkbox>
120:     ------------------------------------------------------------
121:     OBSERVATION 3
122:     ------------------------------------------------------------
123:     <if:details3>
124:     Details:
125:     <details3>
126:     </if:details3>
127:     
128:     <if:findings3>
129:     Findings:
130:     <findings3>
131:     </if:findings3>
132:     
133:     <if:summary3>
134:     Summary:
135:     <summary3>
136:     </if:summary3>
137:     </if:observation3Checkbox>
138:     
139:     <if:observation4Checkbox>
140:     ------------------------------------------------------------
141:     OBSERVATION 4
142:     ------------------------------------------------------------
143:     <if:details4>
144:     Details:
145:     <details4>
146:     </if:details4>
147:     
148:     <if:findings4>
149:     Findings:
150:     <findings4>
151:     </if:findings4>
152:     
153:     <if:summary4>
154:     Summary:
155:     <summary4>
156:     </if:summary4>
157:     </if:observation4Checkbox>
158:     </if:show_all_fields>
159:     
160:     Attachments:           <attachment_count>
161:     
162:     <if:show_json_payload_in_text_file>
163:     ------------------------------------------------------------
164:     COMPLETE TECHNICAL DATA (JSON FORMAT)
165:     ------------------------------------------------------------
166:     
167:     <json_payload>
168:     </if:show_json_payload_in_text_file>
</file>

<file path="config/trailer_audits_config.ini">
  1: [object_type]
  2: abbreviation = TA
  3: full_name = Trailer Audits
  4: 
  5: [api]
  6: endpoint = /rest/object/trailerAudits/$tip
  7: object_type = Trailer Audits
  8: 
  9: [csv_import]
 10: trailerAuditId = noggin_reference
 11: date = inspection_date
 12: inspectedBy = inspected_by
 13: team = team
 14: vehicle = vehicle
 15: rego = rego
 16: regularDriver = regular_driver
 17: 
 18: [fields]
 19: id_field = trailerAuditId:noggin_reference:string
 20: date_field = date:inspection_date:datetime
 21: trailerAuditId = trailer_audit_id:string
 22: date = inspection_date:datetime
 23: team = team_hash:hash:team
 24: inspectedBy = inspected_by:string
 25: regularDriver = regular_driver:string
 26: driverPresentYes = driver_present_yes:bool
 27: driverPresentNo = driver_present_no:bool
 28: vehicle = vehicle_hash:hash:vehicle
 29: rego = rego:string
 30: externallyExcellent = externally_excellent:bool
 31: externallyGood = externally_good:bool
 32: externallyFair = externally_fair:bool
 33: externallyUnacceptable = externally_unacceptable:bool
 34: internallyToolboxExcellent = internally_excellent:bool
 35: internallyToolboxGood = internally_good:bool
 36: internallyToolboxFair = internally_fair:bool
 37: internallyToolboxUnacceptable = internally_unacceptable:bool
 38: comments = comments:string
 39: revolvingBeaconYes = revolving_beacon_yes:bool
 40: revolvingBeaconNo = revolving_beacon_no:bool
 41: revolvingBeaconNA = revolving_beacon_na:bool
 42: spareTyreYes = spare_tyre_yes:bool
 43: spareTyreNo = spare_tyre_no:bool
 44: spareTyreNA = spare_tyre_na:bool
 45: fireExtinguisherYes = fire_extinguisher_yes:bool
 46: fireExtinguisherNo = fire_extinguisher_no:bool
 47: fireExtinguisherNA = fire_extinguisher_na:bool
 48: loadRestraintEquipmentYes = load_restraint_yes:bool
 49: loadRestraintEquipmentNo = load_restraint_no:bool
 50: loadRestraintEquipmentNA = load_restraint_na:bool
 51: noOfWebbingStraps = no_of_webbing_straps:string
 52: noOfChains = no_of_chains:string
 53: noOfGluts = no_of_gluts:string
 54: driverComment = driver_comment:string
 55: 
 56: [output]
 57: folder_pattern = {abbreviation}/{year}/{month}/{date} {inspection_id}
 58: attachment_pattern = {abbreviation}_{inspection_id}_{date}_{stub}_{sequence}.jpg
 59: session_log_header = TIMESTAMP	TIP	INSPECTION_ID	ATTACHMENTS_COUNT	ATTACHMENT_FILENAMES
 60: show_json_payload_in_text_file = true
 61: show_all_fields = false
 62: filename_image_stub = photo
 63: unknown_response_output_text = Unknown
 64: 
 65: [template]
 66: content = 
 67:     ============================================================
 68:     <full_name>
 69:     RECORD GENERATED: <generation_date>
 70:     ============================================================
 71:     
 72:     Audit ID:              <trailerAuditId>
 73:     Date:                  <date>
 74:     Inspected By:          <inspectedBy>
 75:     
 76:     Vehicle:               <vehicle_resolved>
 77:     Rego:                  <rego>
 78:     
 79:     <if:regularDriver>
 80:     Regular Driver:        <regularDriver>
 81:     </if:regularDriver>
 82:     
 83:     Team:                  <team_resolved>
 84:     
 85:     <if:show_all_fields>
 86:     ------------------------------------------------------------
 87:     DRIVER PRESENCE
 88:     ------------------------------------------------------------
 89:     Driver Present:        <driverPresentYes>
 90:     
 91:     ------------------------------------------------------------
 92:     EXTERNAL CONDITION
 93:     ------------------------------------------------------------
 94:     Excellent:             <externallyExcellent>
 95:     Good:                  <externallyGood>
 96:     Fair:                  <externallyFair>
 97:     Unacceptable:          <externallyUnacceptable>
 98:     
 99:     ------------------------------------------------------------
100:     INTERNAL/TOOLBOX CONDITION
101:     ------------------------------------------------------------
102:     Excellent:             <internallyToolboxExcellent>
103:     Good:                  <internallyToolboxGood>
104:     Fair:                  <internallyToolboxFair>
105:     Unacceptable:          <internallyToolboxUnacceptable>
106:     
107:     <if:comments>
108:     ------------------------------------------------------------
109:     COMMENTS
110:     ------------------------------------------------------------
111:     <comments>
112:     </if:comments>
113:     
114:     ------------------------------------------------------------
115:     EQUIPMENT CHECKS
116:     ------------------------------------------------------------
117:     Revolving Beacon:      <revolvingBeaconYes>
118:     Spare Tyre:            <spareTyreYes>
119:     Fire Extinguisher:     <fireExtinguisherYes>
120:     Load Restraint Equip:  <loadRestraintEquipmentYes>
121:     
122:     ------------------------------------------------------------
123:     RESTRAINT EQUIPMENT COUNT
124:     ------------------------------------------------------------
125:     <if:noOfWebbingStraps>
126:     Webbing Straps:        <noOfWebbingStraps>
127:     </if:noOfWebbingStraps>
128:     <if:noOfChains>
129:     Chains:                <noOfChains>
130:     </if:noOfChains>
131:     <if:noOfGluts>
132:     Gluts:                 <noOfGluts>
133:     </if:noOfGluts>
134:     
135:     <if:driverComment>
136:     ------------------------------------------------------------
137:     DRIVER COMMENT
138:     ------------------------------------------------------------
139:     <driverComment>
140:     </if:driverComment>
141:     </if:show_all_fields>
142:     
143:     Attachments:           <attachment_count>
144:     
145:     <if:show_json_payload_in_text_file>
146:     ------------------------------------------------------------
147:     COMPLETE TECHNICAL DATA (JSON FORMAT)
148:     ------------------------------------------------------------
149:     
150:     <json_payload>
151:     </if:show_json_payload_in_text_file>
</file>

<file path="test_circuit_breaker.py">
 1: from common import ConfigLoader, LoggerManager, CircuitBreaker, CircuitBreakerError
 2: from common import UNKNOWN_TEXT
 3: import logging
 4: 
 5: config: ConfigLoader = ConfigLoader(
 6:     'config/base_config.ini',
 7:     'config/load_compliance_check_driver_loader_config.ini'
 8: )
 9: 
10: logger_manager: LoggerManager = LoggerManager(config, script_name='test_circuit_breaker')
11: logger_manager.configure_application_logger()
12: 
13: logger: logging.Logger = logging.getLogger(__name__)
14: 
15: circuit_breaker: CircuitBreaker = CircuitBreaker(config)
16: 
17: logger.info("Testing circuit breaker with simulated requests")
18: 
19: # Simulate 5 successes
20: for i in range(5):
21:     circuit_breaker.before_request()
22:     circuit_breaker.record_success()
23:     logger.info(f"Success {i+1} - State: {circuit_breaker.get_state().value}")
24: 
25: # Simulate failures to trip circuit
26: for i in range(10):
27:     try:
28:         circuit_breaker.before_request()
29:         if i % 2 == 0:
30:             circuit_breaker.record_failure()
31:             logger.info(f"Failure {i+1} - State: {circuit_breaker.get_state().value}")
32:         else:
33:             circuit_breaker.record_success()
34:             logger.info(f"Success {i+1} - State: {circuit_breaker.get_state().value}")
35:     except CircuitBreakerError as e:
36:         logger.warning(f"Request blocked: {e}")
37: 
38: stats = circuit_breaker.get_statistics()
39: logger.info(f"Final statistics: {stats}")
40: 
41: print("\n Circuit breaker test complete")
42: print(f"State: {stats['state']}")
43: print(f"Total requests: {stats['total_requests']}")
44: print(f"Failure rate: {stats['failure_rate']}%")
</file>

<file path="test_hash_manager.py">
 1: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager
 2: from common import UNKNOWN_TEXT
 3: from pathlib import Path
 4: import logging
 5: 
 6: config: ConfigLoader = ConfigLoader(
 7:     'config/base_config.ini',
 8:     'config/load_compliance_check_driver_loader_config.ini'
 9: )
10: 
11: logger_manager: LoggerManager = LoggerManager(config, script_name='test_hash_manager')
12: logger_manager.configure_application_logger()
13: 
14: logger: logging.Logger = logging.getLogger(__name__)
15: 
16: try:
17:     logger.info("Initialising hash manager...")
18:     db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
19:     hash_manager: HashManager = HashManager(config, db_manager)
20:     
21:     lookup_table_path: str = 'lookup_table.csv'
22:     if Path(lookup_table_path).exists():
23:         logger.info(f"Migrating hash lookups from {lookup_table_path}")
24:         imported, skipped = hash_manager.migrate_lookup_table_from_csv(lookup_table_path)
25:         logger.info(f"Migration complete: {imported} imported, {skipped} skipped")
26:     else:
27:         logger.warning(f"lookup_table.csv not found at {lookup_table_path}")
28:     
29:     logger.info("Testing hash lookup...")
30:     test_hash: str = "9d28a0b2a601d53eddcd10b433fd9716a172193e425cc964d263507be3be578e"
31:     result: str = hash_manager.lookup_hash('vehicle', test_hash, 'test_tip', 'LCD-TEST')
32:     logger.info(f"Lookup result: {result}")
33:     
34:     logger.info("Checking unknown hashes...")
35:     unknown = hash_manager.get_unknown_hashes(resolved=False)
36:     logger.info(f"Unknown hashes: {len(unknown)}")
37:     
38:     logger.info(" All hash manager tests passed")
39:     
40: except Exception as e:
41:     logger.error(f"Test failed: {e}", exc_info=True)
42: finally:
43:     if 'db_manager' in locals():
44:         db_manager.close_all()
</file>

<file path="common/object_types.py">
  1: """
  2: Object Type Detection Module
  3: 
  4: Centralised object type detection logic used by:
  5: - csv_importer.py
  6: - sftp_download_tips.py
  7: - noggin_processor_unified.py
  8: - Any future importers
  9: 
 10: This ensures consistent detection across all entry points.
 11: 
 12: CHANGE LOG:
 13: - Renamed 'LCC' to 'LCD' to match Noggin's naming convention (Load Compliance Check Driver/Loader)
 14: """
 15: 
 16: from __future__ import annotations
 17: from typing import Dict, List, Optional, Tuple, Any
 18: from dataclasses import dataclass
 19: import logging
 20: 
 21: logger: logging.Logger = logging.getLogger(__name__)
 22: 
 23: 
 24: @dataclass
 25: class ObjectTypeConfig:
 26:     """Configuration for an object type"""
 27:     abbreviation: str
 28:     full_name: str
 29:     id_column: str
 30:     id_prefix: str
 31:     config_file: str
 32:     date_column: str = 'date'
 33: 
 34: 
 35: # Centralised object type definitions
 36: OBJECT_TYPES: Dict[str, ObjectTypeConfig] = {
 37:     'CCC': ObjectTypeConfig(
 38:         abbreviation='CCC',
 39:         full_name='Coupling Compliance Check',
 40:         id_column='couplingId',
 41:         id_prefix='C - ',
 42:         config_file='coupling_compliance_check_config.ini'
 43:     ),
 44:     'FPI': ObjectTypeConfig(
 45:         abbreviation='FPI',
 46:         full_name='Forklift Prestart Inspection',
 47:         id_column='forkliftPrestartInspectionId',
 48:         id_prefix='FL - Inspection - ',
 49:         config_file='forklift_prestart_inspection_config.ini'
 50:     ),
 51:     'LCS': ObjectTypeConfig(
 52:         abbreviation='LCS',
 53:         full_name='Load Compliance Check Supervisor/Manager',
 54:         id_column='lcsInspectionId',
 55:         id_prefix='LCS - ',
 56:         config_file='load_compliance_check_supervisor_manager_config.ini'
 57:     ),
 58:     'LCD': ObjectTypeConfig(
 59:         abbreviation='LCD',
 60:         full_name='Load Compliance Check Driver/Loader',
 61:         id_column='lcdInspectionId',
 62:         id_prefix='LCD - ',
 63:         config_file='load_compliance_check_driver_loader_config.ini'
 64:     ),
 65:     'SO': ObjectTypeConfig(
 66:         abbreviation='SO',
 67:         full_name='Site Observations',
 68:         id_column='siteObservationId',
 69:         id_prefix='SO - ',
 70:         config_file='site_observations_config.ini'
 71:     ),
 72:     'TA': ObjectTypeConfig(
 73:         abbreviation='TA',
 74:         full_name='Trailer Audits',
 75:         id_column='trailerAuditId',
 76:         id_prefix='TA - ',
 77:         config_file='trailer_audits_config.ini'
 78:     )
 79: }
 80: 
 81: # Build reverse lookup: id_column -> ObjectTypeConfig
 82: ID_COLUMN_TO_TYPE: Dict[str, ObjectTypeConfig] = {
 83:     config.id_column: config for config in OBJECT_TYPES.values()
 84: }
 85: 
 86: # lowercase lookup for case-insensitive matching
 87: ID_COLUMN_TO_TYPE_LOWER: Dict[str, ObjectTypeConfig] = {
 88:     config.id_column.lower(): config for config in OBJECT_TYPES.values()
 89: }
 90: 
 91: 
 92: def detect_object_type_from_headers(headers: List[str]) -> Optional[ObjectTypeConfig]:
 93:     """
 94:     Detect object type from CSV column headers
 95:     
 96:     Args:
 97:         headers: List of column header strings
 98:         
 99:     Returns:
100:         ObjectTypeConfig if detected, None otherwise
101:     """
102:     clean_headers = [h.strip() for h in headers]
103:     clean_headers_lower = [h.lower() for h in clean_headers]
104:     
105:     for id_column, config in ID_COLUMN_TO_TYPE.items():
106:         if id_column in clean_headers:
107:             logger.debug(f"Detected object type {config.abbreviation} via column '{id_column}'")
108:             return config
109:     
110:     for id_column_lower, config in ID_COLUMN_TO_TYPE_LOWER.items():
111:         if id_column_lower in clean_headers_lower:
112:             logger.debug(f"Detected object type {config.abbreviation} via column '{config.id_column}' (case-insensitive)")
113:             return config
114:     
115:     logger.warning(f"Could not detect object type from headers: {clean_headers[:10]}")
116:     return None
117: 
118: 
119: def get_object_type_by_abbreviation(abbreviation: str) -> Optional[ObjectTypeConfig]:
120:     """Get object type config by abbreviation (LCD, CCC, etc.)"""
121:     return OBJECT_TYPES.get(abbreviation.upper())
122: 
123: 
124: def get_object_type_by_full_name(full_name: str) -> Optional[ObjectTypeConfig]:
125:     """Get object type config by full name"""
126:     for config in OBJECT_TYPES.values():
127:         if config.full_name.lower() == full_name.lower():
128:             return config
129:     return None
130: 
131: 
132: def get_all_object_types() -> List[ObjectTypeConfig]:
133:     """Get list of all supported object types (excluding aliases)"""
134:     # Filter out the LCC alias to avoid duplicates
135:     seen_abbrevs = set()
136:     result = []
137:     for abbrev, config in OBJECT_TYPES.items():
138:         if abbrev == 'LCC':
139:             continue
140:         if config.abbreviation not in seen_abbrevs:
141:             seen_abbrevs.add(config.abbreviation)
142:             result.append(config)
143:     return result
144: 
145: 
146: def get_id_column_for_type(abbreviation: str) -> Optional[str]:
147:     """Get the ID column name for an object type"""
148:     config = OBJECT_TYPES.get(abbreviation.upper())
149:     return config.id_column if config else None
150: 
151: 
152: def find_column_index(headers: List[str], column_name: str) -> int:
153:     """
154:     Find column index by name (case-insensitive, handles whitespace)
155:     
156:     Args:
157:         headers: List of column headers
158:         column_name: Column name to find
159:         
160:     Returns:
161:         Column index, or -1 if not found
162:     """
163:     clean_headers = [h.strip().lower() for h in headers]
164:     target = column_name.strip().lower()
165:     
166:     try:
167:         return clean_headers.index(target)
168:     except ValueError:
169:         return -1
170: 
171: 
172: def extract_row_data(row: List[str], headers: List[str], 
173:                      object_config: ObjectTypeConfig) -> Dict[str, Any]:
174:     """
175:     Extract standardised data from a CSV row
176:     
177:     Args:
178:         row: CSV row data
179:         headers: Column headers
180:         object_config: Object type configuration
181:         
182:     Returns:
183:         Dictionary with: tip, inspection_id, inspection_date, object_type, abbreviation
184:     """
185:     tip_value = row[0].strip() if row else ''
186:     
187:     id_index = find_column_index(headers, object_config.id_column)
188:     inspection_id = None
189:     if id_index >= 0 and len(row) > id_index:
190:         inspection_id = row[id_index].strip() or None
191:     
192:     date_index = find_column_index(headers, object_config.date_column)
193:     inspection_date = None
194:     if date_index >= 0 and len(row) > date_index:
195:         inspection_date = row[date_index].strip() or None
196:     
197:     return {
198:         'tip': tip_value,
199:         'inspection_id': inspection_id,
200:         'inspection_date': inspection_date,
201:         'object_type': object_config.abbreviation,
202:         'abbreviation': object_config.abbreviation
203:     }
204: 
205: 
206: def detect_object_type(headers: List[str]) -> Optional[str]:
207:     """
208:     Detect object type and return abbreviation (legacy compatibility)
209:     
210:     Args:
211:         headers: List of column headers
212:         
213:     Returns:
214:         Object type abbreviation or None
215:     """
216:     config = detect_object_type_from_headers(headers)
217:     return config.abbreviation if config else None
218: 
219: 
220: def load_object_types() -> Dict[str, ObjectTypeConfig]:
221:     """Return the object types dictionary (for external access)"""
222:     return OBJECT_TYPES
223: 
224: 
225: if __name__ == "__main__":
226:     test_cases = [
227:         ['nogginId', 'couplingId', 'date', 'team'],
228:         [' ', 'forkliftPrestartInspectionId', 'date'],
229:         ['nogginId', 'lcsInspectionId', 'date'],
230:         ['nogginId', 'lcdInspectionId', 'date'],
231:         ['nogginId', 'siteObservationId', 'date'],
232:         [' ', 'trailerAuditId', 'date'],
233:         ['unknown', 'columns', 'here'],
234:     ]
235:     
236:     print("Object Type Detection Test:")
237:     print("-" * 60)
238:     
239:     for headers in test_cases:
240:         config = detect_object_type_from_headers(headers)
241:         if config:
242:             print(f"Headers: {headers[:3]}...")
243:             print(f"  -> {config.abbreviation} ({config.full_name})")
244:         else:
245:             print(f"Headers: {headers[:3]}...")
246:             print(f"  -> NOT DETECTED")
247:         print()
248:     
249:     print("\nAll supported object types:")
250:     for config in get_all_object_types():
251:         print(f"  {config.abbreviation}: {config.full_name}")
252:         print(f"      ID column: {config.id_column}")
253:         print(f"      Config: {config.config_file}")
</file>

<file path="hash_lookup_sync.py">
  1: """
  2: Hash Lookup Sync - Synchronise hash_lookup table from Noggin exports
  3: 
  4: Reads asset and site CSV exports from Noggin and populates the hash_lookup table.
  5: Supports local file processing, pending folder scanning, and SFTP download.
  6: 
  7: Usage:
  8:     python hash_lookup_sync.py --process-pending
  9:     python hash_lookup_sync.py --asset-file /path/to/asset.csv --site-file /path/to/site.csv
 10:     python hash_lookup_sync.py --sftp
 11:     python hash_lookup_sync.py --stats
 12: """
 13: 
 14: from __future__ import annotations
 15: import argparse
 16: import logging
 17: import shutil
 18: import sys
 19: from dataclasses import dataclass, field
 20: from datetime import datetime
 21: from pathlib import Path
 22: from typing import Optional
 23: 
 24: import pandas as pd
 25: 
 26: logger: logging.Logger = logging.getLogger(__name__)
 27: 
 28: 
 29: @dataclass
 30: class SyncStatistics:
 31:     """Statistics from a sync operation"""
 32:     assets_processed: int = 0
 33:     assets_skipped: int = 0
 34:     sites_processed: int = 0
 35:     sites_skipped: int = 0
 36:     total_inserted: int = 0
 37:     errors: list = field(default_factory=list)
 38: 
 39: 
 40: # Mapping from Noggin assetType to lookup_type
 41: ASSET_TYPE_MAPPING: dict[str, str] = {
 42:     'PRIME MOVER': 'vehicle',
 43:     'RIGID': 'vehicle',
 44:     'VEHICLE': 'vehicle',
 45:     'LIGHT VEHICLE': 'vehicle',
 46:     'FORKLIFT': 'vehicle',
 47:     'TRAILER': 'trailer',
 48:     'DROPDECK': 'trailer',
 49:     'DOLLY': 'trailer',
 50:     'UHF': 'uhf',
 51: }
 52: 
 53: # Site name patterns that indicate department vs team
 54: DEPARTMENT_PATTERNS: tuple[str, ...] = (
 55:     '- Drivers',
 56:     '- Admin',
 57:     'Transport',
 58:     'Workshop',
 59:     'Distribution',
 60: )
 61: 
 62: 
 63: def get_default_paths() -> dict[str, Path]:
 64:     """
 65:     Get default paths based on script location.
 66:     Assumes script is in /home/noggin_admin/scripts/ and etl folder is a sibling.
 67:     """
 68:     script_dir = Path(__file__).parent.resolve()
 69:     etl_dir = Path('/mnt/data/noggin/etl')
 70:     
 71:     return {
 72:         'hash_sync_pending': etl_dir / 'hash_sync' / 'pending',
 73:         'hash_sync_processed': etl_dir / 'hash_sync' / 'processed',
 74:         'hash_sync_error': etl_dir / 'hash_sync' / 'error',
 75:         'sftp_downloads': etl_dir / 'sftp' / 'downloads',
 76:         'sftp_archive': etl_dir / 'sftp' / 'archive',
 77:         'log': etl_dir / 'log',
 78:     }
 79: 
 80: 
 81: def determine_asset_lookup_type(asset_type: Optional[str]) -> str:
 82:     """
 83:     Determine lookup_type from Noggin assetType.
 84:     
 85:     The asset_type parameter comes directly from Noggin's export and can be values
 86:     like 'PRIME MOVER', 'TRAILER', 'uhf', etc. This function normalises these to
 87:     broader categories used for filtering: vehicle, trailer, uhf, or unknown.
 88:     """
 89:     if not asset_type or pd.isna(asset_type):
 90:         return 'unknown'
 91:     
 92:     asset_type_upper = str(asset_type).strip().upper()
 93:     return ASSET_TYPE_MAPPING.get(asset_type_upper, 'unknown')
 94: 
 95: 
 96: def format_source_type(raw_type: Optional[str]) -> str:
 97:     """
 98:     Format source_type as CamelCase.
 99:     
100:     Converts Noggin's raw type values (e.g., 'PRIME MOVER', 'businessUnit') into
101:     consistent CamelCase format (e.g., 'PrimeMover', 'BusinessUnit') for storage.
102:     """
103:     if not raw_type or pd.isna(raw_type):
104:         return 'Unknown'
105:     
106:     raw_str = str(raw_type).strip()
107:     
108:     # Handle already camelCase values (e.g., businessUnit, virtualForReporting)
109:     if ' ' not in raw_str and raw_str[0].islower():
110:         return raw_str[0].upper() + raw_str[1:]
111:     
112:     # Convert UPPER CASE or Title Case to CamelCase
113:     words = raw_str.replace('_', ' ').split()
114:     return ''.join(word.capitalize() for word in words)
115: 
116: 
117: def determine_site_lookup_type(site_name: str, site_type: Optional[str]) -> str:
118:     """
119:     Determine lookup_type from site name patterns and siteType.
120:     
121:     Sites with names containing patterns like '- Drivers' or '- Admin' are classified
122:     as departments. Sites with siteType 'team' that don't match department patterns
123:     are classified as teams. Everything else (businessUnit, virtualForReporting) 
124:     becomes department.
125:     """
126:     site_name_str = str(site_name) if site_name else ''
127:     
128:     # Check for department patterns in site name
129:     for pattern in DEPARTMENT_PATTERNS:
130:         if pattern in site_name_str:
131:             return 'department'
132:     
133:     # siteType 'team' maps to lookup_type 'team' unless name suggests department
134:     if site_type and str(site_type).strip().lower() == 'team':
135:         if any(p in site_name_str for p in DEPARTMENT_PATTERNS):
136:             return 'department'
137:         return 'team'
138:     
139:     # Everything else (businessUnit, virtualForReporting) is department
140:     return 'department'
141: 
142: 
143: def format_site_resolved_value(goldstar_id: Optional[str], site_name: Optional[str]) -> str:
144:     """
145:     Format resolved_value for sites as '<goldstarId> - <siteName>'.
146:     
147:     If goldstar_id is missing or empty, returns just the site name.
148:     """
149:     name = str(site_name).strip() if site_name and not pd.isna(site_name) else 'Unknown'
150:     
151:     if goldstar_id and not pd.isna(goldstar_id):
152:         gid = str(goldstar_id).strip()
153:         return f"{gid} - {name}"
154:     
155:     return name
156: 
157: 
158: def detect_file_type(csv_path: Path) -> Optional[str]:
159:     """
160:     Detect whether a CSV file is an asset export or site export.
161:     
162:     Reads the header row and looks for distinctive columns:
163:     - 'assetType' or 'assetName' indicates asset export
164:     - 'siteType' or 'siteName' indicates site export
165:     
166:     Returns 'asset', 'site', or None if indeterminate.
167:     """
168:     try:
169:         df = pd.read_csv(csv_path, nrows=0, encoding='utf-8-sig')
170:         columns = set(df.columns)
171:         
172:         # Asset exports have assetType and/or assetName columns
173:         if 'assetType' in columns or 'assetName' in columns:
174:             return 'asset'
175:         
176:         # Site exports have siteType and/or siteName columns
177:         if 'siteType' in columns or 'siteName' in columns:
178:             return 'site'
179:         
180:         logger.warning(f"Could not determine file type for {csv_path.name}. Columns: {columns}")
181:         return None
182:         
183:     except Exception as e:
184:         logger.error(f"Error reading {csv_path}: {e}")
185:         return None
186: 
187: 
188: def load_asset_export(csv_path: Path) -> pd.DataFrame:
189:     """
190:     Load and validate asset export CSV.
191:     
192:     Expects columns: nogginId, assetName, assetType (at minimum).
193:     """
194:     logger.info(f"Loading asset export from {csv_path}")
195:     
196:     df = pd.read_csv(csv_path, encoding='utf-8-sig')
197:     
198:     required_columns = ['nogginId', 'assetName', 'assetType']
199:     missing = [col for col in required_columns if col not in df.columns]
200:     
201:     if missing:
202:         raise ValueError(f"Asset CSV missing required columns: {missing}")
203:     
204:     logger.info(f"Loaded {len(df)} asset records")
205:     return df
206: 
207: 
208: def load_site_export(csv_path: Path) -> pd.DataFrame:
209:     """
210:     Load and validate site export CSV.
211:     
212:     Expects columns: nogginId, siteName, goldstarId, siteType (at minimum).
213:     """
214:     logger.info(f"Loading site export from {csv_path}")
215:     
216:     df = pd.read_csv(csv_path, encoding='utf-8-sig')
217:     
218:     required_columns = ['nogginId', 'siteName', 'goldstarId', 'siteType']
219:     missing = [col for col in required_columns if col not in df.columns]
220:     
221:     if missing:
222:         raise ValueError(f"Site CSV missing required columns: {missing}")
223:     
224:     logger.info(f"Loaded {len(df)} site records")
225:     return df
226: 
227: 
228: def process_assets(df: pd.DataFrame) -> list[tuple[str, str, str, str]]:
229:     """
230:     Process asset DataFrame into hash_lookup records.
231:     
232:     Returns list of (tip_hash, lookup_type, resolved_value, source_type) tuples.
233:     """
234:     records = []
235:     skipped = 0
236:     
237:     for _, row in df.iterrows():
238:         tip_hash = row.get('nogginId')
239:         asset_name = row.get('assetName')
240:         asset_type = row.get('assetType')
241:         
242:         if not tip_hash or pd.isna(tip_hash):
243:             skipped += 1
244:             continue
245:         
246:         tip_hash = str(tip_hash).strip()
247:         
248:         # Keep expired/empty name records with 'Unknown' as resolved value
249:         if not asset_name or pd.isna(asset_name):
250:             asset_name = 'Unknown'
251:             logger.debug(f"Asset {tip_hash[:16]}... has no name, using 'Unknown'")
252:         
253:         resolved_value = str(asset_name).strip()
254:         lookup_type = determine_asset_lookup_type(asset_type)
255:         source_type = format_source_type(asset_type)
256:         
257:         if lookup_type == 'unknown':
258:             logger.warning(f"Unknown asset type '{asset_type}' for {resolved_value} ({tip_hash[:16]}...)")
259:         
260:         records.append((tip_hash, lookup_type, resolved_value, source_type))
261:     
262:     logger.info(f"Processed {len(records)} assets, skipped {skipped}")
263:     return records
264: 
265: 
266: def process_sites(df: pd.DataFrame) -> list[tuple[str, str, str, str]]:
267:     """
268:     Process site DataFrame into hash_lookup records.
269:     
270:     Returns list of (tip_hash, lookup_type, resolved_value, source_type) tuples.
271:     """
272:     records = []
273:     skipped = 0
274:     
275:     for _, row in df.iterrows():
276:         tip_hash = row.get('nogginId')
277:         site_name = row.get('siteName')
278:         goldstar_id = row.get('goldstarId')
279:         site_type = row.get('siteType')
280:         
281:         if not tip_hash or pd.isna(tip_hash):
282:             skipped += 1
283:             continue
284:         
285:         tip_hash = str(tip_hash).strip()
286:         
287:         if not site_name or pd.isna(site_name):
288:             skipped += 1
289:             logger.debug(f"Site {tip_hash[:16]}... has no name, skipping")
290:             continue
291:         
292:         resolved_value = format_site_resolved_value(goldstar_id, site_name)
293:         lookup_type = determine_site_lookup_type(site_name, site_type)
294:         source_type = format_source_type(site_type)
295:         
296:         records.append((tip_hash, lookup_type, resolved_value, source_type))
297:     
298:     logger.info(f"Processed {len(records)} sites, skipped {skipped}")
299:     return records
300: 
301: 
302: def sync_to_database(
303:     db_manager: 'DatabaseConnectionManager',
304:     records: list[tuple[str, str, str, str]],
305:     truncate_first: bool = True
306: ) -> int:
307:     """
308:     Sync records to hash_lookup table.
309:     
310:     When truncate_first is True (default), clears the table before inserting.
311:     This ensures the table exactly matches the authoritative source files.
312:     """
313:     if truncate_first:
314:         logger.info("Truncating hash_lookup table")
315:         db_manager.execute_update("TRUNCATE TABLE hash_lookup")
316:     
317:     if not records:
318:         logger.warning("No records to insert")
319:         return 0
320:     
321:     logger.info(f"Inserting {len(records)} records into hash_lookup")
322:     
323:     insert_query = """
324:         INSERT INTO hash_lookup (tip_hash, lookup_type, resolved_value, source_type, created_at, updated_at)
325:         VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
326:         ON CONFLICT (tip_hash) DO UPDATE SET
327:             lookup_type = EXCLUDED.lookup_type,
328:             resolved_value = EXCLUDED.resolved_value,
329:             source_type = EXCLUDED.source_type,
330:             updated_at = CURRENT_TIMESTAMP
331:     """
332:     
333:     inserted = 0
334:     for tip_hash, lookup_type, resolved_value, source_type in records:
335:         try:
336:             db_manager.execute_update(insert_query, (tip_hash, lookup_type, resolved_value, source_type))
337:             inserted += 1
338:         except Exception as e:
339:             logger.error(f"Failed to insert {tip_hash[:16]}...: {e}")
340:     
341:     logger.info(f"Successfully inserted {inserted} records")
342:     return inserted
343: 
344: 
345: def scan_pending_folder(pending_path: Path) -> tuple[Optional[Path], Optional[Path]]:
346:     """
347:     Scan pending folder for asset and site CSV files.
348:     
349:     Auto-detects file types by examining headers. Returns tuple of 
350:     (asset_file, site_file). Either or both may be None if not found.
351:     """
352:     if not pending_path.exists():
353:         logger.warning(f"Pending folder does not exist: {pending_path}")
354:         return None, None
355:     
356:     csv_files = list(pending_path.glob('*.csv'))
357:     
358:     if not csv_files:
359:         logger.info(f"No CSV files found in {pending_path}")
360:         return None, None
361:     
362:     logger.info(f"Found {len(csv_files)} CSV file(s) in pending folder")
363:     
364:     asset_file: Optional[Path] = None
365:     site_file: Optional[Path] = None
366:     
367:     for csv_path in csv_files:
368:         file_type = detect_file_type(csv_path)
369:         
370:         if file_type == 'asset':
371:             if asset_file:
372:                 logger.warning(f"Multiple asset files found. Using {asset_file.name}, ignoring {csv_path.name}")
373:             else:
374:                 asset_file = csv_path
375:                 logger.info(f"Detected asset file: {csv_path.name}")
376:                 
377:         elif file_type == 'site':
378:             if site_file:
379:                 logger.warning(f"Multiple site files found. Using {site_file.name}, ignoring {csv_path.name}")
380:             else:
381:                 site_file = csv_path
382:                 logger.info(f"Detected site file: {csv_path.name}")
383:                 
384:         else:
385:             logger.warning(f"Unknown file type, skipping: {csv_path.name}")
386:     
387:     return asset_file, site_file
388: 
389: 
390: def download_from_sftp(config: 'ConfigLoader', paths: dict[str, Path]) -> tuple[Optional[Path], Optional[Path]]:
391:     """
392:     Download latest export files from SFTP.
393:     
394:     Connects to Noggin's SFTP server, downloads the two most recent CSV files,
395:     and returns paths to the downloaded asset and site files.
396:     """
397:     try:
398:         import paramiko
399:     except ImportError:
400:         logger.error("paramiko not installed. Run: pip install paramiko")
401:         return None, None
402:     
403:     host = config.get('sftp', 'host')
404:     port = config.getint('sftp', 'port')
405:     username = config.get('sftp', 'username')
406:     key_path = config.get('sftp', 'private_key_path')
407:     remote_path = config.get('sftp', 'remote_path')
408:     local_path = paths['sftp_downloads']
409:     
410:     local_path.mkdir(parents=True, exist_ok=True)
411:     
412:     logger.info(f"Connecting to SFTP {host}:{port}")
413:     
414:     try:
415:         key = paramiko.RSAKey.from_private_key_file(key_path)
416:     except Exception as e:
417:         logger.error(f"Failed to load private key from {key_path}: {e}")
418:         return None, None
419:     
420:     transport = paramiko.Transport((host, port))
421:     
422:     try:
423:         transport.connect(username=username, pkey=key)
424:         sftp = paramiko.SFTPClient.from_transport(transport)
425:         
426:         files = sftp.listdir(remote_path)
427:         csv_files = [f for f in files if f.startswith('exported-file-') and f.endswith('.csv')]
428:         
429:         if len(csv_files) < 2:
430:             logger.error(f"Expected at least 2 CSV files, found {len(csv_files)}")
431:             return None, None
432:         
433:         # Sort by modification time (newest first)
434:         csv_files_with_time = []
435:         for f in csv_files:
436:             stat = sftp.stat(f"{remote_path}/{f}")
437:             csv_files_with_time.append((f, stat.st_mtime))
438:         
439:         csv_files_with_time.sort(key=lambda x: x[1], reverse=True)
440:         
441:         downloaded = []
442:         for filename, _ in csv_files_with_time[:2]:
443:             remote_file = f"{remote_path}/{filename}"
444:             local_file = local_path / filename
445:             
446:             logger.info(f"Downloading {filename}")
447:             sftp.get(remote_file, str(local_file))
448:             downloaded.append(local_file)
449:         
450:         sftp.close()
451:         
452:         # Detect file types
453:         asset_file = None
454:         site_file = None
455:         
456:         for file_path in downloaded:
457:             file_type = detect_file_type(file_path)
458:             if file_type == 'asset':
459:                 asset_file = file_path
460:             elif file_type == 'site':
461:                 site_file = file_path
462:         
463:         if not asset_file or not site_file:
464:             logger.error("Could not identify asset and site files from downloaded CSVs")
465:             return None, None
466:         
467:         logger.info(f"Asset file: {asset_file.name}")
468:         logger.info(f"Site file: {site_file.name}")
469:         
470:         return asset_file, site_file
471:         
472:     except Exception as e:
473:         logger.error(f"SFTP operation failed: {e}")
474:         return None, None
475:     
476:     finally:
477:         transport.close()
478: 
479: 
480: def archive_file(file_path: Path, archive_folder: Path) -> Path:
481:     """
482:     Move processed file to archive folder with timestamp suffix.
483:     """
484:     archive_folder.mkdir(parents=True, exist_ok=True)
485:     
486:     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
487:     new_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
488:     dest_path = archive_folder / new_name
489:     
490:     shutil.move(str(file_path), str(dest_path))
491:     logger.info(f"Archived {file_path.name} to {dest_path}")
492:     
493:     return dest_path
494: 
495: 
496: def move_to_error(file_path: Path, error_folder: Path) -> Path:
497:     """
498:     Move failed file to error folder with timestamp suffix.
499:     """
500:     error_folder.mkdir(parents=True, exist_ok=True)
501:     
502:     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
503:     new_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
504:     dest_path = error_folder / new_name
505:     
506:     shutil.move(str(file_path), str(dest_path))
507:     logger.warning(f"Moved {file_path.name} to error folder: {dest_path}")
508:     
509:     return dest_path
510: 
511: 
512: def get_statistics(db_manager: 'DatabaseConnectionManager') -> dict:
513:     """
514:     Get current hash_lookup statistics.
515:     """
516:     stats = {}
517:     
518:     type_counts = db_manager.execute_query_dict("""
519:         SELECT lookup_type, COUNT(*) as count
520:         FROM hash_lookup
521:         GROUP BY lookup_type
522:         ORDER BY lookup_type
523:     """)
524:     
525:     for row in type_counts:
526:         stats[row['lookup_type']] = row['count']
527:     
528:     source_counts = db_manager.execute_query_dict("""
529:         SELECT source_type, COUNT(*) as count
530:         FROM hash_lookup
531:         WHERE source_type IS NOT NULL
532:         GROUP BY source_type
533:         ORDER BY source_type
534:     """)
535:     
536:     stats['by_source_type'] = {row['source_type']: row['count'] for row in source_counts}
537:     
538:     total = db_manager.execute_query_dict("SELECT COUNT(*) as count FROM hash_lookup")
539:     stats['total'] = total[0]['count'] if total else 0
540:     
541:     return stats
542: 
543: 
544: def print_statistics(stats: dict) -> None:
545:     """Print formatted statistics to console."""
546:     print("\n" + "=" * 60)
547:     print("HASH LOOKUP STATISTICS")
548:     print("=" * 60)
549:     
550:     print("\nBy Lookup Type:")
551:     print("-" * 40)
552:     for lookup_type in ['vehicle', 'trailer', 'team', 'department', 'uhf', 'unknown']:
553:         count = stats.get(lookup_type, 0)
554:         if count > 0:
555:             print(f"  {lookup_type:<15} {count:>6}")
556:     
557:     print(f"\n  {'TOTAL':<15} {stats.get('total', 0):>6}")
558:     
559:     if 'by_source_type' in stats and stats['by_source_type']:
560:         print("\nBy Source Type (Noggin assetType/siteType):")
561:         print("-" * 40)
562:         for source_type, count in sorted(stats['by_source_type'].items()):
563:             print(f"  {source_type:<20} {count:>6}")
564:     
565:     print("=" * 60 + "\n")
566: 
567: 
568: def main() -> int:
569:     """Main entry point."""
570:     parser = argparse.ArgumentParser(
571:         description='Synchronise hash_lookup table from Noggin exports',
572:         formatter_class=argparse.RawDescriptionHelpFormatter,
573:         epilog="""
574: Examples:
575:     # Process files from pending folder (auto-detects asset vs site)
576:     python hash_lookup_sync.py --process-pending
577:     
578:     # Sync from specific files
579:     python hash_lookup_sync.py --asset-file assets.csv --site-file sites.csv
580:     
581:     # Download and sync from SFTP
582:     python hash_lookup_sync.py --sftp
583:     
584:     # Show current statistics only
585:     python hash_lookup_sync.py --stats
586:     
587:     # Dry run (no database changes)
588:     python hash_lookup_sync.py --process-pending --dry-run
589:         """
590:     )
591:     
592:     parser.add_argument('--process-pending', action='store_true',
593:                        help='Process files from pending folder with auto-detection')
594:     parser.add_argument('--asset-file', type=Path, help='Path to asset export CSV')
595:     parser.add_argument('--site-file', type=Path, help='Path to site export CSV')
596:     parser.add_argument('--sftp', action='store_true', help='Download files from SFTP')
597:     parser.add_argument('--stats', action='store_true', help='Show statistics only')
598:     parser.add_argument('--dry-run', action='store_true', help='Process files without database changes')
599:     parser.add_argument('--no-archive', action='store_true', help='Do not archive processed files')
600:     parser.add_argument('--config', type=Path, default=Path('config/base_config.ini'),
601:                        help='Path to base config file')
602:     
603:     args = parser.parse_args()
604:     
605:     # Import dependencies here to allow --help without loading modules
606:     sys.path.insert(0, str(Path(__file__).parent))
607:     from common import ConfigLoader, LoggerManager, DatabaseConnectionManager
608:     
609:     # Get default paths based on script location
610:     paths = get_default_paths()
611:     
612:     # Load configuration
613:     config = ConfigLoader(str(args.config))
614:     
615:     # Configure logging
616:     logger_manager = LoggerManager(config, script_name='hash_lookup_sync')
617:     logger_manager.configure_application_logger()
618:     
619:     # Connect to database
620:     db_manager = DatabaseConnectionManager(config)
621:     
622:     try:
623:         # Stats only mode
624:         if args.stats:
625:             stats = get_statistics(db_manager)
626:             print_statistics(stats)
627:             return 0
628:         
629:         # Determine file sources
630:         asset_file: Optional[Path] = None
631:         site_file: Optional[Path] = None
632:         source_mode: str = ''
633:         
634:         # Track records separately for reporting
635:         asset_records: list = []
636:         site_records: list = []
637:         
638:         if args.process_pending:
639:             source_mode = 'pending'
640:             asset_file, site_file = scan_pending_folder(paths['hash_sync_pending'])
641:             
642:             if not asset_file and not site_file:
643:                 print("\nNo files found in pending folder")
644:                 print(f"  Location: {paths['hash_sync_pending']}")
645:                 print("\nPlace asset and site export CSVs in the pending folder and run again.")
646:                 return 0
647:                 
648:         elif args.sftp:
649:             source_mode = 'sftp'
650:             asset_file, site_file = download_from_sftp(config, paths)
651:             if not asset_file or not site_file:
652:                 logger.error("Failed to download files from SFTP")
653:                 return 1
654:                 
655:         elif args.asset_file and args.site_file:
656:             source_mode = 'manual'
657:             asset_file = args.asset_file
658:             site_file = args.site_file
659:             
660:             if not asset_file.exists():
661:                 logger.error(f"Asset file not found: {asset_file}")
662:                 return 1
663:             if not site_file.exists():
664:                 logger.error(f"Site file not found: {site_file}")
665:                 return 1
666:         else:
667:             parser.print_help()
668:             print("\nError: Specify --process-pending, --sftp, or both --asset-file and --site-file")
669:             return 1
670:         
671:         # Validate we have at least one file
672:         if not asset_file and not site_file:
673:             logger.error("No valid files to process")
674:             return 1
675:         
676:         # Load and process files
677:         logger.info("Starting hash lookup sync")
678:         
679:         all_records = []
680:         processed_files = []
681:         
682:         if asset_file:
683:             try:
684:                 asset_df = load_asset_export(asset_file)
685:                 asset_records = process_assets(asset_df)
686:                 all_records.extend(asset_records)
687:                 processed_files.append(('asset', asset_file, True))
688:                 logger.info(f"Asset records: {len(asset_records)}")
689:             except Exception as e:
690:                 logger.error(f"Failed to process asset file: {e}")
691:                 processed_files.append(('asset', asset_file, False))
692:         
693:         if site_file:
694:             try:
695:                 site_df = load_site_export(site_file)
696:                 site_records = process_sites(site_df)
697:                 all_records.extend(site_records)
698:                 processed_files.append(('site', site_file, True))
699:                 logger.info(f"Site records: {len(site_records)}")
700:             except Exception as e:
701:                 logger.error(f"Failed to process site file: {e}")
702:                 processed_files.append(('site', site_file, False))
703:         
704:         logger.info(f"Total records to sync: {len(all_records)}")
705:         
706:         # Sync to database
707:         if args.dry_run:
708:             logger.info("Dry run mode - no database changes")
709:             print(f"\nDry run complete:")
710:             print(f"  Assets: {len(asset_records)}")
711:             print(f"  Sites: {len(site_records)}")
712:             print(f"  Total: {len(all_records)}")
713:         else:
714:             if all_records:
715:                 inserted = sync_to_database(db_manager, all_records, truncate_first=True)
716:                 
717:                 print(f"\nSync complete:")
718:                 print(f"  Assets processed: {len(asset_records)}")
719:                 print(f"  Sites processed: {len(site_records)}")
720:                 print(f"  Total inserted: {inserted}")
721:             else:
722:                 print("\nNo records to sync")
723:         
724:         # Move processed files to appropriate folders
725:         if not args.no_archive and not args.dry_run:
726:             for file_type, file_path, success in processed_files:
727:                 if success:
728:                     archive_file(file_path, paths['hash_sync_processed'])
729:                 else:
730:                     move_to_error(file_path, paths['hash_sync_error'])
731:         
732:         # Show final statistics
733:         if not args.dry_run and all_records:
734:             stats = get_statistics(db_manager)
735:             print_statistics(stats)
736:         
737:         return 0
738:         
739:     except Exception as e:
740:         logger.error(f"Sync failed: {e}", exc_info=True)
741:         print(f"\nError: {e}")
742:         return 1
743:     
744:     finally:
745:         db_manager.close_all()
746: 
747: 
748: if __name__ == "__main__":
749:     sys.exit(main())
</file>

<file path="service_dashboard.py">
  1: """
  2: Service Dashboard
  3: 
  4: Displays comprehensive status including:
  5: - Service status
  6: - Processing statistics by status
  7: - Object type breakdown
  8: - Hash resolution statistics
  9: - Today's activity
 10: """
 11: 
 12: from __future__ import annotations
 13: from common import UNKNOWN_TEXT
 14: import subprocess
 15: import sys
 16: from datetime import datetime
 17: from typing import Dict, Any, List
 18: 
 19: from common import ConfigLoader, DatabaseConnectionManager
 20: 
 21: 
 22: def get_service_status() -> Dict[str, str]:
 23:     """Get systemd service status"""
 24:     try:
 25:         result = subprocess.run(
 26:             ['systemctl', 'is-active', 'noggin-processor'],
 27:             capture_output=True,
 28:             text=True
 29:         )
 30:         status: str = result.stdout.strip()
 31:         
 32:         result = subprocess.run(
 33:             ['systemctl', 'is-enabled', 'noggin-processor'],
 34:             capture_output=True,
 35:             text=True
 36:         )
 37:         enabled: str = result.stdout.strip()
 38:         
 39:         return {'status': status, 'enabled': enabled}
 40:     except Exception as e:
 41:         return {'status': 'unknown', 'enabled': 'unknown', 'error': str(e)}
 42: 
 43: 
 44: def get_database_statistics(db_manager: DatabaseConnectionManager) -> Dict[str, int]:
 45:     """Get processing statistics from database"""
 46:     query: str = """
 47:         SELECT 
 48:             processing_status,
 49:             COUNT(*) as count
 50:         FROM noggin_data
 51:         GROUP BY processing_status
 52:     """
 53:     
 54:     results: List[Dict[str, Any]] = db_manager.execute_query_dict(query)
 55:     
 56:     stats: Dict[str, int] = {
 57:         'complete': 0,
 58:         'pending': 0,
 59:         'failed': 0,
 60:         'partial': 0,
 61:         'interrupted': 0,
 62:         'api_failed': 0
 63:     }
 64:     
 65:     for row in results:
 66:         status: str = row['processing_status']
 67:         count: int = row['count']
 68:         if status in stats:
 69:             stats[status] = count
 70:         else:
 71:             stats[status] = count
 72:     
 73:     return stats
 74: 
 75: 
 76: def get_object_type_statistics(db_manager: DatabaseConnectionManager) -> Dict[str, Dict[str, int]]:
 77:     """Get processing statistics by object type"""
 78:     query: str = """
 79:         SELECT 
 80:             object_type,
 81:             processing_status,
 82:             COUNT(*) as count
 83:         FROM noggin_data
 84:         GROUP BY object_type, processing_status
 85:         ORDER BY object_type
 86:     """
 87:     
 88:     results: List[Dict[str, Any]] = db_manager.execute_query_dict(query)
 89:     
 90:     stats: Dict[str, Dict[str, int]] = {}
 91:     
 92:     for row in results:
 93:         obj_type = row['object_type'] or 'Unknown'
 94:         status = row['processing_status']
 95:         count = row['count']
 96:         
 97:         if obj_type not in stats:
 98:             stats[obj_type] = {'total': 0, 'complete': 0, 'pending': 0, 'failed': 0}
 99:         
100:         stats[obj_type]['total'] += count
101:         if status in stats[obj_type]:
102:             stats[obj_type][status] = count
103:     
104:     return stats
105: 
106: 
107: def get_hash_statistics(db_manager: DatabaseConnectionManager) -> Dict[str, Dict[str, int]]:
108:     """Get hash lookup statistics"""
109:     stats: Dict[str, Dict[str, int]] = {}
110:     
111:     try:
112:         known_query = """
113:             SELECT lookup_type, COUNT(*) as count 
114:             FROM hash_lookup 
115:             GROUP BY lookup_type
116:         """
117:         known_results = db_manager.execute_query_dict(known_query)
118:         
119:         for row in known_results:
120:             lookup_type = row['lookup_type']
121:             if lookup_type not in stats:
122:                 stats[lookup_type] = {'known': 0, 'unknown': 0}
123:             stats[lookup_type]['known'] = row['count']
124:         
125:         # Try unknown_hashes first, fall back to hash_lookup_unknown
126:         try:
127:             unknown_query = """
128:                 SELECT lookup_type, COUNT(*) as count 
129:                 FROM unknown_hashes 
130:                 WHERE resolved_at IS NULL
131:                 GROUP BY lookup_type
132:             """
133:             unknown_results = db_manager.execute_query_dict(unknown_query)
134:         except:
135:             unknown_query = """
136:                 SELECT lookup_type, COUNT(*) as count 
137:                 FROM hash_lookup_unknown 
138:                 WHERE resolved_at IS NULL
139:                 GROUP BY lookup_type
140:             """
141:             unknown_results = db_manager.execute_query_dict(unknown_query)
142:         
143:         for row in unknown_results:
144:             lookup_type = row['lookup_type']
145:             if lookup_type not in stats:
146:                 stats[lookup_type] = {'known': 0, 'unknown': 0}
147:             stats[lookup_type]['unknown'] = row['count']
148:         
149:     except Exception as e:
150:         pass
151:     
152:     return stats
153: 
154: 
155: def get_recent_activity(db_manager: DatabaseConnectionManager) -> Dict[str, Any]:
156:     """Get recent processing activity"""
157:     query: str = """
158:         SELECT 
159:             COUNT(*) as total_today,
160:             SUM(CASE WHEN processing_status = 'complete' THEN 1 ELSE 0 END) as completed_today
161:         FROM noggin_data
162:         WHERE updated_at >= CURRENT_DATE
163:     """
164:     
165:     result: List[Dict[str, Any]] = db_manager.execute_query_dict(query)
166:     return result[0] if result else {'total_today': 0, 'completed_today': 0}
167: 
168: 
169: def get_sftp_activity(db_manager: DatabaseConnectionManager) -> Dict[str, int]:
170:     """Get SFTP import activity if source_filename column exists"""
171:     try:
172:         query = """
173:             SELECT COUNT(*) as count
174:             FROM noggin_data
175:             WHERE source_filename IS NOT NULL
176:               AND csv_imported_at >= CURRENT_DATE
177:         """
178:         result = db_manager.execute_query_dict(query)
179:         return {'sftp_today': result[0]['count'] if result else 0}
180:     except:
181:         return {'sftp_today': 0}
182: 
183: 
184: def main() -> None:
185:     """Display service dashboard"""
186:     config: ConfigLoader = ConfigLoader(
187:         'config/base_config.ini',
188:         'config/load_compliance_check_driver_loader_config.ini'
189:     )
190:     
191:     db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
192:     
193:     try:
194:         print("\n" + "=" * 80)
195:         print("NOGGIN PROCESSOR SERVICE DASHBOARD")
196:         print("=" * 80)
197:         print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
198:         print("=" * 80)
199:         
200:         # Service Status
201:         service_info: Dict[str, str] = get_service_status()
202:         print("\nSERVICE STATUS:")
203:         print(f"  Active:  {service_info['status'].upper()}")
204:         print(f"  Enabled: {service_info['enabled'].upper()}")
205:         
206:         # Processing Statistics
207:         stats: Dict[str, int] = get_database_statistics(db_manager)
208:         total: int = sum(stats.values())
209:         
210:         print("\nPROCESSING STATISTICS:")
211:         print(f"  Total Records:     {total:,}")
212:         print(f"  Complete:          {stats.get('complete', 0):,}")
213:         print(f"  Pending:           {stats.get('pending', 0):,}")
214:         print(f"  Failed:            {stats.get('failed', 0):,}")
215:         print(f"  Partial:           {stats.get('partial', 0):,}")
216:         print(f"  Interrupted:       {stats.get('interrupted', 0):,}")
217:         print(f"  API Failed:        {stats.get('api_failed', 0):,}")
218:         
219:         # Object Type Breakdown
220:         obj_stats = get_object_type_statistics(db_manager)
221:         if obj_stats:
222:             print("\nOBJECT TYPE BREAKDOWN:")
223:             print(f"  {'Type':<45} {'Total':>8} {'Done':>8} {'Pending':>8}")
224:             print(f"  {'-'*45} {'-'*8} {'-'*8} {'-'*8}")
225:             for obj_type, counts in sorted(obj_stats.items()):
226:                 print(f"  {obj_type:<45} {counts['total']:>8,} {counts.get('complete', 0):>8,} {counts.get('pending', 0):>8,}")
227:         
228:         # Hash Statistics
229:         hash_stats = get_hash_statistics(db_manager)
230:         if hash_stats:
231:             print("\nHASH LOOKUP STATUS:")
232:             total_known = sum(s.get('known', 0) for s in hash_stats.values())
233:             total_unknown = sum(s.get('unknown', 0) for s in hash_stats.values())
234:             print(f"  Total Known:       {total_known:,}")
235:             print(f"  Total Unknown:     {total_unknown:,}")
236:             
237:             if total_unknown > 0:
238:                 print("\n  Unknown by Type:")
239:                 for lookup_type, counts in sorted(hash_stats.items()):
240:                     unknown = counts.get('unknown', 0)
241:                     if unknown > 0:
242:                         print(f"    {lookup_type}: {unknown}")
243:         
244:         # Today's Activity
245:         activity: Dict[str, Any] = get_recent_activity(db_manager)
246:         sftp_activity = get_sftp_activity(db_manager)
247:         
248:         print("\nTODAY'S ACTIVITY:")
249:         print(f"  Total Updated:     {activity.get('total_today', 0):,}")
250:         print(f"  Completed:         {activity.get('completed_today', 0):,}")
251:         if sftp_activity.get('sftp_today', 0) > 0:
252:             print(f"  From SFTP:         {sftp_activity['sftp_today']:,}")
253:         
254:         # Work Queue
255:         work_remaining: int = (
256:             stats.get('pending', 0) + 
257:             stats.get('failed', 0) + 
258:             stats.get('partial', 0) + 
259:             stats.get('interrupted', 0) + 
260:             stats.get('api_failed', 0)
261:         )
262:         print("\nWORK QUEUE:")
263:         print(f"  Remaining:         {work_remaining:,}")
264:         
265:         if total > 0:
266:             completion_rate: float = (stats.get('complete', 0) / total) * 100
267:             print(f"  Completion Rate:   {completion_rate:.1f}%")
268:         
269:         print("\n" + "=" * 80)
270:         
271:     except Exception as e:
272:         print(f"\nError: {e}", file=sys.stderr)
273:         sys.exit(1)
274:     finally:
275:         db_manager.close_all()
276: 
277: 
278: if __name__ == "__main__":
279:     main()
</file>

<file path="test_common.py">
 1: from common import ConfigLoader, ConfigurationError, LoggerManager
 2: from common import UNKNOWN_TEXT
 3: 
 4: try:
 5:     config = ConfigLoader(
 6:         'config/base_config.ini',
 7:         'config/load_compliance_check_driver_loader_config.ini'
 8:     )
 9:     print(f"ConfigLoader imported successfully.\nPostgreSQL config:{config.get_postgresql_config()}")
10: except ConfigurationError as e:
11:     print(f"Configuration error: {e}")
12: except ImportError as e:
13:     print(f"Import error: {e}")
</file>

<file path="web/app.py">
  1: from flask import Flask, render_template, jsonify, request, redirect, url_for, flash
  2: from flask_httpauth import HTTPBasicAuth
  3: from werkzeug.security import generate_password_hash, check_password_hash
  4: from datetime import datetime, timedelta
  5: from pathlib import Path
  6: import sys
  7: 
  8: # Add parent directory to path
  9: sys.path.insert(0, str(Path(__file__).parent.parent))
 10: 
 11: from common import ConfigLoader, DatabaseConnectionManager, HashManager
 12: 
 13: app = Flask(__name__)
 14: app.secret_key = 'a1b5a507e8d554cd54f506f3b1056a71f237309a9f4565b6cc9632d4d3352faa'
 15: 
 16: auth = HTTPBasicAuth()
 17: 
 18: config = ConfigLoader(
 19:     '../config/base_config.ini',
 20:     '../config/load_compliance_check_driver_loader_config.ini'
 21: )
 22: db_manager = DatabaseConnectionManager(config)
 23: hash_manager = HashManager(config, db_manager)
 24: 
 25: # Users (in production, use database)
 26: users = {
 27:     "tifunction": generate_password_hash("BankFreePlay13")
 28: }
 29: 
 30: @auth.verify_password
 31: def verify_password(username, password):
 32:     if username in users and check_password_hash(users.get(username), password):
 33:         return username
 34: 
 35: @app.route('/')
 36: # @auth.login_required
 37: def index():
 38:     """Dashboard home page"""
 39:     try:
 40:         # Get statistics
 41:         stats_query = """
 42:             SELECT 
 43:                 processing_status,
 44:                 COUNT(*) as count
 45:             FROM noggin_data
 46:             GROUP BY processing_status
 47:         """
 48:         stats = db_manager.execute_query_dict(stats_query)
 49:         
 50:         # Today's activity
 51:         today_query = """
 52:             SELECT 
 53:                 COUNT(*) as total_today,
 54:                 SUM(CASE WHEN processing_status = 'complete' THEN 1 ELSE 0 END) as completed_today
 55:             FROM noggin_data
 56:             WHERE updated_at >= CURRENT_DATE
 57:         """
 58:         today_stats = db_manager.execute_query_dict(today_query)[0]
 59:         
 60:         # Recent activity
 61:         recent_query = """
 62:             SELECT 
 63:                 tip,
 64:                 noggin_reference,
 65:                 inspection_date,
 66:                 processing_status,
 67:                 total_attachments,
 68:                 completed_attachment_count,
 69:                 updated_at
 70:             FROM noggin_data
 71:             ORDER BY updated_at DESC
 72:             LIMIT 20
 73:         """
 74:         recent = db_manager.execute_query_dict(recent_query)
 75:         
 76:         return render_template(
 77:             'dashboard.html',
 78:             stats=stats,
 79:             today_stats=today_stats,
 80:             recent=recent,
 81:             current_time=datetime.now()
 82:         )
 83:     except Exception as e:
 84:         return f"Error: {e}", 500
 85: 
 86: @app.route('/api/stats')
 87: @auth.login_required
 88: def api_stats():
 89:     """API endpoint for statistics"""
 90:     try:
 91:         stats_query = """
 92:             SELECT 
 93:                 processing_status,
 94:                 COUNT(*) as count
 95:             FROM noggin_data
 96:             GROUP BY processing_status
 97:         """
 98:         stats = db_manager.execute_query_dict(stats_query)
 99:         
100:         return jsonify({
101:             'success': True,
102:             'stats': stats,
103:             'timestamp': datetime.now().isoformat()
104:         })
105:     except Exception as e:
106:         return jsonify({
107:             'success': False,
108:             'error': str(e)
109:         }), 500
110: 
111: @app.route('/inspections')
112: @auth.login_required
113: def inspections():
114:     """List all inspections"""
115:     page = request.args.get('page', 1, type=int)
116:     per_page = 50
117:     offset = (page - 1) * per_page
118:     
119:     # Get filter parameters
120:     status = request.args.get('status', '')
121:     search = request.args.get('search', '')
122:     
123:     # Build query
124:     where_clauses = []
125:     params = []
126:     
127:     if status:
128:         where_clauses.append("processing_status = %s")
129:         params.append(status)
130:     
131:     if search:
132:         where_clauses.append("(lcd_inspection_id ILIKE %s OR vehicle ILIKE %s OR trailer ILIKE %s)")
133:         params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
134:     
135:     where_sql = " WHERE " + " AND ".join(where_clauses) if where_clauses else ""
136:     
137:     # Get total count
138:     count_query = f"SELECT COUNT(*) as total FROM noggin_data{where_sql}"
139:     total = db_manager.execute_query_dict(count_query, tuple(params))[0]['total']
140:     
141:     # Get inspections
142:     params.extend([per_page, offset])
143:     inspections_query = f"""
144:         SELECT 
145:             tip,
146:             noggin_reference,
147:             inspection_date,
148:             vehicle,
149:             trailer,
150:             department,
151:             team,
152:             processing_status,
153:             total_attachments,
154:             completed_attachment_count,
155:             retry_count,
156:             updated_at
157:         FROM noggin_data
158:         {where_sql}
159:         ORDER BY updated_at DESC
160:         LIMIT %s OFFSET %s
161:     """
162:     inspections_list = db_manager.execute_query_dict(inspections_query, tuple(params))
163:     
164:     total_pages = (total + per_page - 1) // per_page
165:     
166:     return render_template(
167:         'inspections.html',
168:         inspections=inspections_list,
169:         page=page,
170:         total_pages=total_pages,
171:         total=total,
172:         status=status,
173:         search=search
174:     )
175: 
176: @app.route('/inspection/<tip>')
177: @auth.login_required
178: def inspection_detail(tip):
179:     """Inspection detail page"""
180:     try:
181:         # Get inspection
182:         inspection_query = """
183:             SELECT * FROM noggin_data WHERE tip = %s
184:         """
185:         inspection = db_manager.execute_query_dict(inspection_query, (tip,))
186:         
187:         if not inspection:
188:             return "Inspection not found", 404
189:         
190:         inspection = inspection[0]
191:         
192:         # Get attachments
193:         attachments_query = """
194:             SELECT 
195:                 filename,
196:                 file_path,
197:                 attachment_status,
198:                 file_size_bytes,
199:                 download_duration_seconds,
200:                 attachment_validation_status
201:             FROM attachments
202:             WHERE record_tip = %s
203:             ORDER BY attachment_sequence
204:         """
205:         attachments = db_manager.execute_query_dict(attachments_query, (tip,))
206:         
207:         # Get errors
208:         errors_query = """
209:             SELECT 
210:                 error_type,
211:                 error_message,
212:                 created_at
213:             FROM processing_errors
214:             WHERE tip = %s
215:             ORDER BY created_at DESC
216:             LIMIT 10
217:         """
218:         errors = db_manager.execute_query_dict(errors_query, (tip,))
219:         
220:         return render_template(
221:             'inspection_detail.html',
222:             inspection=inspection,
223:             attachments=attachments,
224:             errors=errors
225:         )
226:     except Exception as e:
227:         return f"Error: {e}", 500
228: 
229: @app.route('/hashes')
230: @auth.login_required
231: def hashes():
232:     """Hash management page"""
233:     try:
234:         stats = hash_manager.get_hash_statistics()
235:         
236:         # Get unknown hashes count by type
237:         unknown_query = """
238:             SELECT 
239:                 object_type,
240:                 COUNT(*) as count
241:             FROM unknown_hashes
242:             WHERE resolved_at IS NULL
243:             GROUP BY object_type
244:         """
245:         unknown_counts = db_manager.execute_query_dict(unknown_query)
246:         
247:         return render_template(
248:             'hashes.html',
249:             stats=stats,
250:             unknown_counts=unknown_counts
251:         )
252:     except Exception as e:
253:         return f"Error: {e}", 500
254: 
255: @app.route('/service-status')
256: @auth.login_required
257: def service_status():
258:     """Service status page"""
259:     import subprocess
260:     
261:     try:
262:         # Check service status
263:         result = subprocess.run(
264:             ['systemctl', 'is-active', 'noggin-processor'],
265:             capture_output=True,
266:             text=True
267:         )
268:         service_active = result.stdout.strip() == 'active'
269:         
270:         # Get recent logs
271:         log_result = subprocess.run(
272:             ['journalctl', '-u', 'noggin-processor', '-n', '50', '--no-pager'],
273:             capture_output=True,
274:             text=True
275:         )
276:         recent_logs = log_result.stdout
277:         
278:         return render_template(
279:             'service_status.html',
280:             service_active=service_active,
281:             recent_logs=recent_logs
282:         )
283:     except Exception as e:
284:         return f"Error: {e}", 500
285: 
286: if __name__ == '__main__':
287:     app.run(host='0.0.0.0', port=5000, debug=True)
</file>

<file path="manage_hashes.py">
  1: """
  2: Hash Management CLI Tool
  3: 
  4: Simple command-line interface for hash lookup operations.
  5: For bulk imports, use hash_lookup_sync.py instead.
  6: 
  7: Commands:
  8:     stats   - Display hash statistics
  9:     search  - Search for hash or name
 10:     lookup  - Lookup a specific hash
 11: """
 12: 
 13: from __future__ import annotations
 14: import sys
 15: import argparse
 16: import logging
 17: from pathlib import Path
 18: from typing import Optional
 19: 
 20: from tabulate import tabulate
 21: 
 22: logger: logging.Logger = logging.getLogger(__name__)
 23: 
 24: 
 25: def cmd_stats(args: argparse.Namespace, hash_manager: 'HashManager') -> int:
 26:     """Display hash statistics"""
 27:     
 28:     logger.info("Generating hash statistics")
 29:     
 30:     try:
 31:         stats = hash_manager.get_hash_statistics()
 32:         
 33:         print("\n" + "=" * 60)
 34:         print("HASH LOOKUP STATISTICS")
 35:         print("=" * 60)
 36:         
 37:         print("\nBy Lookup Type:")
 38:         print("-" * 40)
 39:         
 40:         table_data = []
 41:         for lookup_type in ['vehicle', 'trailer', 'team', 'department', 'uhf', 'unknown']:
 42:             type_stats = stats.get(lookup_type, {})
 43:             count = type_stats.get('count', 0) if isinstance(type_stats, dict) else 0
 44:             
 45:             if count > 0:
 46:                 table_data.append([lookup_type.capitalize(), count])
 47:         
 48:         if table_data:
 49:             print(tabulate(table_data, headers=['Type', 'Count'], tablefmt='simple'))
 50:         
 51:         print(f"\nTotal: {stats.get('total', 0)}")
 52:         
 53:         # Source type breakdown
 54:         by_source = stats.get('by_source_type', {})
 55:         if by_source:
 56:             print("\nBy Source Type (Noggin classification):")
 57:             print("-" * 40)
 58:             
 59:             source_data = [[source, count] for source, count in sorted(by_source.items())]
 60:             print(tabulate(source_data, headers=['Source Type', 'Count'], tablefmt='simple'))
 61:         
 62:         # Cache stats
 63:         cache_stats = hash_manager.get_cache_stats()
 64:         print(f"\nCache: {'loaded' if cache_stats['cache_loaded'] else 'not loaded'}, "
 65:               f"{cache_stats['cache_size']} entries")
 66:         
 67:         print("=" * 60 + "\n")
 68:         return 0
 69:         
 70:     except Exception as e:
 71:         print(f"\nError getting statistics: {e}")
 72:         logger.error(f"Stats failed: {e}", exc_info=True)
 73:         return 1
 74: 
 75: 
 76: def cmd_search(args: argparse.Namespace, hash_manager: 'HashManager') -> int:
 77:     """Search for hash or name"""
 78:     
 79:     search_term = args.search_term
 80:     logger.info(f"Searching for: {search_term}")
 81:     
 82:     try:
 83:         results = hash_manager.search_hash(search_term)
 84:         
 85:         if not results:
 86:             print(f"\nNo results found for: {search_term}")
 87:             return 0
 88:         
 89:         print(f"\nSEARCH RESULTS ({len(results)} found):")
 90:         print("=" * 80)
 91:         
 92:         table_data = []
 93:         for r in results:
 94:             table_data.append([
 95:                 r['lookup_type'].capitalize(),
 96:                 r['source_type'] or '-',
 97:                 r['resolved_value'],
 98:                 r['tip_hash'][:20] + '...'
 99:             ])
100:         
101:         print(tabulate(
102:             table_data,
103:             headers=['Type', 'Source', 'Name', 'Hash (truncated)'],
104:             tablefmt='simple'
105:         ))
106:         
107:         if len(results) >= 50:
108:             print("\n(Results limited to 50. Refine your search for more specific results.)")
109:         
110:         return 0
111:         
112:     except Exception as e:
113:         print(f"\nSearch failed: {e}")
114:         logger.error(f"Search failed: {e}", exc_info=True)
115:         return 1
116: 
117: 
118: def cmd_lookup(args: argparse.Namespace, hash_manager: 'HashManager') -> int:
119:     """Lookup a specific hash"""
120:     
121:     tip_hash = args.hash_value
122:     logger.info(f"Looking up hash: {tip_hash}")
123:     
124:     try:
125:         metadata = hash_manager.lookup_hash_with_metadata(tip_hash)
126:         
127:         if not metadata:
128:             print(f"\nHash not found: {tip_hash}")
129:             return 1
130:         
131:         print(f"\nHASH LOOKUP RESULT:")
132:         print("=" * 60)
133:         print(f"  Hash:          {tip_hash}")
134:         print(f"  Resolved Name: {metadata['resolved_value']}")
135:         print(f"  Lookup Type:   {metadata['lookup_type']}")
136:         print(f"  Source Type:   {metadata['source_type'] or '-'}")
137:         print("=" * 60 + "\n")
138:         
139:         return 0
140:         
141:     except Exception as e:
142:         print(f"\nLookup failed: {e}")
143:         logger.error(f"Lookup failed: {e}", exc_info=True)
144:         return 1
145: 
146: 
147: def cmd_list_type(args: argparse.Namespace, hash_manager: 'HashManager') -> int:
148:     """List all entries of a specific lookup_type"""
149:     
150:     lookup_type = args.lookup_type
151:     logger.info(f"Listing all {lookup_type} entries")
152:     
153:     try:
154:         results = hash_manager.get_by_type(lookup_type)
155:         
156:         if not results:
157:             print(f"\nNo {lookup_type} entries found")
158:             return 0
159:         
160:         print(f"\n{lookup_type.upper()} ENTRIES ({len(results)} total):")
161:         print("=" * 80)
162:         
163:         # Apply limit
164:         display_results = results[:args.limit]
165:         
166:         table_data = []
167:         for r in display_results:
168:             table_data.append([
169:                 r['resolved_value'],
170:                 r['source_type'] or '-',
171:                 r['tip_hash'][:24] + '...'
172:             ])
173:         
174:         print(tabulate(
175:             table_data,
176:             headers=['Name', 'Source Type', 'Hash (truncated)'],
177:             tablefmt='simple'
178:         ))
179:         
180:         if len(results) > args.limit:
181:             print(f"\n(Showing {args.limit} of {len(results)}. Use --limit to see more.)")
182:         
183:         return 0
184:         
185:     except Exception as e:
186:         print(f"\nList failed: {e}")
187:         logger.error(f"List failed: {e}", exc_info=True)
188:         return 1
189: 
190: 
191: def main() -> int:
192:     """Main entry point"""
193:     
194:     parser = argparse.ArgumentParser(
195:         description='Hash lookup CLI tool',
196:         formatter_class=argparse.RawDescriptionHelpFormatter,
197:         epilog="""
198: Examples:
199:     # Show statistics
200:     python manage_hashes.py stats
201:     
202:     # Search for a name
203:     python manage_hashes.py search "MB26"
204:     python manage_hashes.py search "Air Liquide"
205:     
206:     # Lookup a specific hash
207:     python manage_hashes.py lookup 02409bd3dd8355a53b3cef56e0eb6440b653bfad9579e7e602528db25cfbdc34
208:     
209:     # List all vehicles
210:     python manage_hashes.py list vehicle
211:     
212:     # List all trailers (first 100)
213:     python manage_hashes.py list trailer --limit 100
214: 
215: Note: For importing/syncing hash data, use hash_lookup_sync.py instead.
216:         """
217:     )
218:     
219:     subparsers = parser.add_subparsers(dest='command', help='Command to execute')
220:     
221:     # Stats command
222:     subparsers.add_parser('stats', help='Display hash statistics')
223:     
224:     # Search command
225:     search_parser = subparsers.add_parser('search', help='Search for hash or name')
226:     search_parser.add_argument('search_term', help='Search term (name or partial hash)')
227:     
228:     # Lookup command
229:     lookup_parser = subparsers.add_parser('lookup', help='Lookup a specific hash')
230:     lookup_parser.add_argument('hash_value', help='Full hash value to lookup')
231:     
232:     # List command
233:     list_parser = subparsers.add_parser('list', help='List all entries of a type')
234:     list_parser.add_argument('lookup_type', 
235:                             choices=['vehicle', 'trailer', 'team', 'department', 'uhf'],
236:                             help='Lookup type to list')
237:     list_parser.add_argument('--limit', type=int, default=50, 
238:                             help='Maximum results to show (default: 50)')
239:     
240:     args = parser.parse_args()
241:     
242:     if not args.command:
243:         parser.print_help()
244:         return 1
245:     
246:     # Import dependencies
247:     sys.path.insert(0, str(Path(__file__).parent))
248:     from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager
249:     
250:     # Initialise components
251:     config = ConfigLoader(
252:         'config/base_config.ini',
253:         'config/load_compliance_check_driver_loader_config.ini'
254:     )
255:     
256:     logger_manager = LoggerManager(config, script_name='manage_hashes')
257:     logger_manager.configure_application_logger()
258:     
259:     db_manager = DatabaseConnectionManager(config)
260:     hash_manager = HashManager(config, db_manager)
261:     
262:     try:
263:         if args.command == 'stats':
264:             return cmd_stats(args, hash_manager)
265:         elif args.command == 'search':
266:             return cmd_search(args, hash_manager)
267:         elif args.command == 'lookup':
268:             return cmd_lookup(args, hash_manager)
269:         elif args.command == 'list':
270:             return cmd_list_type(args, hash_manager)
271:         else:
272:             print(f"Unknown command: {args.command}")
273:             return 1
274:     
275:     finally:
276:         db_manager.close_all()
277: 
278: 
279: if __name__ == "__main__":
280:     sys.exit(main())
</file>

<file path="test_csv_importer.py">
 1: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, CSVImporter
 2: from common import UNKNOWN_TEXT
 3: import logging
 4: from typing import Dict, Any
 5: import sys
 6: 
 7: config: ConfigLoader = ConfigLoader(
 8:     'config/base_config.ini',
 9:     'config/load_compliance_check_driver_loader_config.ini'
10: )
11: 
12: logger_manager: LoggerManager = LoggerManager(config, script_name='test_csv_importer')
13: logger_manager.configure_application_logger()
14: 
15: logger: logging.Logger = logging.getLogger(__name__)
16: 
17: try:
18:     logger.info("Initialising CSV importer...")
19:     db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
20: 
21:     print("\n" + "="*60)
22:     print(" DATABASE CLEANUP OPTIONS")
23:     print("="*60)
24:     print("Tables to be truncated:")
25:     print(" - noggin_schema.noggin_data")
26:     print(" - noggin_schema.attachments")
27:     print(" - noggin_schema.processing_errors")
28:     print(" - noggin_schema.session_log")
29:     print(" - noggin_schema.unknown_hashes")
30:     print("-" * 60)
31:     
32:     user_response = input(">>> Do you want to TRUNCATE these tables before importing? (y/n): ").strip().lower()
33:     
34:     if user_response == 'y':
35:         logger.warning("User requested table truncation...")
36:         try:
37:             # Construct the query with all tables
38:             tables = [
39:                 'noggin_data',
40:                 'attachments',
41:                 'processing_errors',
42:                 'session_log',
43:                 'unknown_hashes'
44:             ]
45:             # Add schema prefix
46:             fq_tables = [f"noggin_schema.{t}" for t in tables]
47:             
48:             # TRUNCATE ... CASCADE handles foreign keys; RESTART IDENTITY resets ID counters
49:             query = f"TRUNCATE TABLE {', '.join(fq_tables)} CASCADE;"
50:             
51:             db_manager.execute_update(query)
52:             logger.info("Truncation complete.")
53:             print(" SUCCESS: All specified tables have been truncated.")
54:         except Exception as e:
55:             logger.error(f"Truncation failed: {e}")
56:             print(f" FAILED: Could not truncate tables. Error: {e}")
57:             sys.exit(1)
58:     else:
59:         print("... Skipping truncation. Appending to existing data.")
60:     print("="*60 + "\n")
61:     # --- TRUNCATION PROMPT END ---
62: 
63:     csv_importer: CSVImporter = CSVImporter(config, db_manager)
64: 
65:     logger.info(f"Input folder path: {csv_importer.input_folder}")
66:     logger.info(f"Input folder exists: {csv_importer.input_folder.exists()}")
67:    
68:     csv_files = list(csv_importer.input_folder.glob('*.csv'))
69:     logger.info(f"CSV files found: {csv_files}")
70: 
71:     print("\n")
72:     
73:     logger.info("Scanning for CSV files in input folder...")
74:     summary: Dict[str, Any] = csv_importer.scan_and_import()
75:     
76:     logger.info("CSV Import Summary:")
77:     logger.info(f"  Files processed: {summary['files_processed']}")
78:     logger.info(f"  TIPs imported:   {summary['total_imported']}")
79:     logger.info(f"  Duplicates:      {summary['total_duplicates']}")
80:     logger.info(f"  Errors:          {summary['total_errors']}")
81:     
82:     print(f"\n CSV Import Summary:")
83:     print(f"  Files processed: {summary['files_processed']}")
84:     print(f"  TIPs imported:   {summary['total_imported']}")
85:     print(f"  Duplicates:      {summary['total_duplicates']}")
86:     print(f"  Errors:          {summary['total_errors']}")
87:     print()
88:     
89: except KeyboardInterrupt:
90:     print("\nOperation cancelled by user.")
91: except Exception as e:
92:     logger.error(f"Test failed: {e}", exc_info=True)
93:     print(f" Error: {e}")
94: finally:
95:     if 'db_manager' in locals():
96:         db_manager.close_all()
</file>

<file path="test_database.py">
 1: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, DatabaseConnectionError
 2: from common import UNKNOWN_TEXT
 3: import logging
 4: 
 5: config: ConfigLoader = ConfigLoader(
 6:     'config/base_config.ini',
 7:     'config/load_compliance_check_driver_loader_config.ini'
 8: )
 9: 
10: logger_manager: LoggerManager = LoggerManager(config, script_name='test_database')
11: logger_manager.configure_application_logger()
12: 
13: logger: logging.Logger = logging.getLogger(__name__)
14: 
15: try:
16:     logger.info("Initialising database connection manager...")
17:     db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
18:     
19:     logger.info("Testing basic query...")
20:     version: list = db_manager.execute_query("SELECT version()")
21:     logger.info(f"PostgreSQL version: {version[0][0]}")
22:     
23:     logger.info("Testing dictionary query...")
24:     # tables: list = db_manager.execute_query_dict("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_name")
25:     tables: list = db_manager.execute_query_dict("SELECT table_name FROM information_schema.tables WHERE table_schema = 'noggin_schema' ORDER BY table_name")
26:     logger.info(f"Found {len(tables)} tables in noggin_schema:")
27:     for row in tables:
28:         logger.info(f"  - {row['table_name']}")
29:     
30:     logger.info("Testing context manager...")
31:     with db_manager.get_cursor() as cur:
32:         cur.execute("SELECT COUNT(*) FROM noggin_data")
33:         count = cur.fetchone()[0]
34:         logger.info(f"Records in noggin_data: {count}")
35:     
36:     logger.info("Testing transaction...")
37:     test_queries = [
38:         ("INSERT INTO noggin_data (tip, object_type, processing_status) VALUES (%s, %s, %s)", 
39:          ('test_tip_stage3', 'Load Compliance Check (Driver/Loader)', 'pending')),
40:     ]
41:     success: bool = db_manager.execute_transaction(test_queries)
42:     logger.info(f"Transaction success: {success}")
43:     
44:     logger.info("Cleaning up test data...")
45:     deleted: int = db_manager.execute_update("DELETE FROM noggin_data WHERE tip = %s", ('test_tip_stage3',))
46:     logger.info(f"Deleted {deleted} test records")
47:     
48:     logger.info(" All database tests passed")
49:     
50: except DatabaseConnectionError as e:
51:     logger.error(f"Database connection error: {e}")
52: except Exception as e:
53:     logger.error(f"Unexpected error: {e}", exc_info=True)
54: finally:
55:     if 'db_manager' in locals():
56:         db_manager.close_all()
</file>

<file path="noggin_continuous_processor.py">
  1: #!/usr/bin/env python3
  2: """
  3: Noggin Continuous Processor
  4: 
  5: Runs noggin_processor.py in a continuous loop with configurable sleep intervals.
  6: Includes CSV import and hash resolution cycles.
  7: """
  8: 
  9: from __future__ import annotations
 10: import sys
 11: import time
 12: import signal
 13: import subprocess
 14: import logging
 15: from datetime import datetime
 16: from pathlib import Path
 17: from typing import Dict, Any
 18: 
 19: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, CSVImporter, HashManager
 20: from sftp_download_tips import run_sftp_download
 21: 
 22: logger: logging.Logger = logging.getLogger(__name__)
 23: 
 24: shutdown_requested: bool = False
 25: 
 26: 
 27: def signal_handler(signum: int, frame: Any) -> None:
 28:     """Handle shutdown signals gracefully"""
 29:     global shutdown_requested
 30:     logger.info(f"Received signal {signum}. Initiating graceful shutdown...")
 31:     shutdown_requested = True
 32: 
 33: 
 34: def run_single_processing_cycle(config: ConfigLoader, db_manager: DatabaseConnectionManager) -> Dict[str, int]:
 35:     """
 36:     Execute one processing cycle by running noggin_processor.py
 37:     
 38:     Args:
 39:         config: ConfigLoader instance
 40:         db_manager: DatabaseConnectionManager instance
 41:         
 42:     Returns:
 43:         Dictionary with cycle status and duration
 44:     """
 45:     cycle_start = datetime.now()
 46:     
 47:     logger.info("=" * 80)
 48:     logger.info(f"Starting processing cycle at {cycle_start.strftime('%Y-%m-%d %H:%M:%S')}")
 49:     logger.info("=" * 80)
 50:     
 51:     try:
 52:         script_dir = Path(__file__).parent
 53:         processor_script = script_dir / 'noggin_processor.py'
 54:         
 55:         if not processor_script.exists():
 56:             logger.error(f"Processor script not found: {processor_script}")
 57:             return {'status': 'error', 'duration_seconds': 0}
 58:         
 59:         result = subprocess.run(
 60:             [sys.executable, str(processor_script)],
 61:             cwd=str(script_dir),
 62:             capture_output=True,
 63:             text=True,
 64:             timeout=3600
 65:         )
 66:         
 67:         cycle_end = datetime.now()
 68:         duration = (cycle_end - cycle_start).total_seconds()
 69:         
 70:         if result.returncode == 0:
 71:             logger.info(f"Processing cycle completed successfully in {duration:.1f} seconds")
 72:             return {'status': 'success', 'duration_seconds': duration}
 73:         else:
 74:             logger.error(f"Processing cycle failed with return code {result.returncode}")
 75:             if result.stderr:
 76:                 logger.error(f"Error output: {result.stderr[:500]}")
 77:             return {'status': 'failed', 'duration_seconds': duration}
 78:             
 79:     except subprocess.TimeoutExpired:
 80:         logger.error("Processing cycle timed out after 3600 seconds")
 81:         return {'status': 'timeout', 'duration_seconds': 3600}
 82:     except Exception as e:
 83:         logger.error(f"Processing cycle error: {e}", exc_info=True)
 84:         return {'status': 'error', 'duration_seconds': 0}
 85: 
 86: 
 87: def run_csv_import_cycle(config: ConfigLoader, db_manager: DatabaseConnectionManager) -> Dict[str, int]:
 88:     """
 89:     Execute CSV import cycle
 90:     
 91:     Args:
 92:         config: ConfigLoader instance
 93:         db_manager: DatabaseConnectionManager instance
 94:         
 95:     Returns:
 96:         Dictionary with import statistics
 97:     """
 98:     logger.info("Starting CSV import cycle...")
 99:     
100:     try:
101:         csv_importer = CSVImporter(config, db_manager)
102:         result = csv_importer.scan_and_import_csv_files()
103:         
104:         logger.info(f"CSV import cycle complete: {result['total_imported']} TIPs imported")
105:         return result
106:         
107:     except Exception as e:
108:         logger.error(f"CSV import cycle failed: {e}", exc_info=True)
109:         return {
110:             'files_processed': 0,
111:             'total_imported': 0,
112:             'total_duplicates': 0,
113:             'total_errors': 1
114:         }
115: 
116: 
117: def run_hash_resolution_cycle(config: ConfigLoader, db_manager: DatabaseConnectionManager) -> int:
118:     """
119:     Run automatic hash resolution cycle
120:     
121:     Resolves any unknown hashes that now have entries in hash_lookup table.
122:     
123:     Args:
124:         config: ConfigLoader instance
125:         db_manager: DatabaseConnectionManager instance
126:         
127:     Returns:
128:         Number of hashes resolved
129:     """
130:     logger.info("Starting hash resolution cycle...")
131:     
132:     try:
133:         hash_manager = HashManager(config, db_manager)
134:         resolved = hash_manager.auto_resolve_unknown_hashes()
135:         
136:         if resolved > 0:
137:             logger.info(f"Hash resolution cycle complete: {resolved} hashes resolved")
138:         else:
139:             logger.debug("Hash resolution cycle complete: no hashes needed resolution")
140:         
141:         return resolved
142:         
143:     except Exception as e:
144:         logger.error(f"Hash resolution cycle failed: {e}", exc_info=True)
145:         return 0
146: 
147: 
148: def get_processing_statistics(db_manager: DatabaseConnectionManager) -> Dict[str, int]:
149:     """
150:     Get current processing statistics from database
151:     
152:     Args:
153:         db_manager: DatabaseConnectionManager instance
154:         
155:     Returns:
156:         Dictionary with counts by processing_status
157:     """
158:     try:
159:         stats_query = """
160:             SELECT 
161:                 processing_status,
162:                 COUNT(*) as count
163:             FROM noggin_data
164:             GROUP BY processing_status
165:         """
166:         results = db_manager.execute_query_dict(stats_query)
167:         
168:         stats = {row['processing_status']: row['count'] for row in results}
169:         return stats
170:         
171:     except Exception as e:
172:         logger.error(f"Failed to get processing statistics: {e}")
173:         return {}
174: 
175: def run_sftp_download_cycle(config, db_manager) -> dict:
176:     """
177:     Execute SFTP download cycle
178:     
179:     Args:
180:         config: ConfigLoader instance
181:         db_manager: DatabaseConnectionManager instance
182:         
183:     Returns:
184:         Dictionary with download statistics
185:     """
186:     logger.info("Starting SFTP download cycle...")
187:     
188:     try:
189:         result = run_sftp_download(
190:             sftp_config_path='config/sftp_config.ini',
191:             base_config=config,
192:             db_manager=db_manager
193:         )
194:         
195:         if result['status'] == 'success':
196:             logger.info(
197:                 f"SFTP download cycle complete: "
198:                 f"{result['total_inserted']} TIPs inserted, "
199:                 f"{result['total_duplicates']} duplicates skipped"
200:             )
201:         elif result['status'] == 'no_files':
202:             logger.info("SFTP download cycle complete: no new files on server")
203:         else:
204:             logger.warning(f"SFTP download cycle completed with status: {result['status']}")
205:         
206:         return result
207:         
208:     except Exception as e:
209:         logger.error(f"SFTP download cycle failed: {e}", exc_info=True)
210:         return {'status': 'error', 'total_inserted': 0, 'total_duplicates': 0, 'total_errors': 1}
211: 
212: def main() -> int:
213:     """Main entry point for continuous processor"""
214: 
215:     global shutdown_requested
216:     shutdown_requested = False
217: 
218:     # def signal_handler(signum: int, frame: Any) -> None:
219:     #     """Handle shutdown signals gracefully"""
220:     #     # nonlocal shutdown_requested
221:     #     logger.info(f"Received signal {signum}. Initiating graceful shutdown...")
222:     #     shutdown_requested = True
223:     
224:     signal.signal(signal.SIGINT, signal_handler)
225:     signal.signal(signal.SIGTERM, signal_handler)
226:     
227:     try:
228:         config = ConfigLoader(
229:             'config/base_config.ini',
230:             'config/load_compliance_check_driver_loader_config.ini'
231:         )
232:         
233:         logger_manager = LoggerManager(config, script_name='noggin_continuous_processor')
234:         logger_manager.configure_application_logger()
235:         
236:         cycle_sleep = config.getint('continuous', 'cycle_sleep_seconds')
237:         csv_import_frequency = config.getint('continuous', 'import_csv_every_n_cycles')
238:         sftp_download_frequency = config.getint('continuous', 'sftp_download_every_n_cycles', fallback=6)
239:         
240:         # New configuration for hash resolution
241:         hash_resolution_frequency = config.getint('continuous', 'resolve_hashes_every_n_cycles', fallback=10)
242:         
243:         logger.info("Noggin Continuous Processor started")
244:         logger.info(f"Configuration:")
245:         logger.info(f"  - Cycle sleep: {cycle_sleep} seconds")
246:         logger.info(f"  - CSV import frequency: every {csv_import_frequency} cycles")
247:         logger.info(f"  - Hash resolution frequency: every {hash_resolution_frequency} cycles")
248:         logger.info(f"  - SFTP download frequency: every {sftp_download_frequency} cycles")
249:         
250:         db_manager = DatabaseConnectionManager(config)
251:         
252:         cycle_count = 0
253:         total_processed = 0
254:         
255:         while not shutdown_requested:
256:             cycle_count += 1
257:             
258:             logger.info(f"\n{'='*80}")
259:             logger.info(f"CYCLE {cycle_count}")
260:             logger.info(f"{'='*80}")
261: 
262:             # Run SFTP download cycle (every N cycles)
263:             if cycle_count % sftp_download_frequency == 0:
264:                 sftp_result = run_sftp_download_cycle(config, db_manager)
265:                 # Optionally track statistics
266:                 if sftp_result.get('total_inserted', 0) > 0:
267:                     logger.info(f"SFTP: Added {sftp_result['total_inserted']} new TIPs to queue")
268:             
269:             # Run CSV import cycle (every N cycles)
270:             if cycle_count % csv_import_frequency == 0:
271:                 import_result = run_csv_import_cycle(config, db_manager)
272:                 total_processed += import_result['total_imported']
273:             
274:             # Run hash resolution cycle (every N cycles)
275:             if cycle_count % hash_resolution_frequency == 0:
276:                 resolved = run_hash_resolution_cycle(config, db_manager)
277:                 if resolved > 0:
278:                     logger.info(f"Resolved {resolved} previously unknown hashes")
279: 
280:             
281:             # Run main processing cycle
282:             cycle_result = run_single_processing_cycle(config, db_manager)
283:             
284:             # Get current statistics
285:             stats = get_processing_statistics(db_manager)
286:             logger.info(f"\nCurrent Statistics:")
287:             for status, count in sorted(stats.items()):
288:                 logger.info(f"  {status}: {count}")
289:             
290:             # Check for shutdown before sleeping
291:             if shutdown_requested:
292:                 logger.info("Shutdown requested, exiting...")
293:                 break
294:             
295:             logger.info(f"\nSleeping for {cycle_sleep} seconds...")
296:             
297:             # Sleep in 1-second intervals to allow responsive shutdown
298:             for _ in range(cycle_sleep):
299:                 if shutdown_requested:
300:                     break
301:                 time.sleep(1)
302:         
303:         logger.info("="*80)
304:         logger.info("Continuous processor shutdown complete")
305:         logger.info(f"Total cycles executed: {cycle_count}")
306:         logger.info(f"Total records processed: {total_processed}")
307:         logger.info("="*80)
308: 
309:         # Sleep before next cycle
310:         if not shutdown_requested:
311:             logger.info(f"Sleeping {cycle_sleep}s before next cycle...")
312:             time.sleep(cycle_sleep)        
313:             
314:         return 0
315:         
316:     except KeyboardInterrupt:
317:         logger.info("Keyboard interrupt received, shutting down...")
318:         return 0
319:     except Exception as e:
320:         logger.error(f"Fatal error in continuous processor: {e}", exc_info=True)
321:         return 1
322:     finally:
323:         if 'db_manager' in locals():
324:             db_manager.close_all()
325: 
326: 
327: if __name__ == "__main__":
328:     sys.exit(main())
</file>

<file path="common/csv_importer.py">
  1: """
  2: CSV Importer Module
  3: 
  4: Imports TIP records from Noggin CSV exports into PostgreSQL database.
  5: Supports all object types with configurable preview field extraction.
  6: 
  7: Key features:
  8: - Auto-detects object type from CSV headers
  9: - Resolves hash values using hash_lookup table
 10: - Extracts preview fields for web search interface
 11: - Batch inserts for efficiency
 12: - Sets status to 'csv_imported' to distinguish from API failures
 13: - Detects if value is hash (64-char hex) or resolved text
 14: - Looks up hashes in hash_lookup table
 15: - Stores both hash and resolved value in appropriate columns
 16: - Falls back to default preview fields if INI section missing
 17: 
 18: Usage:
 19:     from common.csv_importer import CSVImporter
 20:     
 21:     importer = CSVImporter(config, db_manager)
 22:     result = importer.scan_and_import()
 23: """
 24: 
 25: from __future__ import annotations
 26: import csv
 27: import re
 28: import logging
 29: from pathlib import Path
 30: from datetime import datetime
 31: from configparser import ConfigParser
 32: from dataclasses import dataclass, field
 33: from typing import Optional, Dict, Any, List, Tuple
 34: 
 35: from .object_types import (
 36:     ObjectTypeConfig,
 37:     detect_object_type_from_headers,
 38:     find_column_index,
 39:     OBJECT_TYPES
 40: )
 41: 
 42: logger: logging.Logger = logging.getLogger(__name__)
 43: 
 44: 
 45: class CSVImportError(Exception):
 46:     """Raised when CSV import operations fail"""
 47:     pass
 48: 
 49: 
 50: @dataclass
 51: class ImportResult:
 52:     """Result of importing a single CSV file"""
 53:     filename: str
 54:     object_type: str
 55:     total_rows: int = 0
 56:     imported_count: int = 0
 57:     duplicate_count: int = 0
 58:     error_count: int = 0
 59:     success: bool = False
 60:     error_message: Optional[str] = None
 61: 
 62: 
 63: @dataclass
 64: class PreviewFieldMapping:
 65:     """
 66:     Mapping for a preview field to extract from CSV
 67:     
 68:     Attributes:
 69:         csv_column: Column name in the CSV file
 70:         db_column: Column name in the database
 71:         is_hash_field: Whether this field may contain hash values
 72:         hash_db_column: If is_hash_field, the column to store the hash
 73:     """
 74:     csv_column: str
 75:     db_column: str
 76:     is_hash_field: bool = False
 77:     hash_db_column: Optional[str] = None
 78: 
 79: 
 80: @dataclass
 81: class ObjectTypePreviewConfig:
 82:     """Preview field configuration for an object type"""
 83:     abbreviation: str
 84:     id_column: str
 85:     date_column: str
 86:     preview_fields: List[PreviewFieldMapping] = field(default_factory=list)
 87: 
 88: 
 89: class HashResolver:
 90:     """
 91:     Resolves hash values to human-readable text using the hash_lookup table.
 92:     
 93:     Caches lookups to minimise database queries during batch imports.
 94:     """
 95:     
 96:     # 64-character hexadecimal pattern for detecting hash values
 97:     HASH_PATTERN = re.compile(r'^[a-fA-F0-9]{64}$')
 98:     
 99:     def __init__(self, db_manager: 'DatabaseConnectionManager') -> None:
100:         self.db_manager = db_manager
101:         self._cache: Dict[str, Optional[str]] = {}
102:         self._cache_hits: int = 0
103:         self._cache_misses: int = 0
104:     
105:     def is_hash(self, value: str) -> bool:
106:         """Check if a value appears to be a 64-character hash"""
107:         if not value or not isinstance(value, str):
108:             return False
109:         return bool(self.HASH_PATTERN.match(value.strip()))
110:     
111:     def resolve(self, hash_value: str) -> Optional[str]:
112:         """
113:         Resolve a hash to its human-readable value.
114:         
115:         Args:
116:             hash_value: The 64-character hash to resolve
117:             
118:         Returns:
119:             Resolved value if found, None otherwise
120:         """
121:         if not hash_value:
122:             return None
123:         
124:         hash_value = hash_value.strip()
125:         
126:         if hash_value in self._cache:
127:             self._cache_hits += 1
128:             return self._cache[hash_value]
129:         
130:         self._cache_misses += 1
131:         
132:         try:
133:             rows = self.db_manager.execute_query_dict(
134:                 "SELECT resolved_value FROM hash_lookup WHERE tip_hash = %s",
135:                 (hash_value,)
136:             )
137:             
138:             if rows:
139:                 resolved = rows[0]['resolved_value']
140:                 self._cache[hash_value] = resolved
141:                 return resolved
142:             else:
143:                 self._cache[hash_value] = None
144:                 return None
145:                 
146:         except Exception as e:
147:             logger.warning(f"Hash lookup failed for {hash_value[:16]}...: {e}")
148:             return None
149:     
150:     def resolve_or_passthrough(self, value: str) -> Tuple[Optional[str], Optional[str]]:
151:         """
152:         Resolve a value if it's a hash, otherwise pass through.
153:         
154:         Args:
155:             value: Value that may be a hash or resolved text
156:             
157:         Returns:
158:             Tuple of (resolved_value, hash_value)
159:             - If input is a hash: (resolved_text or None, original_hash)
160:             - If input is text: (original_text, None)
161:         """
162:         if not value:
163:             return (None, None)
164:         
165:         value = str(value).strip()
166:         
167:         if self.is_hash(value):
168:             resolved = self.resolve(value)
169:             return (resolved, value)
170:         else:
171:             return (value, None)
172:     
173:     def get_cache_stats(self) -> Dict[str, int]:
174:         """Return cache statistics"""
175:         return {
176:             'cache_size': len(self._cache),
177:             'cache_hits': self._cache_hits,
178:             'cache_misses': self._cache_misses
179:         }
180:     
181:     def clear_cache(self) -> None:
182:         """Clear the resolution cache"""
183:         self._cache.clear()
184:         self._cache_hits = 0
185:         self._cache_misses = 0
186: 
187: 
188: class PreviewFieldConfigLoader:
189:     """
190:     Loads preview field configurations from INI files.
191:     
192:     Each object type's INI file should have a [csv_import] section defining
193:     which fields to extract for web search preview.
194:     """
195:     
196:     # Default preview fields if [csv_import] section is missing
197:     DEFAULT_PREVIEW_FIELDS: Dict[str, List[Tuple[str, str, bool]]] = {
198:         # (csv_column, db_column, is_hash_field)
199:         'LCD': [
200:             ('lcdInspectionId', 'lcd_inspection_id', False),
201:             ('date', 'inspection_date', False),
202:             ('inspectedBy', 'inspected_by', False),
203:             ('driverLoaderName', 'driver_loader_name', False),
204:             ('team', 'team', True),
205:             ('vehicle', 'vehicle', True),
206:             ('vehicleId', 'vehicle_id', False),
207:             ('trailer', 'trailer', True),
208:             ('trailerId', 'trailer_id', False),
209:         ],
210:         'LCS': [
211:             ('lcsInspectionId', 'lcs_inspection_id', False),
212:             ('date', 'inspection_date', False),
213:             ('inspectedBy', 'inspected_by', False),
214:             ('driverLoaderName', 'driver_loader_name', False),
215:             ('team', 'team', True),
216:             ('vehicle', 'vehicle', True),
217:             ('vehicleId', 'vehicle_id', False),
218:             ('trailer', 'trailer', True),
219:             ('trailerId', 'trailer_id', False),
220:         ],
221:         'CCC': [
222:             ('couplingId', 'coupling_id', False),
223:             ('date', 'inspection_date', False),
224:             ('personCompleting', 'person_completing', False),
225:             ('team', 'team', True),
226:             ('vehicleId', 'vehicle_id', False),
227:             ('trailer', 'trailer', True),
228:             ('trailerId', 'trailer_id', False),
229:         ],
230:         'FPI': [
231:             ('forkliftPrestartInspectionId', 'forklift_inspection_id', False),
232:             ('date', 'inspection_date', False),
233:             ('personsCompleting', 'persons_completing', False),
234:             ('team', 'team', True),
235:             ('goldstarAsset', 'goldstar_asset', False),
236:             ('preStartStatus', 'prestart_status', False),
237:         ],
238:         'SO': [
239:             ('siteObservationId', 'site_observation_id', False),
240:             ('date', 'inspection_date', False),
241:             ('siteManager', 'site_manager', False),
242:             ('department', 'department', True),
243:             ('inspectedBy', 'inspected_by', False),
244:         ],
245:         'TA': [
246:             ('trailerAuditId', 'trailer_audit_id', False),
247:             ('date', 'inspection_date', False),
248:             ('inspectedBy', 'inspected_by', False),
249:             ('team', 'team', True),
250:             ('vehicle', 'vehicle', True),
251:             ('rego', 'rego', False),
252:         ],
253:     }
254:     
255:     # Hash field to hash column mapping
256:     HASH_COLUMN_MAP: Dict[str, str] = {
257:         'team': 'team_hash',
258:         'vehicle': 'vehicle_hash',
259:         'trailer': 'trailer_hash',
260:         'trailer2': 'trailer2_hash',
261:         'trailer3': 'trailer3_hash',
262:         'department': 'department_hash',
263:     }
264:     
265:     def __init__(self, config_dir: Path) -> None:
266:         self.config_dir = config_dir
267:         self._configs: Dict[str, ObjectTypePreviewConfig] = {}
268:     
269:     def load_config(self, abbreviation: str) -> ObjectTypePreviewConfig:
270:         """
271:         Load preview field config for an object type.
272:         
273:         Args:
274:             abbreviation: Object type abbreviation (LCD, CCC, etc.)
275:             
276:         Returns:
277:             ObjectTypePreviewConfig with preview field mappings
278:         """
279:         if abbreviation in self._configs:
280:             return self._configs[abbreviation]
281:         
282:         object_config = OBJECT_TYPES.get(abbreviation)
283:         if not object_config:
284:             raise CSVImportError(f"Unknown object type: {abbreviation}")
285:         
286:         config_file = self.config_dir / object_config.config_file
287:         
288:         preview_fields: List[PreviewFieldMapping] = []
289:         
290:         if config_file.exists():
291:             parser = ConfigParser()
292:             parser.read(config_file)
293:             
294:             if parser.has_section('csv_import'):
295:                 for csv_col, db_col in parser.items('csv_import'):
296:                     is_hash = db_col in self.HASH_COLUMN_MAP
297:                     hash_col = self.HASH_COLUMN_MAP.get(db_col) if is_hash else None
298:                     
299:                     preview_fields.append(PreviewFieldMapping(
300:                         csv_column=csv_col,
301:                         db_column=db_col,
302:                         is_hash_field=is_hash,
303:                         hash_db_column=hash_col
304:                     ))
305:                     
306:                 logger.debug(f"Loaded {len(preview_fields)} preview fields from {config_file.name}")
307:         
308:         if not preview_fields:
309:             defaults = self.DEFAULT_PREVIEW_FIELDS.get(abbreviation, [])
310:             for csv_col, db_col, is_hash in defaults:
311:                 hash_col = self.HASH_COLUMN_MAP.get(db_col) if is_hash else None
312:                 preview_fields.append(PreviewFieldMapping(
313:                     csv_column=csv_col,
314:                     db_column=db_col,
315:                     is_hash_field=is_hash,
316:                     hash_db_column=hash_col
317:                 ))
318:             
319:             logger.debug(f"Using {len(preview_fields)} default preview fields for {abbreviation}")
320:         
321:         result = ObjectTypePreviewConfig(
322:             abbreviation=abbreviation,
323:             id_column=object_config.id_column,
324:             date_column=object_config.date_column,
325:             preview_fields=preview_fields
326:         )
327:         
328:         self._configs[abbreviation] = result
329:         return result
330: 
331: 
332: class CSVRowParser:
333:     """
334:     Parses CSV rows and extracts preview fields with hash resolution.
335:     """
336:     
337:     def __init__(self, headers: List[str], preview_config: ObjectTypePreviewConfig,
338:                  hash_resolver: HashResolver) -> None:
339:         self.headers = headers
340:         self.preview_config = preview_config
341:         self.hash_resolver = hash_resolver
342:         
343:         self._column_indices: Dict[str, int] = {}
344:         self._build_column_index_map()
345:     
346:     def _build_column_index_map(self) -> None:
347:         """Build mapping of field names to column indices"""
348:         for mapping in self.preview_config.preview_fields:
349:             idx = find_column_index(self.headers, mapping.csv_column)
350:             if idx >= 0:
351:                 self._column_indices[mapping.csv_column] = idx
352:             else:
353:                 logger.debug(f"Column '{mapping.csv_column}' not found in CSV headers")
354:         
355:         idx = find_column_index(self.headers, self.preview_config.id_column)
356:         if idx >= 0:
357:             self._column_indices[self.preview_config.id_column] = idx
358:         
359:         idx = find_column_index(self.headers, self.preview_config.date_column)
360:         if idx >= 0:
361:             self._column_indices[self.preview_config.date_column] = idx
362:     
363:     def parse_row(self, row: List[str]) -> Dict[str, Any]:
364:         """
365:         Parse a CSV row and extract all preview fields.
366:         
367:         Args:
368:             row: CSV row data (list of strings)
369:             
370:         Returns:
371:             Dictionary with database column names as keys
372:         """
373: 
374:         # clean all incoming fields.
375:         row = [val.strip() for val in row]
376: 
377:         
378:         result: Dict[str, Any] = {}
379:         
380:         # TIP is always first column
381:         tip = row[0].strip() if row else ''
382:         if not tip:
383:             return {}
384:         
385:         result['tip'] = tip
386:         result['object_type'] = self.preview_config.abbreviation
387:         
388:         # Extract and convert date
389:         date_col = self.preview_config.date_column
390:         if date_col in self._column_indices:
391:             idx = self._column_indices[date_col]
392:             if idx < len(row):
393:                 raw_date = row[idx].strip()
394:                 result['inspection_date'] = self._parse_date(raw_date)
395:         
396:         # Extract preview fields
397:         for mapping in self.preview_config.preview_fields:
398:             if mapping.csv_column not in self._column_indices:
399:                 continue
400:             
401:             idx = self._column_indices[mapping.csv_column]
402:             if idx >= len(row):
403:                 continue
404:             
405:             raw_value = row[idx].strip()
406:             if not raw_value:
407:                 continue
408:             
409:             if mapping.is_hash_field:
410:                 resolved, hash_val = self.hash_resolver.resolve_or_passthrough(raw_value)
411:                 
412:                 if resolved:
413:                     result[mapping.db_column] = resolved
414:                 if hash_val and mapping.hash_db_column:
415:                     result[mapping.hash_db_column] = hash_val
416:             else:
417:                 result[mapping.db_column] = raw_value
418:         
419:         return result
420:     
421:     def _parse_date(self, date_str: str) -> Optional[datetime]:
422:         """Parse date string into datetime object"""
423:         if not date_str:
424:             return None
425:         
426:         # Common date formats from Noggin exports
427:         formats = [
428:             '%Y-%m-%dT%H:%M:%S.%fZ',
429:             '%Y-%m-%dT%H:%M:%SZ',
430:             '%Y-%m-%dT%H:%M:%S',
431:             '%Y-%m-%d %H:%M:%S',
432:             '%Y-%m-%d',
433:             '%d-%b-%y',      # 4-Jun-24
434:             '%d-%B-%Y',      # 4-June-2024
435:             '%d/%m/%Y',
436:             '%m/%d/%Y',
437:             '%d %b %Y',       # 4 Jun 2024
438:             '%d %b %y',       # 4 Jun 24
439:             '%d-%b-%Y',
440:             '%d-%b-%y',
441:         ]
442:         
443:         for fmt in formats:
444:             try:
445:                 return datetime.strptime(date_str, fmt)
446:             except ValueError:
447:                 continue
448:         
449:         logger.debug(f"Could not parse date: {date_str}")
450:         return None
451: 
452: 
453: class BatchInserter:
454:     """
455:     Handles batch insertion of records into the database.
456:     
457:     Uses INSERT ... ON CONFLICT DO NOTHING to efficiently skip duplicates.
458:     """
459:     
460:     def __init__(self, db_manager: 'DatabaseConnectionManager', batch_size: int = 100) -> None:
461:         self.db_manager = db_manager
462:         self.batch_size = batch_size
463:         self._pending: List[Dict[str, Any]] = []
464:         self._inserted_count: int = 0
465:         self._duplicate_count: int = 0
466:     
467:     def add(self, record: Dict[str, Any]) -> None:
468:         """Add a record to the batch"""
469:         if record and record.get('tip'):
470:             self._pending.append(record)
471:         
472:         if len(self._pending) >= self.batch_size:
473:             self.flush()
474:     
475:     def flush(self) -> Tuple[int, int]:
476:         """
477:         Flush pending records to database.
478:         
479:         Returns:
480:             Tuple of (inserted_count, duplicate_count) for this batch
481:         """
482:         if not self._pending:
483:             return (0, 0)
484:         
485:         batch_inserted = 0
486:         batch_duplicates = 0
487:         
488:         # Check for existing TIPs first (batch query)
489:         tips = [r['tip'] for r in self._pending]
490:         existing_tips = self._get_existing_tips(tips)
491:         
492:         # Filter out duplicates
493:         new_records = [r for r in self._pending if r['tip'] not in existing_tips]
494:         batch_duplicates = len(self._pending) - len(new_records)
495:         
496:         if new_records:
497:             batch_inserted = self._insert_batch(new_records)
498:         
499:         self._inserted_count += batch_inserted
500:         self._duplicate_count += batch_duplicates
501:         self._pending.clear()
502:         
503:         return (batch_inserted, batch_duplicates)
504:     
505:     def _get_existing_tips(self, tips: List[str]) -> set:
506:         """Check which TIPs already exist in database"""
507:         if not tips:
508:             return set()
509:         
510:         placeholders = ','.join(['%s'] * len(tips))
511:         query = f"SELECT tip FROM noggin_data WHERE tip IN ({placeholders})"
512:         
513:         try:
514:             rows = self.db_manager.execute_query_dict(query, tuple(tips))
515:             return {row['tip'] for row in rows}
516:         except Exception as e:
517:             logger.error(f"Error checking existing TIPs: {e}")
518:             return set()
519: 
520:     def _insert_batch(self, records: List[Dict[str, Any]]) -> int:
521:         """Insert a batch of records"""
522:         if not records:
523:             return 0
524:         
525:         # Add standard fields
526:         current_time = datetime.now()
527:         for record in records:
528:             record['processing_status'] = 'csv_imported'
529:             record['csv_imported_at'] = current_time
530: 
531:         # Collect all columns used across records
532:         all_columns = set()
533:         for record in records:
534:             all_columns.update(record.keys())
535:         
536:         # Order columns consistently
537:         priority_cols = ['tip', 'object_type', 'processing_status', 'csv_imported_at']
538:         ordered_columns = [c for c in priority_cols if c in all_columns]
539:         ordered_columns.extend(sorted(c for c in all_columns if c not in priority_cols))
540:         
541:         column_list = ', '.join(ordered_columns)
542:         placeholders = ', '.join(['%s'] * len(ordered_columns))
543:         
544:         insert_sql = f"""
545:             INSERT INTO noggin_data ({column_list})
546:             VALUES ({placeholders})
547:             ON CONFLICT (tip) DO NOTHING
548:         """
549:         
550:         inserted = 0
551:         for record in records:
552:             values = tuple(record.get(col) for col in ordered_columns)
553:             
554:             try:
555:                 result = self.db_manager.execute_update(insert_sql, values)
556:                 if result > 0:
557:                     inserted += 1
558:             except Exception as e:
559:                 logger.error(f"Insert failed for TIP {record.get('tip', 'unknown')[:16]}...: {e}")
560:         
561:         return inserted
562:     
563:     def get_stats(self) -> Dict[str, int]:
564:         """Get insertion statistics"""
565:         return {
566:             'inserted': self._inserted_count,
567:             'duplicates': self._duplicate_count,
568:             'pending': len(self._pending)
569:         }
570: 
571: 
572: class CSVFileProcessor:
573:     """
574:     Processes a single CSV file and imports its records.
575:     """
576:     
577:     def __init__(self, file_path: Path, preview_config_loader: PreviewFieldConfigLoader,
578:                  hash_resolver: HashResolver, db_manager: 'DatabaseConnectionManager',
579:                  batch_size: int = 100) -> None:
580:         self.file_path = file_path
581:         self.preview_config_loader = preview_config_loader
582:         self.hash_resolver = hash_resolver
583:         self.db_manager = db_manager
584:         self.batch_size = batch_size
585:     
586:     def process(self) -> ImportResult:
587:         """
588:         Process the CSV file and import records.
589:         """
590:         result = ImportResult(filename=self.file_path.name, object_type='Unknown')
591:         
592:         # NOTE: Pandas sanitisation is handled by CSVImporter before calling this.
593:         # We assume file_path now points to a clean, UTF-8 CSV.
594:         
595:         try:
596:             # encoding='utf-8-sig' handles any BOM leftover if pandas didn't run
597:             with open(self.file_path, 'r', encoding='utf-8-sig') as f:
598:                 reader = csv.reader(f)
599:                 headers = next(reader, None)
600:                 
601:                 if not headers:
602:                     result.error_message = "CSV file has no headers"
603:                     return result
604:                 
605:                 object_config = detect_object_type_from_headers(headers)
606:                 if not object_config:
607:                     result.error_message = f"Could not detect object type from headers: {headers[:5]}"
608:                     return result
609:                 
610:                 result.object_type = object_config.abbreviation
611:                 logger.info(f"Detected object type: {object_config.abbreviation} ({object_config.full_name})")
612:                 
613:                 preview_config = self.preview_config_loader.load_config(object_config.abbreviation)
614:                 row_parser = CSVRowParser(headers, preview_config, self.hash_resolver)
615:                 inserter = BatchInserter(self.db_manager, self.batch_size)
616:                 
617:                 for row_num, row in enumerate(reader, start=2):
618:                     if not row or not row[0].strip():
619:                         continue
620:                     
621:                     result.total_rows += 1
622:                     try:
623:                         parsed = row_parser.parse_row(row)
624:                         if parsed:
625:                             inserter.add(parsed)
626:                     except Exception as e:
627:                         logger.warning(f"Row {row_num}: Parse error - {e}")
628:                         result.error_count += 1
629:                 
630:                 inserter.flush()
631:                 stats = inserter.get_stats()
632:                 result.imported_count = stats['inserted']
633:                 result.duplicate_count = stats['duplicates']
634:                 result.success = result.error_count == 0
635:                 
636:                 logger.info(
637:                     f"Import complete for {self.file_path.name}: "
638:                     f"{result.imported_count} imported, "
639:                     f"{result.duplicate_count} duplicates, "
640:                     f"{result.error_count} errors"
641:                 )
642:                 
643:         except Exception as e:
644:             result.error_message = str(e)
645:             logger.error(f"Failed to process {self.file_path.name}: {e}")
646:         
647:         return result
648: 
649: 
650: class CSVImporter:
651:     """
652:     Main CSV importer class.
653:     
654:     Scans input folder for CSV files, auto-detects object types,
655:     and imports records with preview field extraction and hash resolution.
656:     """
657:     
658:     def __init__(self, config: 'ConfigLoader', db_manager: 'DatabaseConnectionManager') -> None:
659:         self.config = config
660:         self.db_manager = db_manager
661:         
662:         self.input_folder = Path(config.get('paths', 'input_folder_path'))
663:         self.processed_folder = Path(config.get('paths', 'processed_folder_path'))
664:         self.error_folder = Path(config.get('paths', 'error_folder_path'))
665:         
666:         self.input_folder.mkdir(parents=True, exist_ok=True)
667:         self.processed_folder.mkdir(parents=True, exist_ok=True)
668:         self.error_folder.mkdir(parents=True, exist_ok=True)
669:         
670:         config_dir = Path(config.get('paths', 'config_dir', fallback='config'))
671:         
672:         self.preview_config_loader = PreviewFieldConfigLoader(config_dir)
673:         self.hash_resolver = HashResolver(db_manager)
674:         self.batch_size = config.getint('csv_import', 'batch_size', fallback=100)
675:     
676:     def import_file(self, file_path: Path) -> ImportResult:
677:         """
678:         Import a single CSV file.
679:         
680:         Args:
681:             file_path: Path to the CSV file
682:             
683:         Returns:
684:             ImportResult with import statistics
685:         """
686:         if not file_path.exists():
687:             return ImportResult(
688:                 filename=file_path.name,
689:                 object_type='Unknown',
690:                 error_message=f"File not found: {file_path}"
691:             )
692:         
693:         # --- EXCEL WORKAROUND START ---
694:         # Detect if we have pandas to sanitize the input file
695:         try:
696:             import pandas as pd
697:             # engine='python' is more robust for auto-detecting weird separators
698:             # dtype=str prevents pandas from mangling IDs (e.g. 00123 -> 123)
699:             df = pd.read_csv(file_path, dtype=str, engine='python')
700:             
701:             # Rewrite as clean UTF-8 CSV (removes BOM, normalises delimiters)
702:             df.to_csv(file_path, index=False, encoding='utf-8')
703:             logger.info(f"Sanitised CSV with pandas (BOM removed): {file_path.name}")
704:         except ImportError:
705:             logger.warning("Pandas not installed. Skipping Excel BOM workaround.")
706:         except Exception as e:
707:             logger.warning(f"Failed to sanitise CSV with pandas: {e}")
708:         # --- EXCEL WORKAROUND END ---
709:         
710:         logger.info(f"Importing CSV file: {file_path.name}")
711:         
712:         processor = CSVFileProcessor(
713:             file_path=file_path,
714:             preview_config_loader=self.preview_config_loader,
715:             hash_resolver=self.hash_resolver,
716:             db_manager=self.db_manager,
717:             batch_size=self.batch_size
718:         )
719:         
720:         return processor.process()
721:     
722:     def scan_and_import(self) -> Dict[str, Any]:
723:         """
724:         Scan input folder and import all CSV files.
725:         """
726:         csv_files = list(self.input_folder.glob('*.csv'))
727:         
728:         if not csv_files:
729:             logger.debug(f"No CSV files found in {self.input_folder}")
730:             return {
731:                 'files_processed': 0,
732:                 'total_imported': 0,
733:                 'total_duplicates': 0,
734:                 'total_errors': 0,
735:                 'files_succeeded': 0,
736:                 'files_failed': 0,
737:             }
738:         
739:         logger.info(f"Found {len(csv_files)} CSV file(s) to process")
740:         
741:         results: List[ImportResult] = []
742:         
743:         for csv_file in csv_files:
744:             result = self.import_file(csv_file)
745:             results.append(result)
746:             
747:             if result.success:
748:                 self._move_file(csv_file, self.processed_folder)
749:             else:
750:                 self._move_file(csv_file, self.error_folder)
751:                 logger.error(f"Import failed for {csv_file.name}: {result.error_message}")
752:         
753:         cache_stats = self.hash_resolver.get_cache_stats()
754:         logger.info(
755:             f"Hash resolver stats: {cache_stats['cache_size']} cached, "
756:             f"{cache_stats['cache_hits']} hits, {cache_stats['cache_misses']} misses"
757:         )
758:         
759:         summary = {
760:             'files_processed': len(results),
761:             'total_imported': sum(r.imported_count for r in results),
762:             'total_duplicates': sum(r.duplicate_count for r in results),
763:             'total_errors': sum(r.error_count for r in results),
764:             'files_succeeded': sum(1 for r in results if r.success),
765:             'files_failed': sum(1 for r in results if not r.success),
766:             'results': results,
767:         }
768:         
769:         logger.info(
770:             f"CSV import summary: {summary['files_processed']} files, "
771:             f"{summary['total_imported']} imported, "
772:             f"{summary['total_duplicates']} duplicates, "
773:             f"{summary['total_errors']} errors"
774:         )
775:         
776:         return summary
777:     
778:     def _move_file(self, source: Path, destination_folder: Path) -> Optional[Path]:
779:         """Move file to destination folder with timestamp"""
780:         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
781:         new_filename = f"{source.stem}_{timestamp}{source.suffix}"
782:         destination = destination_folder / new_filename
783:         
784:         try:
785:             source.rename(destination)
786:             logger.info(f"Moved {source.name} to {destination_folder.name}/")
787:             return destination
788:         except Exception as e:
789:             logger.error(f"Failed to move {source.name}: {e}")
790:             return None
791: 
792: 
793: def detect_object_type(headers: List[str]) -> Optional[str]:
794:     """Legacy compatibility"""
795:     config = detect_object_type_from_headers(headers)
796:     return config.abbreviation if config else None
797: 
798: 
799: if __name__ == "__main__":
800:     import sys
801:     from pathlib import Path
802:     
803:     script_dir = Path(__file__).parent.parent
804:     sys.path.insert(0, str(script_dir))
805:     
806:     from common import ConfigLoader, DatabaseConnectionManager, LoggerManager
807:     
808:     try:
809:         config = ConfigLoader('config/base_config.ini')
810:         
811:         logger_manager = LoggerManager(config, script_name='csv_importer')
812:         logger_manager.configure_application_logger()
813:         
814:         db_manager = DatabaseConnectionManager(config)
815:         importer = CSVImporter(config, db_manager)
816:         
817:         summary = importer.scan_and_import()
818:         
819:         print(f"\nCSV Import Summary:")
820:         print(f"  Files processed: {summary['files_processed']}")
821:         print(f"  Files succeeded: {summary['files_succeeded']}")
822:         print(f"  Files failed: {summary['files_failed']}")
823:         print(f"  TIPs imported: {summary['total_imported']}")
824:         print(f"  Duplicates skipped: {summary['total_duplicates']}")
825:         print(f"  Errors: {summary['total_errors']}")
826:         
827:     except Exception as e:
828:         print(f"Error: {e}")
829:         import traceback
830:         traceback.print_exc()
831:     finally:
832:         if 'db_manager' in locals():
833:             db_manager.close_all()
</file>

<file path="common/logger.py">
  1: import logging
  2: import logging.handlers
  3: from pathlib import Path
  4: from datetime import datetime
  5: from typing import Optional, Dict, Any
  6: import sys
  7: import os
  8: 
  9: class AlignedFormatter(logging.Formatter):
 10:     """
 11:     Formatter that ensures strict column alignment by:
 12:     1. Placing variable-length 'message' last.
 13:     2. Truncating metadata fields that exceed their allocated width.
 14:     3. Padding metadata fields to fixed width.
 15:     """
 16: 
 17:     # Configuration Constants (Easy to change)
 18:     LEVEL_WIDTH = 8
 19:     PROCESS_WIDTH = 7
 20:     THREAD_WIDTH = 15
 21:     LOGGER_NAME_WIDTH = 40
 22:     MODULE_WIDTH = 20
 23:     LINENO_WIDTH = 4
 24:     FUNC_NAME_WIDTH = 30
 25: 
 26:     def __init__(self, fmt: str = None, datefmt: str = None):
 27:         super().__init__(fmt, datefmt)
 28:         # Define strict widths for columns using the constants
 29:         self.column_widths = {
 30:             'levelname': self.LEVEL_WIDTH,
 31:             'process': self.PROCESS_WIDTH,
 32:             'threadName': self.THREAD_WIDTH,
 33:             'name': self.LOGGER_NAME_WIDTH,
 34:             'module': self.MODULE_WIDTH,
 35:             'lineno': self.LINENO_WIDTH,
 36:             'funcName': self.FUNC_NAME_WIDTH
 37:         }
 38: 
 39:     def format(self, record: logging.LogRecord) -> str:
 40:         # Create a copy to avoid modifying the original record permanently
 41:         record_dict = record.__dict__.copy()
 42: 
 43:         record_dict['message'] = record.getMessage() # fully interpolated message
 44: 
 45:         for key, width in self.column_widths.items():
 46:             val = str(record_dict.get(key, ''))
 47: 
 48:             # Truncate if too long (reserving 1 char for ellipsis)
 49:             if len(val) > width:
 50:                 val = val[:width-1] + ''
 51: 
 52:             # Pad to fixed width
 53:             record_dict[key] = val.ljust(width)
 54: 
 55:         # Format date manually to ensure millisecond precision consistency
 56:         record_dict['asctime'] = self.formatTime(record, self.datefmt)
 57: 
 58:         # Ensure msecs is formatted as 3 digits
 59:         record_dict['msecs'] = f"{record.msecs:03.0f}"
 60: 
 61:         # Construct the aligned string.
 62:         # CRITICAL: Message is placed LAST to prevent shifting other columns.
 63:         fmt = (
 64:             "{asctime}.{msecs} | {levelname} | PID:{process} | THD:{threadName} | "
 65:             "Logger:{name} | Loc:{module}:{lineno} | Func:{funcName} | {message}"
 66:         )
 67: 
 68:         try:
 69:             return fmt.format(**record_dict)
 70:         except Exception as e:
 71:             # Fallback in case of formatting error
 72:             return f"LOG_FORMAT_ERROR: {e} | Original Message: {record.msg}"
 73: 
 74: 
 75: class LoggerManager:
 76:     """Manages application logging configuration with daily rotation and multiple log types"""
 77: 
 78:     def __init__(self, config: Any, script_name: Optional[str] = None) -> None:
 79:         """
 80:         Initialise logger manager
 81: 
 82:         Args:
 83:             config: ConfigLoader instance
 84:             script_name: Override script name (auto-detected if None)
 85:         """
 86:         self.config = config
 87:         self.script_name: str = script_name or self._detect_script_name()
 88: 
 89:         # Robust path handling
 90:         base_log_path = config.get('paths', 'base_log_path', fallback='/mnt/data/noggin/log')
 91:         self.log_path: Path = Path(base_log_path)
 92: 
 93:         try:
 94:             self.log_path.mkdir(parents=True, exist_ok=True)
 95:         except PermissionError:
 96:             sys.stderr.write(f"CRITICAL: Cannot create log directory at {self.log_path}\n")
 97:             # Fallback to tmp or current directory if permission fails
 98:             self.log_path = Path('./logs_fallback')
 99:             self.log_path.mkdir(parents=True, exist_ok=True)
100: 
101:         self._configured: bool = False
102: 
103:     def _detect_script_name(self) -> str:
104:         """Auto-detect script name from main module"""
105:         import __main__
106:         if hasattr(__main__, '__file__') and __main__.__file__ is not None:
107:             return Path(__main__.__file__).stem
108:         return 'unknown_script'
109: 
110:     def _build_log_filename(self, pattern: str) -> str:
111:         """
112:         Build log filename from pattern
113: 
114:         Args:
115:             pattern: Filename pattern with {script_name}, {date}, {time} placeholders
116:         """
117:         now: datetime = datetime.now()
118:         replacements: Dict[str, str] = {
119:             'script_name': self.script_name,
120:             'date': now.strftime('%Y%m%d'),
121:             'time': now.strftime('%H%M%S')
122:         }
123:         return pattern.format(**replacements)
124: 
125:     def configure_application_logger(self) -> None:
126:         """Configure root logger with aligned file output and console handlers"""
127:         if self._configured:
128:             return
129: 
130:         root_logger: logging.Logger = logging.getLogger()
131:         root_logger.handlers.clear()
132:         root_logger.setLevel(logging.DEBUG)
133: 
134:         # FILE HANDLER SETUP
135:         log_pattern: str = self.config.get('logging', 'log_filename_pattern', fallback='{script_name}_{date}.log')
136:         log_filename: str = self._build_log_filename(log_pattern)
137:         log_file: Path = self.log_path / log_filename
138:         file_level: str = self.config.get('logging', 'file_log_level', fallback='DEBUG')
139: 
140:         try:
141:             file_handler: logging.FileHandler = logging.FileHandler(log_file, encoding='utf-8')
142:             file_handler.setLevel(getattr(logging, file_level.upper()))
143: 
144:             file_formatter = AlignedFormatter(datefmt='%Y-%m-%d %H:%M:%S')
145:             file_handler.setFormatter(file_formatter)
146:             root_logger.addHandler(file_handler)
147: 
148:         except OSError as e:
149:             sys.stderr.write(f"Failed to setup file logging: {e}\n")
150: 
151:         # CONSOLE HANDLER SETUP
152:         console_level: str = self.config.get('logging', 'console_log_level', fallback='INFO')
153: 
154:         console_formatter: logging.Formatter = logging.Formatter( fmt='%(asctime)s | %(levelname)-8s | %(module)-20s | %(message)s', datefmt='%H:%M:%S' )
155: 
156:         console_handler: logging.StreamHandler = logging.StreamHandler(sys.stdout)
157:         console_handler.setLevel(getattr(logging, console_level.upper()))
158:         console_handler.setFormatter(console_formatter)
159:         root_logger.addHandler(console_handler)
160: 
161:         self._configured = True
162: 
163:         logger: logging.Logger = logging.getLogger(__name__)
164:         logger.info(f"Logger initialised. File: {log_file}")
165:         logger.debug(f"Configuration: Console={console_level}, File={file_level}\n")
166: 
167:     def create_session_logger(self, session_id: str) -> logging.Logger:
168:         """
169:         Create dedicated session logger with separate file
170:         """
171:         # dot-notation name - technically a child, but we won't propagate
172:         session_logger_name = f"session.{session_id}"
173:         session_logger: logging.Logger = logging.getLogger(session_logger_name)
174: 
175:         if session_logger.handlers:
176:             return session_logger
177: 
178:         session_logger.setLevel(logging.INFO)
179:         session_logger.propagate = False
180: 
181:         session_log_file: Path = self.log_path / f'session_{session_id}.log'
182: 
183:         try:
184:             session_formatter: logging.Formatter = logging.Formatter( '%(asctime)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
185:             session_handler: logging.FileHandler = logging.FileHandler(session_log_file, encoding='utf-8')
186:             session_handler.setLevel(logging.INFO)
187:             session_handler.setFormatter(session_formatter)
188:             session_logger.addHandler(session_handler)
189: 
190:             main_logger: logging.Logger = logging.getLogger(__name__)
191:             main_logger.info(f"Session logger created: {session_log_file}")
192: 
193:         except OSError as e:
194:             logging.error(f"Failed to create session log file: {e}")
195: 
196:         return session_logger
197: 
198:     def cleanup_old_logs(self, days_to_keep: Optional[int] = None) -> int:
199:         """Remove log files older than specified days"""
200:         if days_to_keep is None:
201:             days_to_keep = self.config.getint('logging', 'log_retention_days', fallback=30)
202: 
203:         cutoff_time: float = datetime.now().timestamp() - (days_to_keep * 86400)
204:         removed_count: int = 0
205: 
206:         logger: logging.Logger = logging.getLogger(__name__)
207: 
208:         try:
209:             for log_file in self.log_path.glob('*.log'):
210:                 if log_file.stat().st_mtime < cutoff_time:
211:                     try:
212:                         log_file.unlink()
213:                         removed_count += 1
214:                     except OSError as e:
215:                         logger.warning(f"Could not remove old log {log_file}: {e}")
216: 
217:             for gz_file in self.log_path.glob('*.gz'):
218:                 if gz_file.stat().st_mtime < cutoff_time:
219:                     try:
220:                         gz_file.unlink()
221:                         removed_count += 1
222:                     except OSError as e:
223:                         logger.warning(f"Could not remove old archive {gz_file}: {e}")
224: 
225:         except Exception as e:
226:             logger.error(f"Error during log cleanup: {e}")
227: 
228:         if removed_count > 0:
229:             logger.info(f"Cleaned up {removed_count} old log files")
230: 
231:         return removed_count
232: 
233:     def compress_old_logs(self, days_before_compress: int = 7) -> int:
234:         """Compress log files older than specified days using gzip"""
235:         import gzip
236:         import shutil
237: 
238:         cutoff_time: float = datetime.now().timestamp() - (days_before_compress * 86400)
239:         compressed_count: int = 0
240:         logger: logging.Logger = logging.getLogger(__name__)
241: 
242:         for log_file in self.log_path.glob('*.log'):
243:             try:
244:                 # Skip if active log file (rough check)
245:                 if log_file.name.startswith(f"{self.script_name}_") and \
246:                    datetime.now().strftime('%Y%m%d') in log_file.name:
247:                     continue
248: 
249:                 if log_file.stat().st_mtime < cutoff_time:
250:                     gz_file: Path = log_file.with_suffix('.log.gz')
251: 
252:                     if gz_file.exists():
253:                         continue
254: 
255:                     with open(log_file, 'rb') as f_in:
256:                         with gzip.open(gz_file, 'wb') as f_out:
257:                             shutil.copyfileobj(f_in, f_out)
258: 
259:                     log_file.unlink()
260:                     compressed_count += 1
261:             except Exception as e:
262:                 logger.warning(f"Could not compress log {log_file}: {e}")
263: 
264:         if compressed_count > 0:
265:             logger.info(f"Compressed {compressed_count} old log files")
266: 
267:         return compressed_count
268: 
269: if __name__ == "__main__":
270:     # Mock ConfigLoader for testing
271:     class MockConfig:
272:         def get(self, section, key, fallback=None):
273:             return fallback
274:         def getint(self, section, key, fallback=None):
275:             return fallback
276: 
277:     try:
278:         print("Initialising Logger Manager...")
279:         config = MockConfig()
280:         logger_manager = LoggerManager(config, script_name='test_logger')
281:         logger_manager.configure_application_logger()
282: 
283:         logger = logging.getLogger("test_module")
284: 
285:         # test length
286:         logger.info("Short message")
287:         logger.info("A much longer message that usually breaks formatting in standard log files")
288:         logger.warning("Warning with medium length")
289:         logger.error("Error occurred in the system with additional details to check alignment")
290: 
291:         # test different module/logger
292:         db_logger = logging.getLogger("common.database.connection.pool")
293:         db_logger.warning("Connection lost")
294: 
295:         print(f"\nTest completed. Check log file in {logger_manager.log_path}\n")
296: 
297:     except Exception as e:
298:         print(f"Test failed: {e}")
</file>

<file path="common/__init__.py">
 1: """
 2: Common Utilities Package
 3: 
 4: Provides shared infrastructure for the Noggin data processing pipeline:
 5: 
 6: Modules:
 7:   - config: ConfigLoader for loading and managing INI-based configuration
 8:   - logger: LoggerManager for setting up application and script logging
 9:   - database: DatabaseConnectionManager for PostgreSQL connections and queries
10:   - hash_manager: HashManager for runtime hash-to-text lookups with caching
11:   - csv_importer: CSVImporter for batch importing CSV files with preview extraction
12:   - object_types: Object type definitions and detection (LCD, CCC, FPI, etc.)
13:   - rate_limiter: CircuitBreaker for transient fault handling in API requests
14: 
15: Key Classes:
16:   - ConfigLoader: Manages multi-file INI configuration merging and type conversion
17:   - LoggerManager: Configures standardised logging with file rotation
18:   - DatabaseConnectionManager: Connection pooling and query execution
19:   - HashManager: In-memory cached hash lookup with search and statistics
20:   - CSVImporter: Batch CSV import with auto object-type detection and preview extraction
21:   - CircuitBreaker: Monitors failure rates and fails fast during outages
22: 
23: Global Constants:
24:   - UNKNOWN_TEXT: Sentinel value for unresolved hashes ('UNKNOWN')
25: 
26: Dependencies:
27:   - PostgreSQL 13+
28:   - Python 3.8+
29:   - psycopg2, python-dotenv, requests
30: 
31: Usage Example:
32:     from common import ConfigLoader, DatabaseConnectionManager, LoggerManager
33:     
34:     config = ConfigLoader('config/base_config.ini', 'config/app_config.ini')
35:     logger_mgr = LoggerManager(config, script_name='my_script')
36:     logger_mgr.configure_application_logger()
37:     
38:     db_mgr = DatabaseConnectionManager(config)
39:     results = db_mgr.execute_query('SELECT * FROM my_table LIMIT 10')
40: """
41: 
42: from .config import ConfigLoader, ConfigurationError
43: from .logger import LoggerManager
44: from .database import DatabaseConnectionManager, DatabaseConnectionError
45: from .hash_manager import HashManager, HashLookupError
46: from .csv_importer import CSVImporter, CSVImportError
47: from .rate_limiter import CircuitBreaker, CircuitBreakerError, CircuitState
48: from .object_types import ObjectTypeConfig
49: 
50: 
51: ### GLOBAL CONSTANTS
52: UNKNOWN_TEXT = "UNKNOWN"
53: 
54: __all__ = [
55:     # Config
56:     'ConfigLoader',
57:     'ConfigurationError',
58:     
59:     # Logger
60:     'LoggerManager',
61:     
62:     # Database
63:     'DatabaseConnectionManager',
64:     'DatabaseConnectionError',
65:     
66:     # Hash Manager
67:     'HashManager',
68:     'HashLookupError',
69:     
70:     # CSV Importer
71:     'CSVImporter',
72:     'CSVImportError',
73:     
74:     # Rate Limiter / Circuit Breaker
75:     'CircuitBreaker',
76:     'CircuitBreakerError',
77:     'CircuitState',
78:     
79:     # Object Types
80:     'ObjectTypeConfig',
81:     'OBJECT_TYPES',
82:     'detect_object_type_from_headers',
83:     'detect_object_type',
84:     'get_object_type_by_abbreviation',
85:     'get_object_type_by_full_name',
86:     'get_all_object_types',
87:     'get_id_column_for_type',
88:     'find_column_index',
89:     'extract_row_data',
90: ]
</file>

<file path="common/hash_manager.py">
  1: """
  2: Hash Manager - Runtime hash lookup operations
  3: 
  4: Provides in-memory cached lookups for resolving Noggin hashes to human-readable values.
  5: The hash_lookup table is populated by hash_lookup_sync.py from weekly Noggin exports.
  6: 
  7: This module is used by processors during data extraction to resolve vehicle, trailer,
  8: team, and department hashes to their display names.
  9: 
 10: Loads hash type detection patterns from config/hash_detection.ini
 11: 
 12: 
 13: Changes from original:
 14: 1. Removed unnecessary pandas dependency and file rewrite operations
 15: 2. Improved type detection with configurable patterns
 16: 3. Added batch database operations for efficiency
 17: 4. Better cache management
 18: 5. Added statistics and reporting methods
 19: 6. Fixed cache invalidation issues
 20: 7. Added get_by_type method for CLI list command
 21: """
 22: from __future__ import annotations
 23: import configparser
 24: import logging
 25: import csv
 26: import re
 27: from pathlib import Path
 28: from typing import Optional, List, Dict, Any, Tuple, Set
 29: from datetime import datetime
 30: 
 31: logger: logging.Logger = logging.getLogger(__name__)
 32: 
 33: 
 34: class HashLookupError(Exception):
 35:     """Raised when hash lookup operations fail"""
 36:     pass
 37: 
 38: 
 39: class HashTypeDetector:
 40:     """Loads and applies hash type detection rules from config"""
 41:     
 42:     _instance: Optional['HashTypeDetector'] = None
 43:     _patterns: Dict[str, Dict[str, Any]] = {}
 44:     _settings: Dict[str, int] = {}
 45:     _loaded: bool = False
 46:     
 47:     def __new__(cls):
 48:         if cls._instance is None:
 49:             cls._instance = super().__new__(cls)
 50:         return cls._instance
 51:     
 52:     def load(self, config_path: Optional[str] = None) -> None:
 53:         """Load detection patterns from config file"""
 54:         if self._loaded and config_path is None:
 55:             return
 56:         
 57:         if config_path is None:
 58:             config_path = self._find_config_file()
 59:         
 60:         if config_path is None:
 61:             logger.warning("hash_detection.ini not found, using defaults")
 62:             self._load_defaults()
 63:             return
 64:         
 65:         config = configparser.ConfigParser()
 66:         config.read(config_path)
 67:         
 68:         self._patterns.clear()
 69:         
 70:         # Load scoring settings
 71:         if config.has_section('hash_detection'):
 72:             self._settings = {
 73:                 'keyword_score': config.getint('hash_detection', 'keyword_score', fallback=10),
 74:                 'prefix_score': config.getint('hash_detection', 'prefix_score', fallback=20),
 75:                 'pattern_score': config.getint('hash_detection', 'pattern_score', fallback=15),
 76:                 'max_code_length': config.getint('hash_detection', 'max_code_length', fallback=10),
 77:             }
 78:         else:
 79:             self._settings = {
 80:                 'keyword_score': 10,
 81:                 'prefix_score': 20,
 82:                 'pattern_score': 15,
 83:                 'max_code_length': 10,
 84:             }
 85:         
 86:         # Load type patterns
 87:         for section in config.sections():
 88:             if not section.startswith('hash_type.'):
 89:                 continue
 90:             
 91:             type_name = section.replace('hash_type.', '')
 92:             
 93:             # Parse comma-separated values, filter empty strings
 94:             keywords_str = config.get(section, 'keywords', fallback='')
 95:             prefixes_str = config.get(section, 'prefixes', fallback='')
 96:             patterns_str = config.get(section, 'patterns', fallback='')
 97:             
 98:             keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
 99:             prefixes = [p.strip() for p in prefixes_str.split(',') if p.strip()]
100:             patterns = [p.strip() for p in patterns_str.split(',') if p.strip()]
101:             
102:             self._patterns[type_name] = {
103:                 'keywords': keywords,
104:                 'prefixes': prefixes,
105:                 'patterns': patterns,
106:                 'priority': config.getint(section, 'priority', fallback=99)
107:             }
108:         
109:         self._loaded = True
110:         logger.info(f"Loaded {len(self._patterns)} hash type patterns from {config_path}")
111:     
112:     def _find_config_file(self) -> Optional[str]:
113:         """Search for hash_detection.ini in common locations"""
114:         search_paths = [
115:             Path('config/hash_detection.ini'),
116:             Path('../config/hash_detection.ini'),
117:             Path(__file__).parent.parent / 'config' / 'hash_detection.ini',
118:             Path(__file__).parent / 'config' / 'hash_detection.ini',
119:             Path('/home/noggin_admin/scripts/config/hash_detection.ini'),
120:         ]
121:         
122:         for path in search_paths:
123:             if path.exists():
124:                 return str(path)
125:         
126:         return None
127:     
128:     def _load_defaults(self) -> None:
129:         """Load default patterns (fallback if no config found)"""
130:         self._settings = {
131:             'keyword_score': 10,
132:             'prefix_score': 20,
133:             'pattern_score': 15,
134:             'max_code_length': 10,
135:         }
136:         
137:         self._patterns = {
138:             'team': {
139:                 'keywords': ['team', 'workers', 'drivers', 'admin', 'yard', 'delivery', 'steel', 'bulk', 'packaged'],
140:                 'prefixes': [],
141:                 'patterns': [r'.+\s+-\s+.+'],
142:                 'priority': 1
143:             },
144:             'department': {
145:                 'keywords': ['department', 'transport', 'workshop', 'distribution'],
146:                 'prefixes': [],
147:                 'patterns': [],
148:                 'priority': 2
149:             },
150:             'trailer': {
151:                 'prefixes': ['T', 'TL', 'TLD', 'TS', 'TSD', 'TSE', 'TSP', 'TDD', 'TEX'],
152:                 'patterns': [r'^T[A-Z]*\d+$', r'^T\d+$'],
153:                 'keywords': [],
154:                 'priority': 3
155:             },
156:             'vehicle': {
157:                 'prefixes': ['MB', 'D', 'F', 'R', 'RC', 'RF', 'H', 'A', 'LV', 'SL', 'FS', 'AL', 'VCT', 'MAN', 'RE', 'RA', 'ALL'],
158:                 'patterns': [r'^[A-Z]{1,3}\d+$'],
159:                 'keywords': [],
160:                 'priority': 4
161:             }
162:         }
163:         
164:         self._loaded = True
165:         logger.debug("Loaded default hash type patterns")
166:     
167:     def detect_type(self, resolved_value: str) -> str:
168:         """
169:         Detect lookup type from resolved value
170:         
171:         Args:
172:             resolved_value: The human-readable value
173:             
174:         Returns:
175:             Detected lookup type or 'unknown'
176:         """
177:         if not self._loaded:
178:             self.load()
179:         
180:         if not resolved_value:
181:             return 'unknown'
182:         
183:         value_upper = resolved_value.upper().strip()
184:         value_lower = resolved_value.lower().strip()
185:         
186:         type_scores: Dict[str, int] = {}
187:         
188:         for type_name, config in self._patterns.items():
189:             score = 0
190:             
191:             # Check keywords
192:             for keyword in config.get('keywords', []):
193:                 if keyword in value_lower:
194:                     score += self._settings['keyword_score']
195:             
196:             # Check prefixes (for code-style values like MB26, T107)
197:             for prefix in config.get('prefixes', []):
198:                 if value_upper.startswith(prefix) and len(resolved_value) <= self._settings['max_code_length']:
199:                     remainder = value_upper[len(prefix):]
200:                     if remainder.isdigit():
201:                         score += self._settings['prefix_score']
202:             
203:             # Check regex patterns
204:             for pattern in config.get('patterns', []):
205:                 if re.match(pattern, resolved_value, re.IGNORECASE):
206:                     score += self._settings['pattern_score']
207:             
208:             if score > 0:
209:                 type_scores[type_name] = score
210:         
211:         if not type_scores:
212:             return 'unknown'
213:         
214:         return max(type_scores, key=type_scores.get)
215:     
216:     def get_all_types(self) -> List[str]:
217:         """Get list of all configured hash types"""
218:         if not self._loaded:
219:             self.load()
220:         return list(self._patterns.keys())
221:     
222:     def add_prefix(self, type_name: str, prefix: str) -> None:
223:         """Dynamically add a prefix to a type (for runtime updates)"""
224:         if type_name in self._patterns:
225:             if prefix not in self._patterns[type_name]['prefixes']:
226:                 self._patterns[type_name]['prefixes'].append(prefix)
227:                 logger.info(f"Added prefix '{prefix}' to {type_name}")
228:     
229:     def add_keyword(self, type_name: str, keyword: str) -> None:
230:         """Dynamically add a keyword to a type"""
231:         if type_name in self._patterns:
232:             if keyword not in self._patterns[type_name]['keywords']:
233:                 self._patterns[type_name]['keywords'].append(keyword)
234:                 logger.info(f"Added keyword '{keyword}' to {type_name}")
235: 
236: 
237: # Module-level detector instance
238: _detector = HashTypeDetector()
239: 
240: 
241: def load_hash_detection_config(config_path: Optional[str] = None) -> None:
242:     """Explicitly load hash detection config"""
243:     _detector.load(config_path)
244: 
245: 
246: class HashManager:
247:     """Manages hash lookups and resolution for Noggin data"""
248:     
249:     def __init__(self, config: 'ConfigLoader', db_manager: 'DatabaseConnectionManager') -> None:
250:         self.config: 'ConfigLoader' = config
251:         self.db_manager: 'DatabaseConnectionManager' = db_manager
252:         self.log_path: Path = Path(config.get('paths', 'base_log_path'))
253:         self.log_path.mkdir(parents=True, exist_ok=True)
254:         
255:         self._cache: Dict[Tuple[str, str], str] = {}
256:         self._cache_loaded: bool = False
257:         self._unknown_hashes_logged: Set[Tuple[str, str]] = set()
258:         
259:         # Load detection config if available
260:         hash_config_path = config.get('hash_detection', 'config_file', fallback=None)
261:         if hash_config_path:
262:             _detector.load(hash_config_path)
263:     
264:     def _load_cache(self) -> None:
265:         """Load hash lookups into memory cache"""
266:         if self._cache_loaded:
267:             return
268:         
269:         try:
270:             results: List[Dict[str, Any]] = self.db_manager.execute_query_dict(
271:                 "SELECT tip_hash, lookup_type, resolved_value FROM hash_lookup"
272:             )
273:             
274:             self._cache.clear()
275:             for row in results:
276:                 cache_key = (row['tip_hash'], row['lookup_type'])
277:                 self._cache[cache_key] = row['resolved_value']
278:             
279:             self._cache_loaded = True
280:             logger.info(f"Loaded {len(self._cache)} hash lookups into cache")
281:             
282:         except Exception as e:
283:             logger.error(f"Failed to load hash lookup cache: {e}")
284:             raise HashLookupError(f"Cache load failed: {e}")
285:     
286:     def invalidate_cache(self) -> None:
287:         """Force cache reload on next lookup"""
288:         self._cache_loaded = False
289:         self._cache.clear()
290:         logger.debug("Hash cache invalidated")
291:     
292:     def lookup_hash(self, lookup_type: str, tip_hash: str, tip_value: Optional[str] = None, 
293:                    inspection_id: Optional[str] = None) -> str:
294:         """
295:         Lookup hash and return resolved value
296:         
297:         Args:
298:             lookup_type: Type of lookup (vehicle, trailer, department, team)
299:             tip_hash: Hash value to resolve
300:             tip_value: TIP value for logging unknown hashes
301:             inspection_id: Inspection ID for logging
302:             
303:         Returns:
304:             Resolved value or "Unknown (hash)" if not found
305:         """
306:         if not tip_hash:
307:             return ""
308:         
309:         if not self._cache_loaded:
310:             self._load_cache()
311:         
312:         cache_key: Tuple[str, str] = (tip_hash, lookup_type)
313:         
314:         if cache_key in self._cache:
315:             return self._cache[cache_key]
316:         
317:         # Also check if hash exists under 'unknown' type
318:         unknown_key = (tip_hash, 'unknown')
319:         if unknown_key in self._cache:
320:             return self._cache[unknown_key]
321:         
322:         self._record_unknown_hash(lookup_type, tip_hash, tip_value, inspection_id)
323:         
324:         return f"Unknown ({tip_hash[:16]}...)"
325:     
326:     def _record_unknown_hash(self, lookup_type: str, tip_hash: str, 
327:                             tip_value: Optional[str], inspection_id: Optional[str]) -> None:
328:         """Record unknown hash to database and log file"""
329:         cache_key = (tip_hash, lookup_type)
330:         
331:         if cache_key in self._unknown_hashes_logged:
332:             return
333:         
334:         self._unknown_hashes_logged.add(cache_key)
335:         
336:         # Log to file
337:         unknown_log_file: Path = self.log_path / 'unknown_hashes.log'
338:         timestamp: str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
339:         log_entry: str = f"{timestamp} | {lookup_type} | {tip_hash} | {inspection_id or 'N/A'} | TIP: {tip_value or 'N/A'}\n"
340:         
341:         try:
342:             with open(unknown_log_file, 'a', encoding='utf-8') as f:
343:                 f.write(log_entry)
344:         except Exception as e:
345:             logger.warning(f"Could not write to unknown hashes log: {e}")
346:         
347:         # Insert to database
348:         try:
349:             self.db_manager.execute_update(
350:                 """
351:                 INSERT INTO unknown_hashes (tip_hash, lookup_type, first_seen_at, first_seen_tip, first_seen_inspection_id)
352:                 VALUES (%s, %s, %s, %s, %s)
353:                 ON CONFLICT (tip_hash, lookup_type) DO UPDATE SET
354:                     last_seen_at = CURRENT_TIMESTAMP,
355:                     occurrence_count = unknown_hashes.occurrence_count + 1
356:                 """,
357:                 (tip_hash, lookup_type, datetime.now(), tip_value, inspection_id)
358:             )
359:         except Exception as e:
360:             logger.debug(f"Could not record unknown hash to database: {e}")
361:         
362:         logger.warning(f"Unknown hash: {lookup_type}={tip_hash[:16]}...")
363:     
364:     def detect_lookup_type(self, resolved_value: str) -> str:
365:         """
366:         Detect lookup type from resolved value using config patterns
367:         
368:         Args:
369:             resolved_value: The human-readable value
370:             
371:         Returns:
372:             Detected lookup type or 'unknown'
373:         """
374:         return _detector.detect_type(resolved_value)
375:     
376:     def migrate_lookup_table_from_csv(self, csv_file_path: str, 
377:                                       batch_size: int = 100) -> Tuple[int, int]:
378:         """
379:         Migrate hash lookup table from CSV to PostgreSQL using batch operations
380:         
381:         Args:
382:             csv_file_path: Path to lookup_table.csv
383:             batch_size: Number of rows per batch insert
384:             
385:         Returns:
386:             Tuple of (imported_count, skipped_count)
387:         """
388:         csv_path: Path = Path(csv_file_path)
389:         
390:         if not csv_path.exists():
391:             raise HashLookupError(f"CSV file not found: {csv_file_path}")
392:         
393:         logger.info(f"Migrating hash lookups from {csv_file_path}")
394:         
395:         imported_count: int = 0
396:         skipped_count: int = 0
397:         batch: List[Tuple[str, str, str]] = []
398:         
399:         try:
400:             with open(csv_path, 'r', encoding='utf-8') as f:
401:                 reader = csv.DictReader(f)
402:                 
403:                 if 'TIP' not in reader.fieldnames or 'VALUE' not in reader.fieldnames:
404:                     raise HashLookupError(f"CSV must contain 'TIP' and 'VALUE' columns. Found: {reader.fieldnames}")
405:                 
406:                 for row in reader:
407:                     tip_hash: str = row['TIP'].strip()
408:                     resolved_value: str = row['VALUE'].strip()
409:                     
410:                     if not tip_hash or not resolved_value:
411:                         skipped_count += 1
412:                         continue
413:                     
414:                     lookup_type: str = self.detect_lookup_type(resolved_value)
415:                     batch.append((tip_hash, lookup_type, resolved_value))
416:                     
417:                     if len(batch) >= batch_size:
418:                         imported_count += self._insert_batch(batch)
419:                         batch = []
420:                 
421:                 if batch:
422:                     imported_count += self._insert_batch(batch)
423:             
424:             self.invalidate_cache()
425:             
426:             logger.info(f"Migration complete: {imported_count} imported, {skipped_count} skipped")
427:             return imported_count, skipped_count
428:             
429:         except Exception as e:
430:             logger.error(f"Migration failed: {e}", exc_info=True)
431:             raise HashLookupError(f"Migration failed: {e}")
432:     
433:     def _insert_batch(self, batch: List[Tuple[str, str, str]]) -> int:
434:         """Insert batch of hash lookups using upsert"""
435:         if not batch:
436:             return 0
437:         
438:         try:
439:             for tip_hash, lookup_type, resolved_value in batch:
440:                 self.db_manager.execute_update(
441:                     """
442:                     INSERT INTO hash_lookup (tip_hash, lookup_type, resolved_value)
443:                     VALUES (%s, %s, %s)
444:                     ON CONFLICT (tip_hash, lookup_type) 
445:                     DO UPDATE SET resolved_value = EXCLUDED.resolved_value, updated_at = CURRENT_TIMESTAMP
446:                     """,
447:                     (tip_hash, lookup_type, resolved_value)
448:                 )
449:                 self._cache[(tip_hash, lookup_type)] = resolved_value
450:             
451:             return len(batch)
452:                 
453:         except Exception as e:
454:             logger.error(f"Batch insert failed: {e}")
455:             return 0
456:     
457:     def resolve_unknown_hash(self, tip_hash: str, lookup_type: str, resolved_value: str) -> bool:
458:         """
459:         Manually resolve an unknown hash
460:         
461:         Args:
462:             tip_hash: Hash to resolve
463:             lookup_type: Type of lookup
464:             resolved_value: Value to associate with hash
465:             
466:         Returns:
467:             True if successful
468:         """
469:         try:
470:             self.db_manager.execute_update(
471:                 """
472:                 INSERT INTO hash_lookup (tip_hash, lookup_type, resolved_value)
473:                 VALUES (%s, %s, %s)
474:                 ON CONFLICT (tip_hash, lookup_type) 
475:                 DO UPDATE SET resolved_value = EXCLUDED.resolved_value, updated_at = CURRENT_TIMESTAMP
476:                 """,
477:                 (tip_hash, lookup_type, resolved_value)
478:             )
479:             
480:             self.db_manager.execute_update(
481:                 """
482:                 UPDATE unknown_hashes
483:                 SET resolved_at = CURRENT_TIMESTAMP, resolved_value = %s
484:                 WHERE tip_hash = %s AND lookup_type = %s AND resolved_at IS NULL
485:                 """,
486:                 (resolved_value, tip_hash, lookup_type)
487:             )
488:             
489:             self._cache[(tip_hash, lookup_type)] = resolved_value
490:             
491:             logger.info(f"Resolved hash: {lookup_type}={tip_hash[:16]}... -> {resolved_value}")
492:             return True
493:             
494:         except Exception as e:
495:             logger.error(f"Failed to resolve hash: {e}")
496:             return False
497:     
498:     def get_unknown_hashes(self, lookup_type: Optional[str] = None, 
499:                           resolved: bool = False, limit: int = 100) -> List[Dict[str, Any]]:
500:         """
501:         Get list of unknown hashes
502:         
503:         Args:
504:             lookup_type: Filter by type (optional)
505:             resolved: If True, get resolved. If False, get unresolved.
506:             limit: Maximum number to return
507:             
508:         Returns:
509:             List of dictionaries with unknown hash information
510:         """
511:         conditions = []
512:         params = []
513:         
514:         if resolved:
515:             conditions.append("resolved_at IS NOT NULL")
516:         else:
517:             conditions.append("resolved_at IS NULL")
518:         
519:         if lookup_type:
520:             conditions.append("lookup_type = %s")
521:             params.append(lookup_type)
522:         
523:         where_clause = " AND ".join(conditions)
524:         params.append(limit)
525:         
526:         query = f"""
527:             SELECT tip_hash, lookup_type, first_seen_at, last_seen_at, 
528:                    occurrence_count, first_seen_inspection_id, resolved_at, resolved_value
529:             FROM unknown_hashes
530:             WHERE {where_clause}
531:             ORDER BY occurrence_count DESC, last_seen_at DESC
532:             LIMIT %s
533:         """
534:         
535:         return self.db_manager.execute_query_dict(query, tuple(params))
536:     
537:     def auto_resolve_unknown_hashes(self) -> int:
538:         """
539:         Automatically resolve unknown hashes that now exist in hash_lookup
540:         
541:         Returns:
542:             Number of hashes resolved
543:         """
544:         logger.info("Starting automatic hash resolution")
545:         
546:         query = """
547:             UPDATE unknown_hashes uh
548:             SET resolved_at = CURRENT_TIMESTAMP,
549:                 resolved_value = hl.resolved_value
550:             FROM hash_lookup hl
551:             WHERE uh.tip_hash = hl.tip_hash
552:               AND uh.lookup_type = hl.lookup_type
553:               AND uh.resolved_at IS NULL
554:         """
555:         
556:         count = self.db_manager.execute_update(query)
557:         
558:         if count > 0:
559:             logger.info(f"Auto-resolved {count} unknown hashes")
560:             self.invalidate_cache()
561:         else:
562:             logger.debug("No unknown hashes could be auto-resolved")
563:         
564:         return count
565:     
566:     def update_lookup_type_if_unknown(self, tip_hash: str, context_key: str) -> None:
567:         """
568:         Update lookup_type from 'unknown' to actual type based on context
569:         
570:         Args:
571:             tip_hash: Hash value
572:             context_key: Key from API response (team, vehicle, whichDepartmentDoesTheLoadBelongTo, trailer)
573:         """
574:         if not tip_hash:
575:             return
576:         
577:         type_mapping = {
578:             'whichDepartmentDoesTheLoadBelongTo': 'department',
579:             'vehicle': 'vehicle',
580:             'trailer': 'trailer',
581:             'trailer2': 'trailer',
582:             'trailer3': 'trailer',
583:             'team': 'team'
584:         }
585:         
586:         normalised_type = type_mapping.get(context_key, context_key)
587:         
588:         try:
589:             rows_updated = self.db_manager.execute_update(
590:                 """
591:                 UPDATE hash_lookup
592:                 SET lookup_type = %s, updated_at = CURRENT_TIMESTAMP
593:                 WHERE tip_hash = %s AND lookup_type = 'unknown'
594:                 """,
595:                 (normalised_type, tip_hash)
596:             )
597:             
598:             if rows_updated > 0:
599:                 if (tip_hash, 'unknown') in self._cache:
600:                     value = self._cache.pop((tip_hash, 'unknown'))
601:                     self._cache[(tip_hash, normalised_type)] = value
602:                 logger.debug(f"Updated lookup_type: {tip_hash[:16]}... from unknown to {normalised_type}")
603:                 
604:         except Exception as e:
605:             logger.debug(f"Could not update lookup_type for {tip_hash[:16]}...: {e}")
606: 
607:     def get_by_type(self, lookup_type: str) -> List[Dict[str, Any]]:
608:         """
609:         Get all hash entries for a specific lookup type.
610: 
611:         Args:
612:             lookup_type: The type of entity to list (e.g., 'vehicle', 'trailer')
613: 
614:         Returns:
615:             List of dictionaries containing hash details
616:         """
617:         query = """
618:             SELECT resolved_value, source_type, tip_hash
619:             FROM hash_lookup
620:             WHERE lookup_type = %s
621:             ORDER BY resolved_value ASC
622:         """
623:         
624:         try:
625:             return self.db_manager.execute_query_dict(query, (lookup_type,))
626:         except Exception as e:
627:             logger.error(f"Failed to get hashes by type {lookup_type}: {e}")
628:             return []
629:     
630:     def get_statistics(self) -> Dict[str, Dict[str, int]]:
631:         """Get statistics about known and unknown hashes by type"""
632:         stats = {}
633:         
634:         known_query = """
635:             SELECT lookup_type, COUNT(*) as count 
636:             FROM hash_lookup 
637:             GROUP BY lookup_type
638:         """
639:         known_results = self.db_manager.execute_query_dict(known_query)
640:         
641:         unknown_query = """
642:             SELECT lookup_type, COUNT(*) as count 
643:             FROM unknown_hashes 
644:             WHERE resolved_at IS NULL
645:             GROUP BY lookup_type
646:         """
647:         unknown_results = self.db_manager.execute_query_dict(unknown_query)
648:         
649:         all_types = set()
650:         known_by_type = {r['lookup_type']: r['count'] for r in known_results}
651:         unknown_by_type = {r['lookup_type']: r['count'] for r in unknown_results}
652:         
653:         all_types.update(known_by_type.keys())
654:         all_types.update(unknown_by_type.keys())
655:         
656:         for lookup_type in sorted(all_types):
657:             stats[lookup_type] = {
658:                 'known': known_by_type.get(lookup_type, 0),
659:                 'unknown': unknown_by_type.get(lookup_type, 0)
660:             }
661:         
662:         return stats
663:     
664:     def export_unknown_hashes(self, output_path: Path, lookup_type: Optional[str] = None) -> int:
665:         """
666:         Export unknown hashes to CSV for manual resolution
667:         
668:         Args:
669:             output_path: Path for output CSV
670:             lookup_type: Filter by type (optional)
671:             
672:         Returns:
673:             Number of hashes exported
674:         """
675:         unknown = self.get_unknown_hashes(lookup_type=lookup_type, resolved=False, limit=10000)
676:         
677:         if not unknown:
678:             logger.info(f"No unknown hashes to export")
679:             return 0
680:         
681:         with open(output_path, 'w', newline='', encoding='utf-8') as f:
682:             writer = csv.writer(f)
683:             writer.writerow(['TIP', 'VALUE', 'lookup_type', 'occurrence_count', 'first_seen_inspection_id'])
684:             
685:             for row in unknown:
686:                 writer.writerow([
687:                     row['tip_hash'],
688:                     '',
689:                     row['lookup_type'],
690:                     row['occurrence_count'],
691:                     row['first_seen_inspection_id'] or ''
692:                 ])
693:         
694:         logger.info(f"Exported {len(unknown)} unknown hashes to {output_path}")
695:         return len(unknown)
696:     
697:     def import_resolved_hashes(self, csv_path: Path) -> Tuple[int, int]:
698:         """
699:         Import resolved hashes from CSV (after manual resolution)
700:         
701:         Expected columns: TIP, VALUE, lookup_type
702:         
703:         Args:
704:             csv_path: Path to CSV with resolved values
705:             
706:         Returns:
707:             Tuple of (resolved_count, skipped_count)
708:         """
709:         if not csv_path.exists():
710:             raise HashLookupError(f"CSV file not found: {csv_path}")
711:         
712:         resolved_count = 0
713:         skipped_count = 0
714:         
715:         with open(csv_path, 'r', encoding='utf-8') as f:
716:             reader = csv.DictReader(f)
717:             
718:             for row in reader:
719:                 tip_hash = row.get('TIP', '').strip()
720:                 resolved_value = row.get('VALUE', '').strip()
721:                 lookup_type = row.get('lookup_type', 'unknown').strip()
722:                 
723:                 if not tip_hash or not resolved_value:
724:                     skipped_count += 1
725:                     continue
726:                 
727:                 if self.resolve_unknown_hash(tip_hash, lookup_type, resolved_value):
728:                     resolved_count += 1
729:                 else:
730:                     skipped_count += 1
731:         
732:         logger.info(f"Import complete: {resolved_count} resolved, {skipped_count} skipped")
733:         return resolved_count, skipped_count
734:     
735:     def import_hashes_from_csv(self, lookup_type: str, csv_path: Path, 
736:                                source: str = 'manual_import') -> Tuple[int, int, int]:
737:         """
738:         Import hashes from CSV file with specified lookup type
739:         
740:         Args:
741:             lookup_type: Type of lookup (vehicle, trailer, department, team)
742:             csv_path: Path to CSV file
743:             source: Source identifier for tracking (unused, kept for API compatibility)
744:             
745:         Returns:
746:             Tuple of (imported_count, duplicate_count, error_count)
747:         """
748:         if not csv_path.exists():
749:             raise HashLookupError(f"CSV file not found: {csv_path}")
750:         
751:         logger.info(f"Importing {lookup_type} hashes from {csv_path}")
752:         
753:         imported_count = 0
754:         duplicate_count = 0
755:         error_count = 0
756:         
757:         try:
758:             with open(csv_path, 'r', encoding='utf-8') as f:
759:                 reader = csv.DictReader(f)
760:                 
761:                 if not reader.fieldnames:
762:                     raise HashLookupError("CSV file is empty or has no headers")
763:                 
764:                 tip_col = 'TIP' if 'TIP' in reader.fieldnames else 'tip_hash'
765:                 value_col = 'VALUE' if 'VALUE' in reader.fieldnames else 'resolved_value'
766:                 
767:                 if tip_col not in reader.fieldnames or value_col not in reader.fieldnames:
768:                     raise HashLookupError(
769:                         f"CSV must contain '{tip_col}' and '{value_col}' columns. "
770:                         f"Found: {reader.fieldnames}"
771:                     )
772:                 
773:                 hashes_to_import: List[Tuple[str, str]] = []
774:                 
775:                 for row in reader:
776:                     tip_hash = row[tip_col].strip()
777:                     resolved_value = row[value_col].strip()
778:                     
779:                     if not tip_hash or not resolved_value:
780:                         error_count += 1
781:                         continue
782:                     
783:                     hashes_to_import.append((tip_hash, resolved_value))
784:                 
785:                 if not hashes_to_import:
786:                     logger.warning("No valid hashes found in CSV")
787:                     return 0, 0, error_count
788:                 
789:                 tip_hashes = [h[0] for h in hashes_to_import]
790:                 placeholders = ', '.join(['%s'] * len(tip_hashes))
791:                 
792:                 existing_results = self.db_manager.execute_query_dict(
793:                     f"""
794:                     SELECT tip_hash FROM hash_lookup 
795:                     WHERE tip_hash IN ({placeholders}) AND lookup_type = %s
796:                     """,
797:                     tuple(tip_hashes) + (lookup_type,)
798:                 )
799:                 existing_hashes = {r['tip_hash'] for r in existing_results}
800:                 
801:                 for tip_hash, resolved_value in hashes_to_import:
802:                     try:
803:                         if tip_hash in existing_hashes:
804:                             self.db_manager.execute_update(
805:                                 """
806:                                 UPDATE hash_lookup 
807:                                 SET resolved_value = %s, updated_at = CURRENT_TIMESTAMP
808:                                 WHERE tip_hash = %s AND lookup_type = %s
809:                                 """,
810:                                 (resolved_value, tip_hash, lookup_type)
811:                             )
812:                             duplicate_count += 1
813:                         else:
814:                             self.db_manager.execute_update(
815:                                 """
816:                                 INSERT INTO hash_lookup (tip_hash, lookup_type, resolved_value)
817:                                 VALUES (%s, %s, %s)
818:                                 """,
819:                                 (tip_hash, lookup_type, resolved_value)
820:                             )
821:                             imported_count += 1
822:                         
823:                         self._cache[(tip_hash, lookup_type)] = resolved_value
824:                         
825:                         self.db_manager.execute_update(
826:                             """
827:                             UPDATE unknown_hashes
828:                             SET resolved_at = CURRENT_TIMESTAMP, resolved_value = %s
829:                             WHERE tip_hash = %s AND lookup_type = %s AND resolved_at IS NULL
830:                             """,
831:                             (resolved_value, tip_hash, lookup_type)
832:                         )
833:                         
834:                     except Exception as e:
835:                         logger.warning(f"Could not import hash {tip_hash[:16]}...: {e}")
836:                         error_count += 1
837:                 
838:                 logger.info(
839:                     f"Import complete: {imported_count} imported, "
840:                     f"{duplicate_count} duplicates/updated, {error_count} errors"
841:                 )
842:                 return imported_count, duplicate_count, error_count
843:                 
844:         except HashLookupError:
845:             raise
846:         except Exception as e:
847:             logger.error(f"Import failed: {e}", exc_info=True)
848:             raise HashLookupError(f"Import failed: {e}")
849: 
850: 
851: if __name__ == "__main__":
852:     import sys
853:     
854:     # Test type detection
855:     config_path = sys.argv[1] if len(sys.argv) > 1 else None
856:     _detector.load(config_path)
857:     
858:     test_values = [
859:         ('MB26', 'vehicle'),
860:         ('D27', 'vehicle'),
861:         ('T107', 'trailer'),
862:         ('TL149', 'trailer'),
863:         ('TLD250', 'trailer'),
864:         ('Steel Delivery', 'team'),
865:         ('Air Liquide Packaged - Drivers', 'team'),
866:         ('Metro Distribution - Admin', 'team'),
867:         ('LH Yard - Team', 'team'),
868:         ('F52', 'vehicle'),
869:         ('RC91', 'vehicle'),
870:         ('TSP304', 'trailer'),
871:         ('Rentco', 'unknown'),
872:         ('CBH', 'unknown'),
873:     ]
874:     
875:     print("Hash Type Detection Test:")
876:     print("-" * 60)
877:     correct = 0
878:     total = len(test_values)
879:     
880:     for value, expected in test_values:
881:         detected = _detector.detect_type(value)
882:         status = 'OK' if detected == expected else 'WRONG'
883:         if detected == expected:
884:             correct += 1
885:         print(f"{value:35} -> {detected:12} (expected: {expected:10}) [{status}]")
886:     
887:     print()
888:     print(f"Result: {correct}/{total} correct")
</file>

<file path="noggin_processor.py">
   1: """ noggin_processor.py
   2:     Module purpose
   3:     ---------------
   4:     This module processes "Load Compliance Check" (LCD) inspection records from a CSV
   5:     of TIP identifiers, retrieves their JSON payloads from a remote API, stores
   6:     metadata in PostgreSQL, downloads and validates attachment media, and writes
   7:     human-readable inspection reports and attachment files to disk. It includes
   8:     robust retry/backoff logic, a circuit breaker to protect the API, and graceful
   9:     shutdown handling.
  10:     High-level behaviour
  11:     --------------------
  12:     - Reads TIPs from a CSV file (expected column header: "tip").
  13:     - For each TIP:
  14:         - Uses an endpoint template ($tip) to build the API URL and GETs the JSON.
  15:         - Inserts/updates a noggin_data row with parsed response fields and meta.
  16:         - Creates a dated folder structure and writes a formatted text payload file.
  17:         - Downloads listed attachments, validates file integrity, computes MD5 and
  18:         records attachment state in the attachments table.
  19:         - Tracks errors in processing_errors and updates retry/backoff state on errors.
  20:     - Honours graceful shutdown (SIGINT/SIGTERM) finishing the current TIP when
  21:     possible; a second signal forces immediate exit.
  22:     - Emits logging to both application logger and a session logger file.
  23:     Primary public functions/classes
  24:     -------------------------------
  25:     - GracefulShutdownHandler(db_conn, logger_instance)
  26:         Handles SIGINT/SIGTERM, closes DB connections on exit, and provides a
  27:         should_continue_processing() method to let main stop gracefully.
  28:     - sanitise_filename(text)
  29:         Sanitises a string for safe use in filenames (removes or replaces unsafe
  30:         characters).
  31:     - flatten_json(nested_json, parent_key='', sep='_')
  32:         Flattens arbitrarily nested JSON (dicts/lists) into a single-level dict with
  33:         concatenated keys suitable for CSV or tabular storage.
  34:     - create_inspection_folder_structure(date_str, lcd_inspection_id)
  35:         Builds and creates a hierarchical path base_path/YYYY/MM/YYYY-MM-DD <id>
  36:         for storing inspection payloads/attachments; falls back to base_path/unknown_date.
  37:     - construct_attachment_filename(lcd_inspection_id, date_str, attachment_num)
  38:         Returns a standardized filename for attachments including a sanitized inspection
  39:         id, date (YYYYMMDD or "unknown") and zero-padded sequence number.
  40:     - calculate_md5_hash(file_path)
  41:         Computes and returns the MD5 hex digest for the specified file path.
  42:         Returns empty string on error.
  43:     - validate_attachment_file(file_path, expected_min_size=1024)
  44:         Performs basic integrity checks on a downloaded file: existence, minimum
  45:         size threshold, and a small header read to detect emptiness. Returns
  46:         (is_valid, file_size_bytes, error_message).
  47:     - save_formatted_payload_text_file(inspection_folder, response_data, lcd_inspection_id)
  48:         Writes a human-readable inspection report (and optionally the full JSON
  49:         payload) to a text file inside inspection_folder. Uses hash_manager to
  50:         resolve hashed fields to human-readable names where available. Returns the
  51:         saved file Path or None on I/O error.
  52:     - make_api_request(url, headers, tip_value, max_retries=5, backoff_factor=2,
  53:                     timeout=30, max_backoff=60)
  54:         Performs a GET with exponential backoff and retries on transient
  55:         connection/timeout errors. Attaches a private _retry_count attribute to the
  56:         returned Response for bookkeeping.
  57:     - handle_api_error(response, tip_value, request_url)
  58:         Produces a detailed, human-friendly error string for logging and DB storage
  59:         based on HTTP response code and body.
  60:     - download_attachment(attachment_url, filename, lcd_inspection_id, attachment_tip,
  61:                         inspection_folder, record_tip, attachment_sequence)
  62:         Downloads a single attachment to disk (temporary .tmp file -> final rename),
  63:         validates it, computes MD5, updates / inserts attachment row(s) and any
  64:         related processing_errors. Returns a tuple:
  65:         (success: bool, retry_count: int, file_size_mb: float, error_msg: Optional[str]).
  66:     - process_attachments(response_data, lcd_inspection_id, tip_value)
  67:         Orchestrates saving the payload file, creating the folder, iteratively
  68:         downloading and validating attachments, updating noggin_data status fields
  69:         (complete/partial/failed/interrupted) and logging a per-session record.
  70:     - insert_noggin_data_record(tip_value, response_data)
  71:         Parses response_data into typed fields (inspection_date, hashes, names,
  72:         load_compliance, $meta etc.) and INSERTs or UPDATEs the noggin_data table.
  73:         Also records the raw API payload and meta JSON for traceability.
  74:     - calculate_next_retry_time(retry_count)
  75:         Computes an exponential backoff next_retry_at datetime based on configured
  76:         retry settings. Returns a datetime far in the future when retry_count
  77:         exceeds configured max.
  78:     - get_tips_to_process_from_database(limit=10)
  79:         Convenience DB query that returns TIPs considered eligible for processing
  80:         according to retry/backoff rules and processing_status priorities.
  81:     - mark_permanently_failed(tip_value)
  82:         Marks a TIP as permanently failed after max retries are exhausted.
  83:     - should_process_tip(tip_value)
  84:         Inspect the noggin_data record (if present) and determines whether the TIP
  85:         should be processed now. Returns (should_process: bool, current_retry_count: Optional[int]).
  86:     - get_total_tip_count(tip_csv_file_path)
  87:         Counts valid (non-empty) TIPs in the CSV file for progress estimates.
  88:     - update_progress_tracking(processed_count, total_count, start_time_val)
  89:         Logs a progress update with TIPs/sec and ETA.
  90:     - log_shutdown_summary(processed_count, total_count, start_time_val, reason="manual")
  91:         Logs an overall shutdown summary including elapsed time and estimated
  92:         remaining duration.
  93:     - main()
  94:         The main processing loop: reads tip.csv, iterates rows, coordinates
  95:         circuit breaker interactions, API calling, DB updates, attachment processing,
  96:         retries/backoff and graceful shutdown. Returns number of TIPs processed.
  97:     Configuration and dependencies
  98:     ------------------------------
  99:     - Requires a ConfigLoader instance populated from:
 100:         'config/base_config.ini' and 'config/load_compliance_check_driver_loader_config.ini'
 101:     Expected config sections/keys used in this module (examples):
 102:         - [api]: base_url, media_service_url
 103:         - [processing]: too_many_requests_sleep_time, attachment_pause, max_api_retries,
 104:                         api_backoff_factor, api_max_backoff, api_timeout
 105:         - [output]: show_json_payload_in_text_file, show_compliance_status,
 106:                     filename_image_stub, unknown_response_output_text
 107:         - [paths]: base_output_path
 108:         - [retry]: max_retry_attempts, retry_backoff_multiplier
 109:         - object_type mapping via config.get_object_type_config()
 110:     - Relies on the following helper classes from common:
 111:         ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager,
 112:         CircuitBreaker, CircuitBreakerError
 113:     - External libraries used:
 114:         requests, psycopg2 (indirectly via DatabaseConnectionManager), standard
 115:         library modules (json, logging, pathlib, datetime, uuid, time, signal, sys,
 116:         atexit, hashlib, typing, csv).
 117:     Database schema expectations
 118:     ---------------------------
 119:     This module expects the following tables and important columns (non-exhaustive):
 120:     - noggin_data
 121:         - tip (primary key)
 122:         - object_type, inspection_date, lcd_inspection_id, coupling_id
 123:         - inspected_by, vehicle_hash, vehicle, vehicle_id, trailer_hash, trailer, trailer_id, ...
 124:         - load_compliance, processing_status, last_error_message
 125:         - retry_count, next_retry_at, last_retry_at, csv_imported_at
 126:         - total_attachments, completed_attachment_count, all_attachments_complete
 127:         - api_meta_created_date, api_meta_modified_date, api_meta_* fields
 128:         - api_payload_raw, api_meta_raw, updated_at, permanently_failed, has_unknown_hashes
 129:     - attachments
 130:         - record_tip, attachment_tip (unique constraint)
 131:         - attachment_sequence, filename, file_path
 132:         - attachment_status ('downloading', 'complete', 'failed', ...)
 133:         - attachment_validation_status ('not_validated', 'valid', 'validation_failed', ...)
 134:         - file_size_bytes, file_hash_md5
 135:         - download_started_at, download_completed_at, download_duration_seconds
 136:         - validation_error_message, last_error_message
 137:     - processing_errors
 138:         - tip, error_type, error_message, error_details (JSON/text), created_at
 139:     Side effects and filesystem behavior
 140:     -----------------------------------
 141:     - Writes per-inspection directories under configured base_output_path with
 142:     subfolders year/month and a folder named "<YYYY-MM-DD> <lcd_inspection_id>".
 143:     - Writes a human-readable inspection data text file (and optionally full JSON).
 144:     - Downloads attachments into the inspection folder; temporary files use .tmp
 145:     suffix until validation and rename succeed.
 146:     - Uses session-specific logging via LoggerManager to write a session log with a
 147:     header and per-record lines.
 148:     Important operational notes
 149:     ---------------------------
 150:     - Circuit breaker: before making API calls circuit_breaker.before_request() is
 151:     called; failures are recorded via circuit_breaker.record_failure() and a
 152:     successful call invokes circuit_breaker.record_success(). The circuit breaker
 153:     may prevent retries to avoid cascading failures.
 154:     - Retries/backoff: API-level transient errors are retried with exponential
 155:     backoff. Permanent HTTP errors (4xx/5xx) are stored in DB and the TIP is
 156:     scheduled for future retry based on calculate_next_retry_time().
 157:     - Graceful shutdown: the first SIGINT/SIGTERM will set an internal flag so the
 158:     script finishes the current TIP and stops taking new TIPs. A second signal
 159:     forces immediate cleanup and exit.
 160:     - Concurrency: the module is written for single-process execution. If multiple
 161:     workers/processes operate on the same DB, the schema must handle upserts and
 162:     conflicts appropriately (the code uses ON CONFLICT for key operations).
 163:     - Validation thresholds: validate_attachment_file() uses a default minimum file
 164:     size (1024 bytes). Adjust this threshold via the function parameters if the
 165:     media service produces smaller-but-valid files.
 166:     Exit codes
 167:     ----------
 168:     - main() returns 1 when fatal startup errors occur (missing CSV or CSV format).
 169:     - When executed as __main__, the script logs status and exits normally (0) on
 170:     success; unexpected exceptions are re-raised after logging.
 171:     Example usage
 172:     -------------
 173:     - Run from the system environment where config files and tip.csv are available:
 174:         python noggin_processor.py
 175:     - Ensure config files contain correct API endpoints, DB connection details (used
 176:     by DatabaseConnectionManager), and output path permissions.
 177:     Extensibility suggestions
 178:     -------------------------
 179:     - Abstract retry/backoff configuration to be injected for easier unit testing.
 180:     - Replace direct requests.get calls with a wrapper interface to facilitate
 181:     mocking in tests.
 182:     - Add more granular attachment type detection (content-type / magic bytes)
 183:     beyond basic size/header checks if required.
 184:     Security and privacy
 185:     --------------------
 186:     - Ensure access tokens or API credentials used by headers are kept secure and not
 187:     logged. This module attempts to avoid logging full payloads unless explicitly
 188:     enabled by configuration (show_json_payload_in_text_file).
 189:     - Attachment storage may contain sensitive images; secure file permissions and
 190:     accessible output path accordingly.
 191:     Limitations
 192:     -----------
 193:     - The module assumes stable database connectivity and that the DatabaseConnectionManager
 194:     implements execute_query_dict and execute_update semantics shown. Errors closing
 195:     DB connections are logged but not fatal during shutdown cleanup.
 196:     - Date parsing uses datetime.fromisoformat after normalizing 'Z' to '+00:00';
 197:     non-ISO date formats may be treated as unknown.
 198: """
 199: from __future__ import annotations
 200: import requests
 201: import json
 202: import logging
 203: import uuid
 204: from datetime import datetime, timedelta
 205: from pathlib import Path
 206: import time
 207: import signal
 208: import sys
 209: import atexit
 210: import hashlib
 211: from typing import Optional, List, Dict, Any, Tuple
 212: 
 213: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager, CircuitBreaker, CircuitBreakerError, UNKNOWN_TEXT
 214: 
 215: start_time: float = time.perf_counter()
 216: 
 217: config: ConfigLoader = ConfigLoader(
 218:     'config/base_config.ini',
 219:     'config/load_compliance_check_driver_loader_config.ini'
 220: )
 221: 
 222: batch_session_id: str = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_batch_{str(uuid.uuid4())[:8].upper()}"
 223: 
 224: logger_manager: LoggerManager = LoggerManager(config, script_name=Path(__file__).stem)
 225: logger_manager.configure_application_logger()
 226: session_logger: logging.Logger = logger_manager.create_session_logger(batch_session_id)
 227: 
 228: logger: logging.Logger = logging.getLogger(__name__)
 229: 
 230: db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
 231: hash_manager: HashManager = HashManager(config, db_manager)
 232: circuit_breaker: CircuitBreaker = CircuitBreaker(config)
 233: 
 234: base_url: str = config.get('api', 'base_url')
 235: attachment_base_url: str = config.get('api', 'media_service_url')
 236: headers: Dict[str, str] = config.get_api_headers()
 237: 
 238: base_path: Path = Path(config.get('paths', 'base_output_path'))
 239: base_path.mkdir(parents=True, exist_ok=True)
 240: 
 241: too_many_requests_sleep_time: int = config.getint('processing', 'too_many_requests_sleep_time')
 242: attachment_pause: int = config.getint('processing', 'attachment_pause')
 243: max_api_retries: int = config.getint('processing', 'max_api_retries')
 244: api_backoff_factor: int = config.getint('processing', 'api_backoff_factor')
 245: api_max_backoff: int = config.getint('processing', 'api_max_backoff')
 246: api_timeout: int = config.getint('processing', 'api_timeout')
 247: 
 248: show_json_payload_in_text_file: bool = config.getboolean('output', 'show_json_payload_in_text_file', from_specific=True)
 249: show_compliance_status: bool = config.getboolean('output', 'show_compliance_status', from_specific=True)
 250: filename_image_stub: str = config.get('output', 'filename_image_stub', from_specific=True)
 251: unknown_response_output_text: str = config.get('output', 'unknown_response_output_text', from_specific=True)
 252: folder_pattern: str = config.get('output', 'folder_pattern', from_specific=True)
 253: attachment_pattern: str = config.get('output', 'attachment_pattern', from_specific=True)
 254: 
 255: abbreviation: str = config.get('object_type', 'abbreviation', from_specific=True)
 256: 
 257: object_type_config: Dict[str, str] = config.get_object_type_config()
 258: endpoint_template: str = object_type_config['endpoint']
 259: object_type: str = object_type_config['object_type']
 260: 
 261: shutdown_requested: bool = False
 262: current_tip_being_processed: Optional[str] = None
 263: 
 264: logger.info("="*80)
 265: logger.info(f"NOGGIN PROCESSOR - {object_type.upper()}")
 266: logger.info("="*80)
 267: logger.info(f"Session ID:         {batch_session_id}")
 268: logger.info(f"Object Type:        {object_type}")
 269: logger.info(f"Abbreviation:       {abbreviation}")
 270: logger.info(f"Base Output Path:   {base_path}")
 271: logger.info(f"Folder Pattern:     {folder_pattern}")
 272: logger.info(f"Attachment Pattern: {attachment_pattern}")
 273: logger.info("="*80)
 274: 
 275: session_logger.info(f"SESSION START: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
 276: session_logger.info(f"SESSION ID: {batch_session_id}")
 277: session_logger.info(f"OBJECT TYPE: {object_type}")
 278: session_logger.info("")
 279: session_logger.info("TIMESTAMP\tTIP\tINSPECTION_ID\tATTACHMENTS_COUNT\tATTACHMENT_FILENAMES")
 280: 
 281: class GracefulShutdownHandler:
 282:     """Handles Ctrl+C and system shutdown signals"""
 283: 
 284:     def __init__(self, db_conn: DatabaseConnectionManager, logger_instance: logging.Logger) -> None:
 285:         self.db_conn: DatabaseConnectionManager = db_conn
 286:         self.logger: logging.Logger = logger_instance
 287:         self.shutdown_requested: bool = False
 288: 
 289:         signal.signal(signal.SIGINT, self._signal_handler)
 290:         signal.signal(signal.SIGTERM, self._signal_handler)
 291:         atexit.register(self._cleanup_on_exit)
 292: 
 293:         self.logger.info("Graceful shutdown handler initialised")
 294: 
 295:     def _signal_handler(self, signum: int, frame: Any) -> None:
 296:         global shutdown_requested
 297:         signal_name: str = "SIGINT (Ctrl+C)" if signum == signal.SIGINT else f"Signal {signum}"
 298: 
 299:         if not self.shutdown_requested:
 300:             self.shutdown_requested = True
 301:             shutdown_requested = True
 302:             self.logger.warning(f"\n{signal_name} received. Finishing current TIP then shutting down...")
 303:             self.logger.warning(f"Currently processing: {current_tip_being_processed or 'None'}")
 304:             self.logger.warning("Press Ctrl+C again to force immediate exit")
 305:         else:
 306:             self.logger.error("Second shutdown signal - forcing immediate exit")
 307:             self._emergency_cleanup()
 308:             sys.exit(1)
 309: 
 310:     def _cleanup_on_exit(self) -> None:
 311:         if self.db_conn:
 312:             try:
 313:                 self.db_conn.close_all()
 314:             except Exception as e:
 315:                 self.logger.error(f"Error during exit cleanup: {e}")
 316: 
 317:     def _emergency_cleanup(self) -> None:
 318:         try:
 319:             if self.db_conn:
 320:                 self.db_conn.close_all()
 321:         except:
 322:             pass
 323: 
 324:     def should_continue_processing(self) -> bool:
 325:         return not self.shutdown_requested
 326: 
 327: shutdown_handler: GracefulShutdownHandler = GracefulShutdownHandler(db_manager, logger)
 328: 
 329: def sanitise_filename(text: Optional[str]) -> str:
 330:     """Sanitise text for use in filenames"""
 331:     if not text:
 332:         return "unknown"
 333:     return (text.replace(" - ", "-").replace(" ", "_").replace("/", "")
 334:             .replace("\\", "").replace("*", "").replace("<", "")
 335:             .replace(">", "").replace("?", "").replace("|", "").replace(":", ""))
 336: 
 337: def flatten_json(nested_json: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
 338:     """Flatten nested JSON structure"""
 339:     items: List[Tuple[str, Any]] = []
 340:     for key, value in nested_json.items():
 341:         new_key: str = f"{parent_key}{sep}{key}" if parent_key else key
 342: 
 343:         if isinstance(value, dict):
 344:             items.extend(flatten_json(value, new_key, sep=sep).items())
 345:         elif isinstance(value, list):
 346:             for i, item in enumerate(value):
 347:                 if isinstance(item, dict):
 348:                     items.extend(flatten_json(item, f"{new_key}_{i}", sep=sep).items())
 349:                 else:
 350:                     items.append((f"{new_key}_{i}", item))
 351:         else:
 352:             items.append((new_key, value))
 353:     return dict(items)
 354: 
 355: def create_inspection_folder_structure(date_str: str, inspection_id: str) -> Path:
 356:     """Create hierarchical folder structure for inspection using configured pattern.
 357: 
 358:     Pattern placeholders: {abbreviation}, {year}, {month}, {date}, {inspection_id}
 359: 
 360:     Args:
 361:         date_str: Date string in ISO format (e.g. "2023-05-20" or "2023-05-20Z")
 362:         inspection_id: Unique identifier for the inspection
 363: 
 364:     Returns:
 365:         Path object pointing to the created inspection folder
 366:     """
 367:     try:
 368:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 369:         year: str = str(date_obj.year)
 370:         month: str = f"{date_obj.month:02d}"
 371:         formatted_date: str = date_obj.strftime('%Y-%m-%d')
 372: 
 373:         # Substitute pattern placeholders
 374:         folder_path: str = folder_pattern.format(
 375:             abbreviation=abbreviation,
 376:             year=year,
 377:             month=month,
 378:             date=formatted_date,
 379:             inspection_id=inspection_id
 380:         )
 381: 
 382:         inspection_folder: Path = base_path / folder_path
 383:         inspection_folder.mkdir(parents=True, exist_ok=True)
 384:         logger.debug(f"Created inspection folder: {inspection_folder}")
 385:         return inspection_folder
 386:     except (ValueError, AttributeError) as e:
 387:         logger.warning(f"Could not parse date '{date_str}': {e}")
 388:         folder_name = f"unknown-date {inspection_id}"
 389:         fallback_folder: Path = base_path / abbreviation / "unknown_date" / folder_name
 390:         fallback_folder.mkdir(parents=True, exist_ok=True)
 391:         logger.info(f"Created fallback folder: {fallback_folder}")
 392:         return fallback_folder
 393: 
 394: def construct_attachment_filename(inspection_id: str, date_str: str, attachment_num: int) -> str:
 395:     """Construct attachment filename using configured pattern.
 396:     
 397:     Pattern placeholders: {abbreviation}, {inspection_id}, {date}, {stub}, {sequence}
 398:     """
 399:     sanitised_id: str = sanitise_filename(inspection_id)
 400: 
 401:     try:
 402:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 403:         date_part: str = date_obj.strftime('%Y-%m-%d')
 404:     except (ValueError, AttributeError):
 405:         logger.warning(f"Could not parse date '{date_str}', using 'unknown'")
 406:         date_part = "unknown"
 407: 
 408:     filename: str = attachment_pattern.format(
 409:         abbreviation=abbreviation,
 410:         inspection_id=sanitised_id,
 411:         date=date_part,
 412:         stub=filename_image_stub,
 413:         sequence=f"{attachment_num:03d}"
 414:     )
 415:     return filename
 416: 
 417: def calculate_md5_hash(file_path: Path) -> str:
 418:     """Calculate MD5 hash of file"""
 419:     md5_hash: hashlib._Hash = hashlib.md5()
 420:     try:
 421:         with open(file_path, 'rb') as f:
 422:             for chunk in iter(lambda: f.read(8192), b""):
 423:                 md5_hash.update(chunk)
 424:         return md5_hash.hexdigest()
 425:     except Exception as e:
 426:         logger.warning(f"Could not calculate MD5 for {file_path}: {e}")
 427:         return ""
 428: 
 429: def validate_attachment_file(file_path: Path, expected_min_size: int = 1024) -> Tuple[bool, int, Optional[str]]:
 430:     """Validate downloaded file integrity"""
 431:     try:
 432:         if not file_path.exists():
 433:             return False, 0, "File does not exist"
 434: 
 435:         file_size: int = file_path.stat().st_size
 436: 
 437:         if file_size < expected_min_size:
 438:             return False, file_size, f"File too small ({file_size} bytes)"
 439: 
 440:         with open(file_path, 'rb') as f:
 441:             header_bytes: bytes = f.read(10)
 442:             if len(header_bytes) == 0:
 443:                 return False, file_size, "File appears empty"
 444: 
 445:         return True, file_size, None
 446: 
 447:     except Exception as e:
 448:         return False, 0, f"Validation error: {e}"
 449: 
 450: def save_formatted_payload_text_file(inspection_folder: Path, response_data: Dict[str, Any],
 451:                                     lcd_inspection_id: str) -> Optional[Path]:
 452:     """Generate formatted text file with inspection data"""
 453:     sanitised_lcd_id: str = sanitise_filename(lcd_inspection_id)
 454:     payload_filename: str = f"{sanitised_lcd_id}_inspection_data.txt"
 455:     payload_path: Path = inspection_folder / payload_filename
 456: 
 457:     try:
 458:         with open(payload_path, 'w', encoding='utf-8') as f:
 459:             f.write("="*60 + "\n")
 460:             f.write("LOAD COMPLIANCE CHECK INSPECTION REPORT\n")
 461:             f.write(f"RECORD GENERATED: {datetime.now().strftime('%d-%m-%Y')}\n")
 462:             f.write("="*60 + "\n\n")
 463: 
 464:             f.write(f"LCD Inspection ID:     {response_data.get('lcdInspectionId', unknown_response_output_text)}\n\n")
 465:             f.write(f"Date:                  {response_data.get('date', unknown_response_output_text)}\n\n")
 466:             f.write(f"Inspected By:          {response_data.get('inspectedBy', unknown_response_output_text)}\n\n")
 467: 
 468:             vehicle_hash: str = response_data.get('vehicle', '')
 469:             vehicle_name: str = hash_manager.lookup_hash('vehicle', vehicle_hash, response_data.get('tip', ''), lcd_inspection_id) if vehicle_hash else unknown_response_output_text
 470:             f.write(f"Vehicle:               {vehicle_name}\n\n")
 471:             f.write(f"Vehicle ID:            {response_data.get('vehicleId', unknown_response_output_text)}\n\n")
 472: 
 473:             trailer_hash: str = response_data.get('trailer', '')
 474:             trailer_name: str = hash_manager.lookup_hash('trailer', trailer_hash, response_data.get('tip', ''), lcd_inspection_id) if trailer_hash else unknown_response_output_text
 475:             f.write(f"Trailer:               {trailer_name}\n\n")
 476:             f.write(f"Trailer ID:            {response_data.get('trailerId', unknown_response_output_text)}\n\n")
 477: 
 478:             trailer2_hash: str = response_data.get('trailer2', '')
 479:             if trailer2_hash:
 480:                 trailer2_name: str = hash_manager.lookup_hash('trailer', trailer2_hash, response_data.get('tip', ''), lcd_inspection_id)
 481:                 f.write(f"Trailer 2:             {trailer2_name}\n\n")
 482:                 f.write(f"Trailer 2 ID:          {response_data.get('trailerId2', unknown_response_output_text)}\n\n")
 483: 
 484:             trailer3_hash: str = response_data.get('trailer3', '')
 485:             if trailer3_hash:
 486:                 trailer3_name: str = hash_manager.lookup_hash('trailer', trailer3_hash, response_data.get('tip', ''), lcd_inspection_id)
 487:                 f.write(f"Trailer 3:             {trailer3_name}\n\n")
 488:                 f.write(f"Trailer 3 ID:          {response_data.get('trailerId3', unknown_response_output_text)}\n\n")
 489: 
 490:             f.write(f"Job Number:            {response_data.get('jobNumber', unknown_response_output_text)}\n\n")
 491:             f.write(f"Run Number:            {response_data.get('runNumber', unknown_response_output_text)}\n\n")
 492:             f.write(f"Driver/Loader Name:    {response_data.get('driverLoaderName', unknown_response_output_text)}\n\n")
 493: 
 494:             dept_hash: str = response_data.get('whichDepartmentDoesTheLoadBelongTo', '')
 495:             dept_name: str = hash_manager.lookup_hash('department', dept_hash, response_data.get('tip', ''), lcd_inspection_id) if dept_hash else unknown_response_output_text
 496:             f.write(f"Department:            {dept_name}\n\n")
 497: 
 498:             team_hash: str = response_data.get('team', '')
 499:             team_name: str = hash_manager.lookup_hash('team', team_hash, response_data.get('tip', ''), lcd_inspection_id) if team_hash else unknown_response_output_text
 500:             f.write(f"Team:                  {team_name}\n\n")
 501: 
 502:             if show_compliance_status:
 503:                 compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 504:                 compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 505:                 if compliant_yes:
 506:                     f.write("Load Compliance:       COMPLIANT\n\n")
 507:                 elif compliant_no:
 508:                     f.write("Load Compliance:       NON-COMPLIANT\n\n")
 509:                 else:
 510:                     f.write(f"Load Compliance:       {unknown_response_output_text}\n\n")
 511: 
 512:             straps_value = response_data.get('straps')
 513:             if straps_value is not None:
 514:                 f.write(f"Straps:                {straps_value}\n\n")
 515:                 no_of_straps = response_data.get('noOfStraps', UNKNOWN_TEXT)
 516:                 f.write(f"Number of Straps:      {no_of_straps}\n\n")
 517: 
 518:             chains_value = response_data.get('chains')
 519:             if chains_value is not None:
 520:                 f.write(f"Chains:                {chains_value}\n\n")
 521: 
 522:             mass_value = response_data.get('mass', UNKNOWN_TEXT)
 523:             f.write(f"Mass:                  {mass_value}\n\n")
 524: 
 525:             attachment_count: int = len(response_data.get('attachments', []))
 526:             f.write(f"Attachments:           {attachment_count}\n\n")
 527: 
 528:             if show_json_payload_in_text_file:
 529:                 f.write("-"*60 + "\n")
 530:                 f.write("COMPLETE TECHNICAL DATA (JSON FORMAT)\n")
 531:                 f.write("-"*60 + "\n\n")
 532:                 json.dump(response_data, f, indent=2, ensure_ascii=False)
 533: 
 534:         logger.info(f"Saved formatted payload to: {payload_path}")
 535:         return payload_path
 536: 
 537:     except IOError as e:
 538:         logger.error(f"IOError saving payload {payload_path}: {e}", exc_info=True)
 539:         return None
 540: 
 541: def make_api_request(url: str, headers: Dict[str, str], tip_value: str,
 542:                     max_retries: int = 5, backoff_factor: int = 2,
 543:                     timeout: int = 30, max_backoff: int = 60) -> requests.Response:
 544:     """Make API request with exponential backoff retry logic"""
 545:     last_exception: Optional[Exception] = None
 546: 
 547:     for attempt in range(max_retries):
 548:         try:
 549:             logger.debug(f"API request attempt {attempt + 1}/{max_retries} for TIP {tip_value}")
 550:             response: requests.Response = requests.get(url, headers=headers, timeout=timeout)
 551:             response._retry_count = attempt
 552:             logger.debug(f"Request attempt {attempt + 1} succeeded for TIP {tip_value}")
 553:             return response
 554: 
 555:         except requests.exceptions.ConnectionError as connection_error:
 556:             last_exception = connection_error
 557: 
 558:             if attempt == max_retries - 1:
 559:                 logger.error(f"All {max_retries} connection attempts failed for TIP {tip_value}", exc_info=True)
 560:                 raise connection_error
 561: 
 562:             wait_time: float = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 563:             logger.warning(f"Connection failed for TIP {tip_value}, retrying in {wait_time}s... "
 564:                           f"(attempt {attempt + 1}/{max_retries})")
 565:             time.sleep(wait_time)
 566: 
 567:         except requests.exceptions.Timeout as timeout_error:
 568:             last_exception = timeout_error
 569: 
 570:             if attempt == max_retries - 1:
 571:                 logger.error(f"All {max_retries} timeout attempts failed for TIP {tip_value}", exc_info=True)
 572:                 raise timeout_error
 573: 
 574:             wait_time = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 575:             logger.warning(f"Request timeout for TIP {tip_value}, retrying in {wait_time}s... "
 576:                           f"(attempt {attempt + 1}/{max_retries})")
 577:             time.sleep(wait_time)
 578: 
 579:         except requests.exceptions.RequestException as request_error:
 580:             last_exception = request_error
 581: 
 582:             if attempt == max_retries - 1:
 583:                 logger.error(f"Request failed permanently for TIP {tip_value}: {str(request_error)}", exc_info=True)
 584:                 raise request_error
 585: 
 586:             wait_time = backoff_factor
 587:             logger.warning(f"Request error for TIP {tip_value}, retrying in {wait_time}s... "
 588:                           f"(attempt {attempt + 1}/{max_retries})")
 589:             time.sleep(wait_time)
 590: 
 591:     if last_exception:
 592:         raise last_exception
 593:     else:
 594:         raise Exception(f"Unexpected error in retry logic for TIP {tip_value}")
 595: 
 596: def handle_api_error(response: requests.Response, tip_value: str, request_url: str) -> str:
 597:     """Generate detailed error message from API response"""
 598:     status_code: int = response.status_code
 599: 
 600:     try:
 601:         response_text: str = response.text
 602:         if response_text:
 603:             try:
 604:                 error_json: Dict[str, Any] = response.json()
 605:                 additional_info: str = f" Response body: {json.dumps(error_json, indent=2)}"
 606:             except json.JSONDecodeError:
 607:                 additional_info = f" Response body: {response_text[:500]}{'...' if len(response_text) > 500 else ''}"
 608:         else:
 609:             additional_info = " (No response body provided)"
 610:     except Exception:
 611:         additional_info = " (Could not read response body)"
 612: 
 613:     if status_code == 401:
 614:         error_message: str = (f"Authentication failed for TIP {tip_value}. "
 615:                              f"Status code: {status_code} (Unauthorised). "
 616:                              f"The access token is missing or invalid. "
 617:                              f"URL: {request_url}{additional_info}")
 618: 
 619:     elif status_code == 403:
 620:         error_message = (f"Access forbidden for TIP {tip_value}. "
 621:                         f"Status code: {status_code} (Forbidden). "
 622:                         f"You don't have permission to access this resource. "
 623:                         f"URL: {request_url}{additional_info}")
 624: 
 625:     elif status_code == 404:
 626:         error_message = (f"Resource not found for TIP {tip_value}. "
 627:                         f"Status code: {status_code} (Not Found). "
 628:                         f"The requested object does not exist. "
 629:                         f"URL: {request_url}{additional_info}")
 630: 
 631:     elif status_code == 429:
 632:         error_message = (f"Rate limit exceeded for TIP {tip_value}. "
 633:                         f"Status code: {status_code} (Too Many Requests). "
 634:                         f"URL: {request_url}{additional_info}")
 635: 
 636:     elif 400 <= status_code < 500:
 637:         error_message = (f"Client error for TIP {tip_value}. "
 638:                         f"Status code: {status_code}. "
 639:                         f"URL: {request_url}{additional_info}")
 640: 
 641:     elif 500 <= status_code < 600:
 642:         error_message = (f"Server error for TIP {tip_value}. "
 643:                         f"Status code: {status_code}. "
 644:                         f"URL: {request_url}{additional_info}")
 645: 
 646:     else:
 647:         error_message = (f"Unexpected response for TIP {tip_value}. "
 648:                         f"Status code: {status_code}. "
 649:                         f"URL: {request_url}{additional_info}")
 650: 
 651:     return error_message
 652: 
 653: def download_attachment(attachment_url: str, filename: str, lcd_inspection_id: str,
 654:                        attachment_tip: str, inspection_folder: Path,
 655:                        record_tip: str, attachment_sequence: int) -> Tuple[bool, int, float, Optional[str]]:
 656:     """Download and validate attachment with database tracking"""
 657:     if attachment_url.startswith('/media'):
 658:         attachment_url = attachment_url[6:]
 659: 
 660:     full_url: str = attachment_base_url + attachment_url
 661:     output_path: Path = inspection_folder / filename
 662:     temp_path: Path = output_path.with_suffix('.tmp')
 663: 
 664:     existing_attachment: List[Dict[str, Any]] = db_manager.execute_query_dict(
 665:         "SELECT attachment_status, file_size_bytes FROM attachments WHERE record_tip = %s AND attachment_tip = %s",
 666:         (record_tip, attachment_tip)
 667:     )
 668: 
 669:     if existing_attachment and existing_attachment[0]['attachment_status'] == 'complete':
 670:         if output_path.exists():
 671:             is_valid, file_size, error_msg = validate_attachment_file(output_path)
 672:             if is_valid:
 673:                 file_size_mb: float = file_size / (1024 * 1024)
 674:                 logger.info(f"Skipping existing valid attachment: {filename} ({file_size_mb:.2f} MB)")
 675:                 return True, 0, file_size_mb, None
 676: 
 677:     download_start_time: float = time.perf_counter()
 678:     logger.info(f"Downloading {lcd_inspection_id}: {filename}")
 679: 
 680:     db_manager.execute_update(
 681:         """
 682:         INSERT INTO attachments (
 683:             record_tip, attachment_tip, attachment_sequence, filename, file_path,
 684:             attachment_status, attachment_validation_status, download_started_at
 685:         ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
 686:         ON CONFLICT (record_tip, attachment_tip)
 687:         DO UPDATE SET
 688:             attachment_status = EXCLUDED.attachment_status,
 689:             download_started_at = EXCLUDED.download_started_at
 690:         """,
 691:         (record_tip, attachment_tip, attachment_sequence, filename, str(output_path.resolve()),
 692:          'downloading', 'not_validated', datetime.now())
 693:     )
 694: 
 695:     try:
 696:         response: requests.Response = make_api_request(full_url, headers, f"attachment {filename}", timeout=60)
 697:         retry_count: int = getattr(response, '_retry_count', 0)
 698: 
 699:         if response.status_code == 200:
 700:             with open(temp_path, 'wb') as f:
 701:                 f.write(response.content)
 702: 
 703:             is_valid, file_size, validation_error = validate_attachment_file(temp_path)
 704: 
 705:             if is_valid:
 706:                 temp_path.rename(output_path)
 707: 
 708:                 file_hash: str = calculate_md5_hash(output_path)
 709:                 download_duration: float = time.perf_counter() - download_start_time
 710:                 file_size_mb = file_size / (1024 * 1024)
 711: 
 712:                 db_manager.execute_update(
 713:                     """
 714:                     UPDATE attachments
 715:                     SET attachment_status = %s,
 716:                         attachment_validation_status = %s,
 717:                         file_size_bytes = %s,
 718:                         file_hash_md5 = %s,
 719:                         download_completed_at = %s,
 720:                         download_duration_seconds = %s
 721:                     WHERE record_tip = %s AND attachment_tip = %s
 722:                     """,
 723:                     ('complete', 'valid', file_size, file_hash, datetime.now(),
 724:                      round(download_duration, 2), record_tip, attachment_tip)
 725:                 )
 726: 
 727:                 logger.info(f"Downloaded: {filename} ({file_size_mb:.2f} MB) in {download_duration:.2f}s")
 728:                 return True, retry_count, file_size_mb, None
 729:             else:
 730:                 if temp_path.exists():
 731:                     temp_path.unlink()
 732: 
 733:                 error_msg: str = f"Validation failed: {validation_error}"
 734:                 logger.error(f"Download validation failed: {error_msg}")
 735: 
 736:                 db_manager.execute_update(
 737:                     """
 738:                     UPDATE attachments
 739:                     SET attachment_status = %s,
 740:                         attachment_validation_status = %s,
 741:                         validation_error_message = %s,
 742:                         last_error_message = %s
 743:                     WHERE record_tip = %s AND attachment_tip = %s
 744:                     """,
 745:                     ('failed', 'validation_failed', validation_error, error_msg, record_tip, attachment_tip)
 746:                 )
 747: 
 748:                 db_manager.execute_update(
 749:                     """
 750:                     INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 751:                     VALUES (%s, %s, %s, %s)
 752:                     """,
 753:                     (record_tip, 'attachment_failed', error_msg, json.dumps({
 754:                         'filename': filename,
 755:                         'attachment_tip': attachment_tip,
 756:                         'validation_error': validation_error
 757:                     }))
 758:                 )
 759: 
 760:                 return False, retry_count, 0, error_msg
 761:         else:
 762:             if temp_path.exists():
 763:                 temp_path.unlink()
 764: 
 765:             error_msg = f"HTTP {response.status_code}"
 766:             download_duration = time.perf_counter() - download_start_time
 767:             error_details: str = handle_api_error(response, f"attachment {filename}", full_url)
 768:             logger.error(f"Download failed: {error_details}")
 769: 
 770:             db_manager.execute_update(
 771:                 """
 772:                 UPDATE attachments
 773:                 SET attachment_status = %s,
 774:                     last_error_message = %s
 775:                 WHERE record_tip = %s AND attachment_tip = %s
 776:                 """,
 777:                 ('failed', error_msg, record_tip, attachment_tip)
 778:             )
 779: 
 780:             db_manager.execute_update(
 781:                 """
 782:                 INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 783:                 VALUES (%s, %s, %s, %s)
 784:                 """,
 785:                 (record_tip, 'attachment_failed', error_msg, json.dumps({
 786:                     'filename': filename,
 787:                     'attachment_tip': attachment_tip,
 788:                     'http_status': response.status_code,
 789:                     'url': full_url
 790:                 }))
 791:             )
 792: 
 793:             return False, retry_count, 0, error_msg
 794: 
 795:     except Exception as e:
 796:         if temp_path.exists():
 797:             temp_path.unlink()
 798: 
 799:         error_msg = f"Exception: {str(e)}"
 800:         download_duration = time.perf_counter() - download_start_time
 801:         logger.error(f"Download exception: {filename} - {error_msg}", exc_info=True)
 802: 
 803:         db_manager.execute_update(
 804:             """
 805:             UPDATE attachments
 806:             SET attachment_status = %s,
 807:                 last_error_message = %s
 808:             WHERE record_tip = %s AND attachment_tip = %s
 809:             """,
 810:             ('failed', error_msg, record_tip, attachment_tip)
 811:         )
 812: 
 813:         db_manager.execute_update(
 814:             """
 815:             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 816:             VALUES (%s, %s, %s, %s)
 817:             """,
 818:             (record_tip, 'attachment_failed', error_msg, json.dumps({
 819:                 'filename': filename,
 820:                 'attachment_tip': attachment_tip,
 821:                 'exception': str(e)
 822:             }))
 823:         )
 824: 
 825:         return False, 0, 0, error_msg
 826: 
 827: def process_attachments(response_data: Dict[str, Any], lcd_inspection_id: str, tip_value: str) -> None:
 828:     """Process all attachments for an inspection with database tracking"""
 829:     global shutdown_requested
 830: 
 831:     if shutdown_requested:
 832:         logger.warning(f"Shutdown requested during {lcd_inspection_id}")
 833:         db_manager.execute_update(
 834:             "UPDATE noggin_data SET processing_status = %s WHERE tip = %s",
 835:             ('interrupted', tip_value)
 836:         )
 837:         return
 838: 
 839:     processing_start_time: float = time.perf_counter()
 840: 
 841:     date_str: str = response_data.get('date', '')
 842:     inspection_folder: Path = create_inspection_folder_structure(date_str, lcd_inspection_id)
 843:     save_formatted_payload_text_file(inspection_folder, response_data, lcd_inspection_id)
 844: 
 845:     if 'attachments' not in response_data or not response_data['attachments']:
 846:         processing_end_time: float = time.perf_counter()
 847:         processing_duration: float = processing_end_time - processing_start_time
 848: 
 849:         logger.info(f"No attachments found for {lcd_inspection_id}")
 850:         session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{lcd_inspection_id}\t0\tNONE")
 851: 
 852:         db_manager.execute_update(
 853:             """
 854:             UPDATE noggin_data
 855:             SET processing_status = %s,
 856:                 total_attachments = 0,
 857:                 completed_attachment_count = 0,
 858:                 all_attachments_complete = TRUE
 859:             WHERE tip = %s
 860:             """,
 861:             ('complete', tip_value)
 862:         )
 863:         return
 864: 
 865:     attachments: List[str] = response_data['attachments']
 866:     logger.info(f"Processing {len(attachments)} attachments for {lcd_inspection_id}")
 867: 
 868:     db_manager.execute_update(
 869:         "UPDATE noggin_data SET total_attachments = %s WHERE tip = %s",
 870:         (len(attachments), tip_value)
 871:     )
 872: 
 873:     successful_downloads: int = 0
 874:     attachment_filenames: List[str] = []
 875:     total_attachment_retries: int = 0
 876:     total_file_size_mb: float = 0.0
 877: 
 878:     for i, attachment_url in enumerate(attachments, 1):
 879:         if shutdown_requested:
 880:             logger.warning(f"Shutdown during attachment {i}/{len(attachments)} for {lcd_inspection_id}")
 881:             break
 882: 
 883:         attachment_tip: str = attachment_url.split('tip=')[-1] if 'tip=' in attachment_url else 'unknown'
 884:         filename: str = construct_attachment_filename(lcd_inspection_id, date_str, i)
 885: 
 886:         success, retry_count, file_size_mb, error_msg = download_attachment(
 887:             attachment_url, filename, lcd_inspection_id, attachment_tip,
 888:             inspection_folder, tip_value, i
 889:         )
 890: 
 891:         total_attachment_retries += retry_count
 892: 
 893:         if success:
 894:             successful_downloads += 1
 895:             attachment_filenames.append(filename)
 896:             total_file_size_mb += file_size_mb
 897: 
 898:         if attachment_pause > 0 and i < len(attachments):
 899:             logger.debug(f"Pausing {attachment_pause}s before next attachment")
 900:             time.sleep(attachment_pause)
 901: 
 902:     processing_end_time = time.perf_counter()
 903:     processing_duration = processing_end_time - processing_start_time
 904: 
 905:     if shutdown_requested:
 906:         final_status: str = 'interrupted'
 907:     elif successful_downloads == len(attachments):
 908:         final_status = 'complete'
 909:     elif successful_downloads > 0:
 910:         final_status = 'partial'
 911:     else:
 912:         final_status = 'failed'
 913: 
 914:     db_manager.execute_update(
 915:         """
 916:         UPDATE noggin_data
 917:         SET processing_status = %s
 918:         WHERE tip = %s
 919:         """,
 920:         (final_status, tip_value)
 921:     )
 922: 
 923:     logger.info(f"Inspection complete for {lcd_inspection_id}: {successful_downloads}/{len(attachments)} attachments")
 924: 
 925:     attachment_names_str: str = ";".join(attachment_filenames) if attachment_filenames else "FAILED"
 926:     session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{lcd_inspection_id}\t{successful_downloads}\t{attachment_names_str}")
 927: 
 928: def insert_noggin_data_record(tip_value: str, response_data: Dict[str, Any]) -> None:
 929:     """Insert or update noggin_data record with API response"""
 930:     meta: Dict[str, Any] = response_data.get('$meta', {})
 931: 
 932:     lcd_inspection_id: Optional[str] = response_data.get('lcdInspectionId')
 933:     coupling_id: Optional[str] = response_data.get('couplingId')
 934: 
 935:     inspection_date_str: Optional[str] = response_data.get('date')
 936:     inspection_date: Optional[datetime] = None
 937:     if inspection_date_str:
 938:         try:
 939:             inspection_date = datetime.fromisoformat(inspection_date_str.replace('Z', '+00:00'))
 940:         except (ValueError, AttributeError):
 941:             logger.warning(f"Could not parse date: {inspection_date_str}")
 942: 
 943:     vehicle_hash: Optional[str] = response_data.get('vehicle')
 944:     vehicle: Optional[str] = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, lcd_inspection_id) if vehicle_hash else None
 945:     if vehicle_hash:
 946:         vehicle = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, lcd_inspection_id)
 947:         hash_manager.update_lookup_type_if_unknown(vehicle_hash, 'vehicle')
 948: 
 949:     trailer_hash: Optional[str] = response_data.get('trailer')
 950:     trailer: Optional[str] = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, lcd_inspection_id) if trailer_hash else None
 951:     if trailer_hash:
 952:         trailer = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, lcd_inspection_id)
 953:         hash_manager.update_lookup_type_if_unknown(trailer_hash, 'trailer')
 954: 
 955:     trailer2_hash: Optional[str] = response_data.get('trailer2')
 956:     trailer2: Optional[str] = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, lcd_inspection_id) if trailer2_hash else None
 957:     if trailer2_hash:
 958:         trailer2 = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, lcd_inspection_id)
 959:         hash_manager.update_lookup_type_if_unknown(trailer2_hash, 'trailer')
 960:         
 961:     trailer3_hash: Optional[str] = response_data.get('trailer3')
 962:     trailer3: Optional[str] = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, lcd_inspection_id) if trailer3_hash else None
 963:     if trailer3_hash:
 964:         trailer3 = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, lcd_inspection_id)
 965:         hash_manager.update_lookup_type_if_unknown(trailer3_hash, 'trailer')
 966:         
 967:     department_hash: Optional[str] = response_data.get('whichDepartmentDoesTheLoadBelongTo')
 968:     department: Optional[str] = hash_manager.lookup_hash('department', department_hash, tip_value, lcd_inspection_id) if department_hash else None
 969:     if department_hash:
 970:         department = hash_manager.lookup_hash('department', department_hash, tip_value, lcd_inspection_id)
 971:         hash_manager.update_lookup_type_if_unknown(department_hash, 'department')
 972: 
 973:     team_hash: Optional[str] = response_data.get('team')
 974:     team: Optional[str] = hash_manager.lookup_hash('team', team_hash, tip_value, lcd_inspection_id) if team_hash else None
 975:     if team_hash:
 976:         team = hash_manager.lookup_hash('team', team_hash, tip_value, lcd_inspection_id)
 977:         hash_manager.update_lookup_type_if_unknown(team_hash, 'team')
 978: 
 979:     compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 980:     compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 981:     if compliant_yes:
 982:         load_compliance: str = 'COMPLIANT'
 983:     elif compliant_no:
 984:         load_compliance = 'NON-COMPLIANT'
 985:     else:
 986:         load_compliance = 'UNKNOWN'
 987: 
 988:     has_unknown: bool = any([
 989:         vehicle and vehicle.startswith('Unknown'),
 990:         trailer and trailer.startswith('Unknown'),
 991:         trailer2 and trailer2.startswith('Unknown'),
 992:         trailer3 and trailer3.startswith('Unknown'),
 993:         department and department.startswith('Unknown'),
 994:         team and team.startswith('Unknown')
 995:     ])
 996: 
 997:     api_meta_created: Optional[datetime] = None
 998:     api_meta_modified: Optional[datetime] = None
 999:     if meta.get('createdDate'):
1000:         try:
1001:             api_meta_created = datetime.fromisoformat(meta['createdDate'].replace('Z', '+00:00'))
1002:         except (ValueError, AttributeError):
1003:             pass
1004:     if meta.get('modifiedDate'):
1005:         try:
1006:             api_meta_modified = datetime.fromisoformat(meta['modifiedDate'].replace('Z', '+00:00'))
1007:         except (ValueError, AttributeError):
1008:             pass
1009: 
1010:     parent_array: Optional[List[str]] = meta.get('parent')
1011: 
1012:     db_manager.execute_update(
1013:         """
1014:         INSERT INTO noggin_data (
1015:             tip, object_type, inspection_date, lcd_inspection_id, coupling_id,
1016:             inspected_by, vehicle_hash, vehicle, vehicle_id,
1017:             trailer_hash, trailer, trailer_id,
1018:             trailer2_hash, trailer2, trailer2_id,
1019:             trailer3_hash, trailer3, trailer3_id,
1020:             job_number, run_number, driver_loader_name,
1021:             department_hash, department, team_hash, team,
1022:             load_compliance, processing_status, has_unknown_hashes,
1023:             total_attachments, csv_imported_at,
1024:             straps, no_of_straps, chains, mass,
1025:             api_meta_created_date, api_meta_modified_date,
1026:             api_meta_security, api_meta_type, api_meta_tip,
1027:             api_meta_sid, api_meta_branch, api_meta_parent,
1028:             api_meta_errors, api_meta_raw, api_payload_raw, raw_json
1029:         ) VALUES (
1030:             %s, %s, %s, %s, %s,
1031:             %s, %s, %s, %s,
1032:             %s, %s, %s,
1033:             %s, %s, %s,
1034:             %s, %s, %s,
1035:             %s, %s, %s,
1036:             %s, %s, %s, %s,
1037:             %s, %s, %s,
1038:             %s, %s,
1039:             %s, %s, %s, %s,
1040:             %s, %s,
1041:             %s, %s, %s,
1042:             %s, %s, %s,
1043:             %s, %s, %s, %s
1044:         )
1045:         ON CONFLICT (tip) DO UPDATE SET
1046:             object_type = EXCLUDED.object_type,
1047:             inspection_date = EXCLUDED.inspection_date,
1048:             lcd_inspection_id = EXCLUDED.lcd_inspection_id,
1049:             coupling_id = EXCLUDED.coupling_id,
1050:             inspected_by = EXCLUDED.inspected_by,
1051:             vehicle_hash = EXCLUDED.vehicle_hash,
1052:             vehicle = EXCLUDED.vehicle,
1053:             vehicle_id = EXCLUDED.vehicle_id,
1054:             trailer_hash = EXCLUDED.trailer_hash,
1055:             trailer = EXCLUDED.trailer,
1056:             trailer_id = EXCLUDED.trailer_id,
1057:             trailer2_hash = EXCLUDED.trailer2_hash,
1058:             trailer2 = EXCLUDED.trailer2,
1059:             trailer2_id = EXCLUDED.trailer2_id,
1060:             trailer3_hash = EXCLUDED.trailer3_hash,
1061:             trailer3 = EXCLUDED.trailer3,
1062:             trailer3_id = EXCLUDED.trailer3_id,
1063:             job_number = EXCLUDED.job_number,
1064:             run_number = EXCLUDED.run_number,
1065:             driver_loader_name = EXCLUDED.driver_loader_name,
1066:             department_hash = EXCLUDED.department_hash,
1067:             department = EXCLUDED.department,
1068:             team_hash = EXCLUDED.team_hash,
1069:             team = EXCLUDED.team,
1070:             load_compliance = EXCLUDED.load_compliance,
1071:             processing_status = EXCLUDED.processing_status,
1072:             has_unknown_hashes = EXCLUDED.has_unknown_hashes,
1073:             total_attachments = EXCLUDED.total_attachments,
1074:             straps = EXCLUDED.straps,
1075:             no_of_straps = EXCLUDED.no_of_straps,
1076:             chains = EXCLUDED.chains,
1077:             mass = EXCLUDED.mass,
1078:             api_meta_created_date = EXCLUDED.api_meta_created_date,
1079:             api_meta_modified_date = EXCLUDED.api_meta_modified_date,
1080:             api_meta_security = EXCLUDED.api_meta_security,
1081:             api_meta_type = EXCLUDED.api_meta_type,
1082:             api_meta_tip = EXCLUDED.api_meta_tip,
1083:             api_meta_sid = EXCLUDED.api_meta_sid,
1084:             api_meta_branch = EXCLUDED.api_meta_branch,
1085:             api_meta_parent = EXCLUDED.api_meta_parent,
1086:             api_meta_errors = EXCLUDED.api_meta_errors,
1087:             api_meta_raw = EXCLUDED.api_meta_raw,
1088:             api_payload_raw = EXCLUDED.api_payload_raw,
1089:             raw_json = EXCLUDED.raw_json,
1090:             updated_at = CURRENT_TIMESTAMP
1091:         """,
1092:         (
1093:             tip_value, object_type, inspection_date, lcd_inspection_id, coupling_id,
1094:             response_data.get('inspectedBy'), vehicle_hash, vehicle, response_data.get('vehicleId'),
1095:             trailer_hash, trailer, response_data.get('trailerId'),
1096:             trailer2_hash, trailer2, response_data.get('trailerId2'),
1097:             trailer3_hash, trailer3, response_data.get('trailerId3'),
1098:             response_data.get('jobNumber'), response_data.get('runNumber'), response_data.get('driverLoaderName'),
1099:             department_hash, department, team_hash, team,
1100:             load_compliance, 'api_success', has_unknown,
1101:             len(response_data.get('attachments', [])), None,
1102:             response_data.get('straps'), response_data.get('noOfStraps'), 
1103:             response_data.get('chains'), response_data.get('mass'),
1104:             api_meta_created, api_meta_modified,
1105:             meta.get('security'), meta.get('type'), meta.get('tip'),
1106:             meta.get('sid'), meta.get('branch'), parent_array,
1107:             json.dumps(meta.get('errors', [])), json.dumps(meta), json.dumps(response_data),
1108:             json.dumps(response_data)
1109:         )
1110:     )
1111: 
1112:     logger.debug(f"Inserted/updated noggin_data record for TIP {tip_value}")
1113: 
1114: def calculate_next_retry_time(retry_count: int) -> datetime:
1115:     """Calculate next retry time with exponential backoff"""
1116:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1117:     retry_backoff_multiplier: int = config.getint('retry', 'retry_backoff_multiplier')
1118: 
1119:     if retry_count >= max_retry_attempts:
1120:         return datetime.now() + timedelta(days=365)
1121: 
1122:     backoff_seconds: int = (retry_backoff_multiplier ** retry_count) * 60
1123:     max_backoff_seconds: int = 3600
1124:     backoff_seconds = min(backoff_seconds, max_backoff_seconds)
1125: 
1126:     return datetime.now() + timedelta(seconds=backoff_seconds)
1127: 
1128: 
1129: def get_tips_to_process_from_database(limit: int = 10) -> List[Dict[str, Any]]:
1130:     """
1131:     Query database for TIPs that need processing
1132:     Priority: failed  interrupted  partial  api_failed  pending
1133:     """
1134:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1135: 
1136:     query: str = """
1137:         SELECT tip, processing_status, retry_count, next_retry_at
1138:         FROM noggin_data
1139:         WHERE permanently_failed = FALSE
1140:           AND (
1141:               (processing_status IN ('failed', 'interrupted', 'partial', 'api_failed')
1142:                AND retry_count < %s
1143:                AND (next_retry_at IS NULL OR next_retry_at <= CURRENT_TIMESTAMP))
1144:               OR processing_status = 'pending'
1145:           )
1146:         ORDER BY
1147:             CASE processing_status
1148:                 WHEN 'failed' THEN 1
1149:                 WHEN 'interrupted' THEN 2
1150:                 WHEN 'partial' THEN 3
1151:                 WHEN 'api_failed' THEN 4
1152:                 WHEN 'pending' THEN 5
1153:                 ELSE 6
1154:             END,
1155:             csv_imported_at ASC
1156:         LIMIT %s
1157:     """
1158: 
1159:     tips: List[Dict[str, Any]] = db_manager.execute_query_dict(query, (max_retry_attempts, limit))
1160:     return tips
1161: 
1162: 
1163: def mark_permanently_failed(tip_value: str) -> None:
1164:     """Mark TIP as permanently failed after max retries"""
1165:     db_manager.execute_update(
1166:         """
1167:         UPDATE noggin_data
1168:         SET permanently_failed = TRUE,
1169:             processing_status = 'failed',
1170:             last_error_message = 'Max retry attempts exceeded'
1171:         WHERE tip = %s
1172:         """,
1173:         (tip_value,)
1174:     )
1175:     logger.warning(f"TIP {tip_value} marked as permanently failed")
1176: 
1177: def should_process_tip(tip_value: str) -> Tuple[bool, Optional[int]]:
1178:     """
1179:     Check if TIP should be processed based on database state
1180: 
1181:     Returns:
1182:         Tuple of (should_process, current_retry_count)
1183:     """
1184:     result: List[Dict[str, Any]] = db_manager.execute_query_dict(
1185:         """
1186:         SELECT processing_status, all_attachments_complete, retry_count,
1187:                permanently_failed, next_retry_at
1188:         FROM noggin_data
1189:         WHERE tip = %s
1190:         """,
1191:         (tip_value,)
1192:     )
1193: 
1194:     if not result:
1195:         logger.debug(f"TIP {tip_value} not in database - will process")
1196:         return True, 0
1197: 
1198:     record: Dict[str, Any] = result[0]
1199:     status: str = record['processing_status']
1200:     all_complete: bool = record['all_attachments_complete']
1201:     retry_count: int = record['retry_count'] or 0
1202:     permanently_failed: bool = record['permanently_failed']
1203:     next_retry_at: Optional[datetime] = record['next_retry_at']
1204: 
1205:     if permanently_failed:
1206:         logger.info(f"TIP {tip_value} permanently failed - skipping")
1207:         return False, retry_count
1208: 
1209:     if status == 'complete' and all_complete:
1210:         logger.info(f"TIP {tip_value} already completed successfully - skipping")
1211:         return False, retry_count
1212: 
1213:     if next_retry_at and datetime.now() < next_retry_at:
1214:         wait_seconds: float = (next_retry_at - datetime.now()).total_seconds()
1215:         logger.debug(f"TIP {tip_value} in backoff period - retry in {wait_seconds:.0f}s")
1216:         return False, retry_count
1217: 
1218:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1219:     if retry_count >= max_retry_attempts:
1220:         mark_permanently_failed(tip_value)
1221:         return False, retry_count
1222: 
1223:     logger.info(f"TIP {tip_value} needs processing (status: {status}, retry: {retry_count})")
1224:     return True, retry_count
1225: 
1226: 
1227: def get_total_tip_count(tip_csv_file_path: Path) -> int:
1228:     """Count valid TIPs in CSV for progress tracking"""
1229:     try:
1230:         import csv
1231: 
1232:         with open(tip_csv_file_path, 'r', newline='', encoding='utf-8') as file:
1233:             tip_csv_reader = csv.reader(file)
1234:             header: List[str] = next(tip_csv_reader)
1235: 
1236:             header = [col.strip().lower() for col in header]
1237:             tip_column_index: int = header.index('tip')
1238: 
1239:             valid_tip_count: int = 0
1240:             for row in tip_csv_reader:
1241:                 if row and len(row) > tip_column_index and row[tip_column_index].strip():
1242:                     valid_tip_count += 1
1243: 
1244:             return valid_tip_count
1245: 
1246:     except Exception as e:
1247:         logger.warning(f"Could not count TIPs: {e}")
1248:         return 0
1249: 
1250: 
1251: def update_progress_tracking(processed_count: int, total_count: int, start_time_val: float) -> None:
1252:     """Display progress updates with time estimates"""
1253:     if processed_count == 0:
1254:         return
1255: 
1256:     elapsed_time: float = time.perf_counter() - start_time_val
1257:     tips_per_second: float = processed_count / elapsed_time
1258:     remaining_tips: int = total_count - processed_count
1259: 
1260:     if tips_per_second > 0:
1261:         estimated_remaining_seconds: float = remaining_tips / tips_per_second
1262: 
1263:         if estimated_remaining_seconds >= 3600:
1264:             time_estimate: str = f"{estimated_remaining_seconds/3600:.1f} hours"
1265:         elif estimated_remaining_seconds >= 60:
1266:             time_estimate = f"{estimated_remaining_seconds/60:.1f} minutes"
1267:         else:
1268:             time_estimate = f"{estimated_remaining_seconds:.1f} seconds"
1269: 
1270:         progress_percentage: float = (processed_count / total_count) * 100
1271: 
1272:         logger.info(f"Progress: {processed_count}/{total_count} ({progress_percentage:.1f}%) - "
1273:                    f"Rate: {tips_per_second:.2f} TIPs/sec - ETA: {time_estimate}")
1274: 
1275: 
1276: def log_shutdown_summary(processed_count: int, total_count: int, start_time_val: float, reason: str = "manual") -> None:
1277:     """Log comprehensive shutdown summary"""
1278:     elapsed_time: float = time.perf_counter() - start_time_val
1279:     completion_percentage: float = (processed_count / total_count) * 100 if total_count > 0 else 0
1280: 
1281:     logger.info("="*80)
1282:     logger.info("SHUTDOWN SUMMARY")
1283:     logger.info("="*80)
1284:     logger.info(f"Shutdown reason: {reason}")
1285:     logger.info(f"TIPs processed:  {processed_count:,} of {total_count:,} ({completion_percentage:.1f}%)")
1286:     logger.info(f"Processing time: {elapsed_time/3600:.1f} hours")
1287:     if processed_count > 0:
1288:         logger.info(f"Average rate: {processed_count/elapsed_time:.2f} TIPs/second")
1289:         remaining_estimate: float = (total_count - processed_count) * (elapsed_time / processed_count)
1290:         logger.info(f"Estimated time for remaining: {remaining_estimate/3600:.1f} hours")
1291:     logger.info("All work saved to PostgreSQL database")
1292:     logger.info("="*80)
1293: 
1294: 
1295: def main() -> int:
1296:     """Main processing function"""
1297:     global current_tip_being_processed
1298: 
1299:     tip_csv_file_path: Path = Path('tip.csv')
1300: 
1301:     if not tip_csv_file_path.exists():
1302:         logger.error(f"TIP CSV file not found: {tip_csv_file_path}")
1303:         return 1
1304: 
1305:     total_tip_count: int = get_total_tip_count(tip_csv_file_path)
1306:     logger.info(f"Found {total_tip_count} valid TIPs to process")
1307: 
1308:     processed_count: int = 0
1309:     main_start_time: float = time.perf_counter()
1310: 
1311:     logger.info(f"Opening TIP CSV file: {tip_csv_file_path}")
1312: 
1313:     import csv
1314: 
1315:     with open(tip_csv_file_path, 'r', newline='', encoding='utf-8') as file:
1316:         tip_csv_reader = csv.reader(file)
1317:         header: List[str] = next(tip_csv_reader)
1318: 
1319:         header = [col.strip().lower() for col in header]
1320:         logger.info(f"CSV headers: {header}")
1321: 
1322:         try:
1323:             tip_column_index: int = header.index('tip')
1324:             logger.info(f"TIP column found at index {tip_column_index}")
1325:         except ValueError:
1326:             logger.error(f"CSV must contain 'tip' column. Found: {header}")
1327:             return 1
1328: 
1329:         for row_num, row in enumerate(tip_csv_reader, start=2):
1330:             if not shutdown_handler.should_continue_processing():
1331:                 logger.warning(f"Graceful shutdown after processing {processed_count} TIPs")
1332:                 break
1333: 
1334:             if not row or all(not cell.strip() for cell in row):
1335:                 continue
1336: 
1337:             if len(row) <= tip_column_index:
1338:                 logger.warning(f"Row {row_num}: insufficient columns")
1339:                 continue
1340: 
1341:             tip_value: str = row[tip_column_index].strip()
1342:             if not tip_value:
1343:                 continue
1344: 
1345:             current_tip_being_processed = tip_value
1346: 
1347:             should_process, current_retry_count = should_process_tip(tip_value)
1348:             if not should_process:
1349:                 continue
1350: 
1351:             processed_count += 1
1352: 
1353:             if processed_count % 10 == 0:
1354:                 update_progress_tracking(processed_count, total_tip_count, main_start_time)
1355: 
1356:             try:
1357:                 circuit_breaker.before_request()
1358:             except CircuitBreakerError as e:
1359:                 logger.warning(f"Circuit breaker blocked request for TIP {tip_value}: {e}")
1360:                 time.sleep(10)
1361:                 continue
1362: 
1363:             endpoint: str = endpoint_template.replace('$tip', tip_value)
1364:             url: str = base_url + endpoint
1365:             logger.info(f"Processing TIP {processed_count}/{total_tip_count}: {tip_value} (retry: {current_retry_count})")
1366:             logger.debug(f"Request URL: {url}")
1367: 
1368:             try:
1369:                 api_start_time: float = time.perf_counter()
1370:                 response: requests.Response = requests.get(url, headers=headers, timeout=api_timeout)
1371:                 circuit_breaker.record_success()
1372:                 api_retry_count: int = 0
1373: 
1374:                 if response.status_code == 429:
1375:                     circuit_breaker.record_failure()
1376:                     logger.warning(f"Rate limited for TIP {tip_value}. Sleeping {too_many_requests_sleep_time}s")
1377:                     time.sleep(too_many_requests_sleep_time)
1378:                     try:
1379:                         circuit_breaker.before_request()
1380:                         response = requests.get(url, headers=headers, timeout=api_timeout)
1381:                         circuit_breaker.record_success()
1382:                         api_retry_count = 1
1383:                     except CircuitBreakerError as cb_error:
1384:                         logger.warning(f"Circuit breaker blocked retry: {cb_error}")
1385:                         
1386:                         new_retry_count: int = current_retry_count + 1
1387:                         next_retry: datetime = calculate_next_retry_time(new_retry_count)
1388:                         
1389:                         db_manager.execute_update(
1390:                             """
1391:                             INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1392:                                                     retry_count, last_retry_at, next_retry_at)
1393:                             VALUES (%s, %s, %s, %s, %s, %s, %s)
1394:                             ON CONFLICT (tip) DO UPDATE SET
1395:                                 processing_status = EXCLUDED.processing_status,
1396:                                 last_error_message = EXCLUDED.last_error_message,
1397:                                 retry_count = EXCLUDED.retry_count,
1398:                                 last_retry_at = EXCLUDED.last_retry_at,
1399:                                 next_retry_at = EXCLUDED.next_retry_at
1400:                             """,
1401:                             (tip_value, object_type, 'api_failed', str(cb_error),
1402:                              new_retry_count, datetime.now(), next_retry)
1403:                         )
1404:                         continue
1405:                     except requests.exceptions.RequestException as retry_error:
1406:                         circuit_breaker.record_failure()
1407:                         logger.error(f"Retry failed for TIP {tip_value}: {retry_error}", exc_info=True)
1408: 
1409:                         new_retry_count = current_retry_count + 1
1410:                         next_retry = calculate_next_retry_time(new_retry_count)
1411: 
1412:                         db_manager.execute_update(
1413:                             """
1414:                             INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1415:                                                     retry_count, last_retry_at, next_retry_at)
1416:                             VALUES (%s, %s, %s, %s, %s, %s, %s)
1417:                             ON CONFLICT (tip) DO UPDATE SET
1418:                                 processing_status = EXCLUDED.processing_status,
1419:                                 last_error_message = EXCLUDED.last_error_message,
1420:                                 retry_count = EXCLUDED.retry_count,
1421:                                 last_retry_at = EXCLUDED.last_retry_at,
1422:                                 next_retry_at = EXCLUDED.next_retry_at
1423:                             """,
1424:                             (tip_value, object_type, 'api_failed', str(retry_error),
1425:                              new_retry_count, datetime.now(), next_retry)
1426:                         )
1427: 
1428:                         db_manager.execute_update(
1429:                             """
1430:                             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1431:                             VALUES (%s, %s, %s, %s)
1432:                             """,
1433:                             (tip_value, 'api_failed', str(retry_error), json.dumps({'url': url}))
1434:                         )
1435:                         continue
1436: 
1437:                 if response.status_code == 200:
1438:                     logger.info(f"Successful API response for TIP {tip_value}")
1439:                     response_data: Dict[str, Any] = response.json()
1440: 
1441:                     insert_noggin_data_record(tip_value, response_data)
1442: 
1443:                     lcd_inspection_id: str = response_data.get('lcdInspectionId', 'unknown')
1444: 
1445:                     process_attachments(response_data, lcd_inspection_id, tip_value)
1446: 
1447:                 else:
1448:                     circuit_breaker.record_failure()
1449:                     error_details: str = handle_api_error(response, tip_value, url)
1450:                     logger.error(error_details)
1451:                     session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\tAPI_ERROR_{response.status_code}\t0\tERROR")
1452: 
1453:                     new_retry_count = current_retry_count + 1
1454:                     next_retry = calculate_next_retry_time(new_retry_count)
1455: 
1456:                     db_manager.execute_update(
1457:                         """
1458:                         INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1459:                                                 retry_count, last_retry_at, next_retry_at)
1460:                         VALUES (%s, %s, %s, %s, %s, %s, %s)
1461:                         ON CONFLICT (tip) DO UPDATE SET
1462:                             processing_status = EXCLUDED.processing_status,
1463:                             last_error_message = EXCLUDED.last_error_message,
1464:                             retry_count = EXCLUDED.retry_count,
1465:                             last_retry_at = EXCLUDED.last_retry_at,
1466:                             next_retry_at = EXCLUDED.next_retry_at
1467:                         """,
1468:                         (tip_value, object_type, 'api_failed', error_details,
1469:                          new_retry_count, datetime.now(), next_retry)
1470:                     )
1471: 
1472:                     db_manager.execute_update(
1473:                         """
1474:                         INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1475:                         VALUES (%s, %s, %s, %s)
1476:                         """,
1477:                         (tip_value, 'api_failed', error_details, json.dumps({
1478:                             'http_status': response.status_code,
1479:                             'url': url
1480:                         }))
1481:                     )
1482: 
1483:             except requests.exceptions.ConnectionError as connection_error:
1484:                 circuit_breaker.record_failure()
1485:                 logger.error(f"Connection error for TIP {tip_value}: {connection_error}", exc_info=True)
1486: 
1487:                 new_retry_count = current_retry_count + 1
1488:                 next_retry = calculate_next_retry_time(new_retry_count)
1489: 
1490:                 db_manager.execute_update(
1491:                     """
1492:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1493:                                             retry_count, last_retry_at, next_retry_at)
1494:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1495:                     ON CONFLICT (tip) DO UPDATE SET
1496:                         processing_status = EXCLUDED.processing_status,
1497:                         last_error_message = EXCLUDED.last_error_message,
1498:                         retry_count = EXCLUDED.retry_count,
1499:                         last_retry_at = EXCLUDED.last_retry_at,
1500:                         next_retry_at = EXCLUDED.next_retry_at
1501:                     """,
1502:                     (tip_value, object_type, 'api_failed', str(connection_error),
1503:                      new_retry_count, datetime.now(), next_retry)
1504:                 )
1505:                 continue
1506: 
1507:             except requests.exceptions.RequestException as request_error:
1508:                 circuit_breaker.record_failure()
1509:                 logger.error(f"Request error for TIP {tip_value}: {request_error}", exc_info=True)
1510: 
1511:                 new_retry_count = current_retry_count + 1
1512:                 next_retry = calculate_next_retry_time(new_retry_count)
1513: 
1514:                 db_manager.execute_update(
1515:                     """
1516:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1517:                                             retry_count, last_retry_at, next_retry_at)
1518:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1519:                     ON CONFLICT (tip) DO UPDATE SET
1520:                         processing_status = EXCLUDED.processing_status,
1521:                         last_error_message = EXCLUDED.last_error_message,
1522:                         retry_count = EXCLUDED.retry_count,
1523:                         last_retry_at = EXCLUDED.last_retry_at,
1524:                         next_retry_at = EXCLUDED.next_retry_at
1525:                     """,
1526:                     (tip_value, object_type, 'api_failed', str(request_error),
1527:                      new_retry_count, datetime.now(), next_retry)
1528:                 )
1529:                 continue
1530: 
1531:             except Exception as e:
1532:                 circuit_breaker.record_failure()
1533:                 logger.error(f"Unexpected error processing TIP {tip_value}: {e}", exc_info=True)
1534: 
1535:                 new_retry_count = current_retry_count + 1
1536:                 next_retry = calculate_next_retry_time(new_retry_count)
1537: 
1538:                 db_manager.execute_update(
1539:                     """
1540:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1541:                                             retry_count, last_retry_at, next_retry_at)
1542:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1543:                     ON CONFLICT (tip) DO UPDATE SET
1544:                         processing_status = EXCLUDED.processing_status,
1545:                         last_error_message = EXCLUDED.last_error_message,
1546:                         retry_count = EXCLUDED.retry_count,
1547:                         last_retry_at = EXCLUDED.last_retry_at,
1548:                         next_retry_at = EXCLUDED.next_retry_at
1549:                     """,
1550:                     (tip_value, object_type, 'failed', str(e),
1551:                      new_retry_count, datetime.now(), next_retry)
1552:                 )
1553:                 continue
1554: 
1555:         current_tip_being_processed = None
1556: 
1557:     return processed_count
1558: 
1559: 
1560: if __name__ == "__main__":
1561:     try:
1562:         logger.info("Starting main processing loop")
1563:         processed_count: int = main()
1564: 
1565:         logger.info(f"Processing completed. Total TIPs processed: {processed_count}")
1566:         logger.info("Script completed successfully")
1567: 
1568:     except KeyboardInterrupt:
1569:         logger.warning("Processing interrupted by user")
1570:         log_shutdown_summary(0, 0, start_time, "keyboard_interrupt")
1571: 
1572:     except Exception as e:
1573:         logger.error(f"Unexpected error: {e}", exc_info=True)
1574:         log_shutdown_summary(0, 0, start_time, "error")
1575:         raise
1576: 
1577:     finally:
1578:         if 'db_manager' in locals():
1579:             db_manager.close_all()
1580: 
1581:         end_time: float = time.perf_counter()
1582:         total_duration: float = end_time - start_time
1583:         logger.info(f"Total execution time: {total_duration/3600:.2f} hours ({total_duration:.2f} seconds)")
1584:         logger.info(f"Session ID: {batch_session_id}")
1585:         logger.info("="*80)
1586: 
1587:         session_logger.info(f"\nSESSION END: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
1588:         session_logger.info(f"TOTAL EXECUTION TIME: {total_duration:.2f} seconds")
</file>

<file path="config/base_config.ini">
 1: [postgresql]
 2: host = localhost
 3: port = 5432
 4: database = noggin_db
 5: user = noggin_app
 6: password = GoodKingCoat16
 7: schema = noggin_schema
 8: pool_min_connections = 2
 9: pool_max_connections = 10
10: 
11: [paths]
12: #/mnt/data --> 1TB volume
13: base_log_path = /mnt/data/noggin/log
14: base_output_path = /mnt/data/noggin/out
15: error_folder_path = /mnt/data/noggin/etl/in/error
16: input_folder_path = /mnt/data/noggin/etl/in/pending
17: processed_folder_path = /mnt/data/noggin/etl/in/processed
18: 
19: # Hash lookup sync paths (script auto-detects these from its location; can be overridden here)
20: # hash_sync_pending_path = /home/noggin_admin/scripts/etl/hash_sync/pending
21: # hash_sync_processed_path = /home/noggin_admin/scripts/etl/hash_sync/processed
22: # hash_sync_error_path = /home/noggin_admin/scripts/etl/hash_sync/error
23: 
24: [api]
25: base_url = https://services.apse2.elasticnoggin.com
26: media_service_url = https://services.apse2.elasticnoggin.com/mediaservice
27: namespace = 6649a25a06337e51a87b77a7d83e58f522795452456649b8e019574837f8674
28: 
29: ; PJG KEY
30: bearer_token = ZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKSVV6STFOaUo5LmV5SnpaWE56YVc5dVZHOXJaVzRpT2lKallqVm1NemRrWkRkak1XVXhPV1ZrTldNeU1EWmhZbVptTnprMVlXUmxaVEprTUdNM09ERmhOVFE0WkRCaFlXTTFNR1V6TXpRMlpqRXpaamd6TjJFMUlpd2ljMlZ6YzJsdmJrbGtJam9pTjJRek5tVTVNak16TUdZME5tSTFZVEF6TURkbU5HRXpZVFl3T0dZek5UQTJaVEZtWXpFNFpEUmpNR1ZtTldRME5UQXdZVGhqTVdRMk5EZzFaak5qTVNJc0ltNWhiV1Z6Y0dGalpTSTZJalkyTkRsaE1qVmhNRFl6TXpkbE5URmhPRGRpTnpkaE4yUTRNMlUxT0dZMU1qSTNPVFUwTlRJME5UWTJORGxpT0dVd01UazFOelE0TXpkbU9EWTNOQ0lzSW1WNGNDSTZNak01T1RNNE5qVTFNeXdpWTNWemRHOXRVR0Y1Ykc5aFpDSTZleUoxYzJWeVZHbHdJam9pWlRGa05XRm1OakJpWkdJM01UQmxaRFkyWVRaaE1HTXpPREF4WTJKa05ERXhabU5pTURJNE9UQmlNR1kwTjJJMk9HUXhORGd3WW1ZM1pHWTFabVE1T0NKOWZRLmlnX1hBX2JZLS1pTE9NeWJ3cjRQQ3l1YUhDVFVrS2pwV3E4aUlPTE1iMDg=
31: 
32: ; nvm API KEY
33: ; bearer_token = ZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKSVV6STFOaUo5LmV5SnpaWE56YVc5dVZHOXJaVzRpT2lJd09Ea3dPV1kwWVRrMFltTmtPR1kzWlRrek0yVTNNell4TVRFMk5qRTVOekpsT0RRMU5qTm1OekJtWlRBeVptUmlOR0V6TURjM01XWm1OemszWWpCa0lpd2ljMlZ6YzJsdmJrbGtJam9pTldVeE5tWmhNakJrTXpRM1lqazNNR1F5TnpFMU1HTTBOelE0TnpSaE1ERXpOekkzT1dZM01UQmxaV001T0dWaU1XRmxNamN6Wm1VeE1tWXdaR1l3TkNJc0ltNWhiV1Z6Y0dGalpTSTZJalkyTkRsaE1qVmhNRFl6TXpkbE5URmhPRGRpTnpkaE4yUTRNMlUxT0dZMU1qSTNPVFUwTlRJME5UWTJORGxpT0dVd01UazFOelE0TXpkbU9EWTNOQ0lzSW1WNGNDSTZNak00TkRVMk9Ua3pPQ3dpWTNWemRHOXRVR0Y1Ykc5aFpDSTZleUoxYzJWeVZHbHdJam9pWlRGa05XRm1OakJpWkdJM01UQmxaRFkyWVRaaE1HTXpPREF4WTJKa05ERXhabU5pTURJNE9UQmlNR1kwTjJJMk9HUXhORGd3WW1ZM1pHWTFabVE1T0NKOWZRLm5Jb05FOXd5d21mQ01rT1dpQ0FDRlVWSUFrUFhBMzg1TjlhNEtGWnVfMHc=
34: 
35: 
36: [processing]
37: too_many_requests_sleep_time = 60
38: attachment_pause = 2
39: max_api_retries = 5
40: api_backoff_factor = 2
41: api_max_backoff = 60
42: api_timeout = 30
43: 
44: [input]
45: csv_check_interval_seconds = 86400
46: # amend to exported-file-*.csv
47: csv_filename_pattern = *.csv
48: import_batch_size = 100
49: 
50: [retry]
51: max_retry_attempts = 3
52: retry_backoff_multiplier = 2
53: idle_sleep_seconds = 60
54: active_sleep_seconds = 5
55: tips_per_batch = 10
56: 
57: [circuit_breaker]
58: failure_threshold_percent = 50
59: recovery_threshold_percent = 10
60: circuit_open_duration_seconds = 300
61: sample_size = 10
62: 
63: [logging]
64: log_level = INFO
65: extended_debug_mode = false
66: log_retention_days = 30
67: compress_old_logs = true
68: log_filename_pattern = {script_name}_{date}.log
69: 
70: [continuous]
71: ; default 300, 3, 10, 6
72: cycle_sleep_seconds = 300
73: import_csv_every_n_cycles = 3
74: resolve_hashes_every_n_cycles = 10
75: sftp_download_every_n_cycles = 6
76: 
77: [sftp]
78: enabled = true
79: host = ssh.noggin-sftp.goldstartransport.com.au
80: port = 18765
81: username = u824-zdigcggtoza6
82: private_key_path = /home/noggin_admin/scripts/.ssh/noggin/noggin-openssh.pem
83: host_key_fingerprint = ssh-ed25519 255 Tc8ZNyPlk1EGa6u/DPp7UsJR1lhaw4rxb8IP1IWOCVM
84: remote_path = /home/customer/sftp
85: # exported-file-*.csv
86: file_pattern = *.csv
87: config_file = config/sftp_config.ini
88: 
89: [csv_import]
90: # default batch size = 100. higher values more efficient, but more memory
91: batch_size = 100
92: config_dir = config
</file>

</files>
