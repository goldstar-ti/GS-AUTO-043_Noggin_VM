This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: noggin_processor.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
noggin_processor.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="noggin_processor.py">
   1: """ noggin_processor.py
   2:     Module purpose
   3:     ---------------
   4:     This module processes "Load Compliance Check" (LCD) inspection records from a CSV
   5:     of TIP identifiers, retrieves their JSON payloads from a remote API, stores
   6:     metadata in PostgreSQL, downloads and validates attachment media, and writes
   7:     human-readable inspection reports and attachment files to disk. It includes
   8:     robust retry/backoff logic, a circuit breaker to protect the API, and graceful
   9:     shutdown handling.
  10:     High-level behaviour
  11:     --------------------
  12:     - Reads TIPs from a CSV file (expected column header: "tip").
  13:     - For each TIP:
  14:         - Uses an endpoint template ($tip) to build the API URL and GETs the JSON.
  15:         - Inserts/updates a noggin_data row with parsed response fields and meta.
  16:         - Creates a dated folder structure and writes a formatted text payload file.
  17:         - Downloads listed attachments, validates file integrity, computes MD5 and
  18:         records attachment state in the attachments table.
  19:         - Tracks errors in processing_errors and updates retry/backoff state on errors.
  20:     - Honors graceful shutdown (SIGINT/SIGTERM) finishing the current TIP when
  21:     possible; a second signal forces immediate exit.
  22:     - Emits logging to both application logger and a session logger file.
  23:     Primary public functions/classes
  24:     -------------------------------
  25:     - GracefulShutdownHandler(db_conn, logger_instance)
  26:         Handles SIGINT/SIGTERM, closes DB connections on exit, and provides a
  27:         should_continue_processing() method to let main stop gracefully.
  28:     - sanitise_filename(text)
  29:         Sanitises a string for safe use in filenames (removes or replaces unsafe
  30:         characters).
  31:     - flatten_json(nested_json, parent_key='', sep='_')
  32:         Flattens arbitrarily nested JSON (dicts/lists) into a single-level dict with
  33:         concatenated keys suitable for CSV or tabular storage.
  34:     - create_inspection_folder_structure(date_str, lcd_inspection_id)
  35:         Builds and creates a hierarchical path base_path/YYYY/MM/YYYY-MM-DD <id>
  36:         for storing inspection payloads/attachments; falls back to base_path/unknown_date.
  37:     - construct_attachment_filename(lcd_inspection_id, date_str, attachment_num)
  38:         Returns a standardized filename for attachments including a sanitized inspection
  39:         id, date (YYYYMMDD or "unknown") and zero-padded sequence number.
  40:     - calculate_md5_hash(file_path)
  41:         Computes and returns the MD5 hex digest for the specified file path.
  42:         Returns empty string on error.
  43:     - validate_attachment_file(file_path, expected_min_size=1024)
  44:         Performs basic integrity checks on a downloaded file: existence, minimum
  45:         size threshold, and a small header read to detect emptiness. Returns
  46:         (is_valid, file_size_bytes, error_message).
  47:     - save_formatted_payload_text_file(inspection_folder, response_data, lcd_inspection_id)
  48:         Writes a human-readable inspection report (and optionally the full JSON
  49:         payload) to a text file inside inspection_folder. Uses hash_manager to
  50:         resolve hashed fields to human-readable names where available. Returns the
  51:         saved file Path or None on I/O error.
  52:     - make_api_request(url, headers, tip_value, max_retries=5, backoff_factor=2,
  53:                     timeout=30, max_backoff=60)
  54:         Performs a GET with exponential backoff and retries on transient
  55:         connection/timeout errors. Attaches a private _retry_count attribute to the
  56:         returned Response for bookkeeping.
  57:     - handle_api_error(response, tip_value, request_url)
  58:         Produces a detailed, human-friendly error string for logging and DB storage
  59:         based on HTTP response code and body.
  60:     - download_attachment(attachment_url, filename, lcd_inspection_id, attachment_tip,
  61:                         inspection_folder, record_tip, attachment_sequence)
  62:         Downloads a single attachment to disk (temporary .tmp file -> final rename),
  63:         validates it, computes MD5, updates / inserts attachment row(s) and any
  64:         related processing_errors. Returns a tuple:
  65:         (success: bool, retry_count: int, file_size_mb: float, error_msg: Optional[str]).
  66:     - process_attachments(response_data, lcd_inspection_id, tip_value)
  67:         Orchestrates saving the payload file, creating the folder, iteratively
  68:         downloading and validating attachments, updating noggin_data status fields
  69:         (complete/partial/failed/interrupted) and logging a per-session record.
  70:     - insert_noggin_data_record(tip_value, response_data)
  71:         Parses response_data into typed fields (inspection_date, hashes, names,
  72:         load_compliance, $meta etc.) and INSERTs or UPDATEs the noggin_data table.
  73:         Also records the raw API payload and meta JSON for traceability.
  74:     - calculate_next_retry_time(retry_count)
  75:         Computes an exponential backoff next_retry_at datetime based on configured
  76:         retry settings. Returns a datetime far in the future when retry_count
  77:         exceeds configured max.
  78:     - get_tips_to_process_from_database(limit=10)
  79:         Convenience DB query that returns TIPs considered eligible for processing
  80:         according to retry/backoff rules and processing_status priorities.
  81:     - mark_permanently_failed(tip_value)
  82:         Marks a TIP as permanently failed after max retries are exhausted.
  83:     - should_process_tip(tip_value)
  84:         Inspect the noggin_data record (if present) and determines whether the TIP
  85:         should be processed now. Returns (should_process: bool, current_retry_count: Optional[int]).
  86:     - get_total_tip_count(tip_csv_file_path)
  87:         Counts valid (non-empty) TIPs in the CSV file for progress estimates.
  88:     - update_progress_tracking(processed_count, total_count, start_time_val)
  89:         Logs a progress update with TIPs/sec and ETA.
  90:     - log_shutdown_summary(processed_count, total_count, start_time_val, reason="manual")
  91:         Logs an overall shutdown summary including elapsed time and estimated
  92:         remaining duration.
  93:     - main()
  94:         The main processing loop: reads tip.csv, iterates rows, coordinates
  95:         circuit breaker interactions, API calling, DB updates, attachment processing,
  96:         retries/backoff and graceful shutdown. Returns number of TIPs processed.
  97:     Configuration and dependencies
  98:     ------------------------------
  99:     - Requires a ConfigLoader instance populated from:
 100:         'config/base_config.ini' and 'config/load_compliance_check_config.ini'
 101:     Expected config sections/keys used in this module (examples):
 102:         - [api]: base_url, media_service_url
 103:         - [processing]: too_many_requests_sleep_time, attachment_pause, max_api_retries,
 104:                         api_backoff_factor, api_max_backoff, api_timeout
 105:         - [output]: show_json_payload_in_text_file, show_compliance_status,
 106:                     filename_image_stub, unknown_response_output_text
 107:         - [paths]: base_output_path
 108:         - [retry]: max_retry_attempts, retry_backoff_multiplier
 109:         - object_type mapping via config.get_object_type_config()
 110:     - Relies on the following helper classes from common:
 111:         ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager,
 112:         CircuitBreaker, CircuitBreakerError
 113:     - External libraries used:
 114:         requests, psycopg2 (indirectly via DatabaseConnectionManager), standard
 115:         library modules (json, logging, pathlib, datetime, uuid, time, signal, sys,
 116:         atexit, hashlib, typing, csv).
 117:     Database schema expectations
 118:     ---------------------------
 119:     This module expects the following tables and important columns (non-exhaustive):
 120:     - noggin_data
 121:         - tip (primary key)
 122:         - object_type, inspection_date, lcd_inspection_id, coupling_id
 123:         - inspected_by, vehicle_hash, vehicle, vehicle_id, trailer_hash, trailer, trailer_id, ...
 124:         - load_compliance, processing_status, last_error_message
 125:         - retry_count, next_retry_at, last_retry_at, csv_imported_at
 126:         - total_attachments, completed_attachment_count, all_attachments_complete
 127:         - api_meta_created_date, api_meta_modified_date, api_meta_* fields
 128:         - api_payload_raw, api_meta_raw, updated_at, permanently_failed, has_unknown_hashes
 129:     - attachments
 130:         - record_tip, attachment_tip (unique constraint)
 131:         - attachment_sequence, filename, file_path
 132:         - attachment_status ('downloading', 'complete', 'failed', ...)
 133:         - attachment_validation_status ('not_validated', 'valid', 'validation_failed', ...)
 134:         - file_size_bytes, file_hash_md5
 135:         - download_started_at, download_completed_at, download_duration_seconds
 136:         - validation_error_message, last_error_message
 137:     - processing_errors
 138:         - tip, error_type, error_message, error_details (JSON/text), created_at
 139:     Side effects and filesystem behavior
 140:     -----------------------------------
 141:     - Writes per-inspection directories under configured base_output_path with
 142:     subfolders year/month and a folder named "<YYYY-MM-DD> <lcd_inspection_id>".
 143:     - Writes a human-readable inspection data text file (and optionally full JSON).
 144:     - Downloads attachments into the inspection folder; temporary files use .tmp
 145:     suffix until validation and rename succeed.
 146:     - Uses session-specific logging via LoggerManager to write a session log with a
 147:     header and per-record lines.
 148:     Important operational notes
 149:     ---------------------------
 150:     - Circuit breaker: before making API calls circuit_breaker.before_request() is
 151:     called; failures are recorded via circuit_breaker.record_failure() and a
 152:     successful call invokes circuit_breaker.record_success(). The circuit breaker
 153:     may prevent retries to avoid cascading failures.
 154:     - Retries/backoff: API-level transient errors are retried with exponential
 155:     backoff. Permanent HTTP errors (4xx/5xx) are stored in DB and the TIP is
 156:     scheduled for future retry based on calculate_next_retry_time().
 157:     - Graceful shutdown: the first SIGINT/SIGTERM will set an internal flag so the
 158:     script finishes the current TIP and stops taking new TIPs. A second signal
 159:     forces immediate cleanup and exit.
 160:     - Concurrency: the module is written for single-process execution. If multiple
 161:     workers/processes operate on the same DB, the schema must handle upserts and
 162:     conflicts appropriately (the code uses ON CONFLICT for key operations).
 163:     - Validation thresholds: validate_attachment_file() uses a default minimum file
 164:     size (1024 bytes). Adjust this threshold via the function parameters if the
 165:     media service produces smaller-but-valid files.
 166:     Exit codes
 167:     ----------
 168:     - main() returns 1 when fatal startup errors occur (missing CSV or CSV format).
 169:     - When executed as __main__, the script logs status and exits normally (0) on
 170:     success; unexpected exceptions are re-raised after logging.
 171:     Example usage
 172:     -------------
 173:     - Run from the system environment where config files and tip.csv are available:
 174:         python noggin_processor.py
 175:     - Ensure config files contain correct API endpoints, DB connection details (used
 176:     by DatabaseConnectionManager), and output path permissions.
 177:     Extensibility suggestions
 178:     -------------------------
 179:     - Abstract retry/backoff configuration to be injected for easier unit testing.
 180:     - Replace direct requests.get calls with a wrapper interface to facilitate
 181:     mocking in tests.
 182:     - Add more granular attachment type detection (content-type / magic bytes)
 183:     beyond basic size/header checks if required.
 184:     Security and privacy
 185:     --------------------
 186:     - Ensure access tokens or API credentials used by headers are kept secure and not
 187:     logged. This module attempts to avoid logging full payloads unless explicitly
 188:     enabled by configuration (show_json_payload_in_text_file).
 189:     - Attachment storage may contain sensitive images; secure file permissions and
 190:     accessible output path accordingly.
 191:     Limitations
 192:     -----------
 193:     - The module assumes stable database connectivity and that the DatabaseConnectionManager
 194:     implements execute_query_dict and execute_update semantics shown. Errors closing
 195:     DB connections are logged but not fatal during shutdown cleanup.
 196:     - Date parsing uses datetime.fromisoformat after normalizing 'Z' to '+00:00';
 197:     non-ISO date formats may be treated as unknown.
 198: """
 199: from __future__ import annotations
 200: import requests
 201: import json
 202: import logging
 203: import uuid
 204: from datetime import datetime
 205: from pathlib import Path
 206: import time
 207: import signal
 208: import sys
 209: import atexit
 210: import hashlib
 211: from typing import Optional, List, Dict, Any, Tuple
 212: 
 213: from common import ConfigLoader, LoggerManager, DatabaseConnectionManager, HashManager, CircuitBreaker, CircuitBreakerError, UNKNOWN_TEXT
 214: 
 215: start_time: float = time.perf_counter()
 216: 
 217: config: ConfigLoader = ConfigLoader(
 218:     'config/base_config.ini',
 219:     'config/load_compliance_check_config.ini'
 220: )
 221: 
 222: batch_session_id: str = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_batch_{str(uuid.uuid4())[:8].upper()}"
 223: 
 224: logger_manager: LoggerManager = LoggerManager(config, script_name=Path(__file__).stem)
 225: logger_manager.configure_application_logger()
 226: session_logger: logging.Logger = logger_manager.create_session_logger(batch_session_id)
 227: 
 228: logger: logging.Logger = logging.getLogger(__name__)
 229: 
 230: db_manager: DatabaseConnectionManager = DatabaseConnectionManager(config)
 231: hash_manager: HashManager = HashManager(config, db_manager)
 232: circuit_breaker: CircuitBreaker = CircuitBreaker(config)
 233: 
 234: base_url: str = config.get('api', 'base_url')
 235: attachment_base_url: str = config.get('api', 'media_service_url')
 236: headers: Dict[str, str] = config.get_api_headers()
 237: 
 238: base_path: Path = Path(config.get('paths', 'base_output_path'))
 239: base_path.mkdir(parents=True, exist_ok=True)
 240: 
 241: too_many_requests_sleep_time: int = config.getint('processing', 'too_many_requests_sleep_time')
 242: attachment_pause: int = config.getint('processing', 'attachment_pause')
 243: max_api_retries: int = config.getint('processing', 'max_api_retries')
 244: api_backoff_factor: int = config.getint('processing', 'api_backoff_factor')
 245: api_max_backoff: int = config.getint('processing', 'api_max_backoff')
 246: api_timeout: int = config.getint('processing', 'api_timeout')
 247: 
 248: show_json_payload_in_text_file: bool = config.getboolean('output', 'show_json_payload_in_text_file', from_specific=True)
 249: show_compliance_status: bool = config.getboolean('output', 'show_compliance_status', from_specific=True)
 250: filename_image_stub: str = config.get('output', 'filename_image_stub', from_specific=True)
 251: unknown_response_output_text: str = config.get('output', 'unknown_response_output_text', from_specific=True)
 252: 
 253: object_type_config: Dict[str, str] = config.get_object_type_config()
 254: endpoint_template: str = object_type_config['endpoint']
 255: object_type: str = object_type_config['object_type']
 256: 
 257: shutdown_requested: bool = False
 258: current_tip_being_processed: Optional[str] = None
 259: 
 260: logger.info("="*80)
 261: logger.info(f"NOGGIN PROCESSOR - LOAD COMPLIANCE CHECK (DRIVER/LOADER)")
 262: logger.info("="*80)
 263: logger.info(f"Session ID:       {batch_session_id}")
 264: logger.info(f"Object Type:      {object_type}")
 265: logger.info(f"Base Output Path: {base_path}")
 266: logger.info("="*80)
 267: 
 268: session_logger.info(f"SESSION START: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
 269: session_logger.info(f"SESSION ID: {batch_session_id}")
 270: session_logger.info(f"OBJECT TYPE: {object_type}")
 271: session_logger.info("")
 272: session_logger.info("TIMESTAMP\tTIP\tLCD_INSPECTION_ID\tATTACHMENTS_COUNT\tATTACHMENT_FILENAMES")
 273: 
 274: class GracefulShutdownHandler:
 275:     """Handles Ctrl+C and system shutdown signals"""
 276: 
 277:     def __init__(self, db_conn: DatabaseConnectionManager, logger_instance: logging.Logger) -> None:
 278:         self.db_conn: DatabaseConnectionManager = db_conn
 279:         self.logger: logging.Logger = logger_instance
 280:         self.shutdown_requested: bool = False
 281: 
 282:         signal.signal(signal.SIGINT, self._signal_handler)
 283:         signal.signal(signal.SIGTERM, self._signal_handler)
 284:         atexit.register(self._cleanup_on_exit)
 285: 
 286:         self.logger.info("Graceful shutdown handler initialised")
 287: 
 288:     def _signal_handler(self, signum: int, frame: Any) -> None:
 289:         global shutdown_requested
 290:         signal_name: str = "SIGINT (Ctrl+C)" if signum == signal.SIGINT else f"Signal {signum}"
 291: 
 292:         if not self.shutdown_requested:
 293:             self.shutdown_requested = TrueZ
 294:             shutdown_requested = True
 295:             self.logger.warning(f"\n{signal_name} received. Finishing current TIP then shutting down...")
 296:             self.logger.warning(f"Currently processing: {current_tip_being_processed or 'None'}")
 297:             self.logger.warning("Press Ctrl+C again to force immediate exit")
 298:         else:
 299:             self.logger.error("Second shutdown signal - forcing immediate exit")
 300:             self._emergency_cleanup()
 301:             sys.exit(1)
 302: 
 303:     def _cleanup_on_exit(self) -> None:
 304:         if self.db_conn:
 305:             try:
 306:                 self.db_conn.close_all()
 307:             except Exception as e:
 308:                 self.logger.error(f"Error during exit cleanup: {e}")
 309: 
 310:     def _emergency_cleanup(self) -> None:
 311:         try:
 312:             if self.db_conn:
 313:                 self.db_conn.close_all()
 314:         except:
 315:             pass
 316: 
 317:     def should_continue_processing(self) -> bool:
 318:         return not self.shutdown_requested
 319: 
 320: shutdown_handler: GracefulShutdownHandler = GracefulShutdownHandler(db_manager, logger)
 321: 
 322: def sanitise_filename(text: Optional[str]) -> str:
 323:     """Sanitise text for use in filenames"""
 324:     if not text:
 325:         return "unknown"
 326:     return (text.replace(" - ", "-").replace(" ", "_").replace("/", "")
 327:             .replace("\\", "").replace("*", "").replace("<", "")
 328:             .replace(">", "").replace("?", "").replace("|", "").replace(":", ""))
 329: 
 330: def flatten_json(nested_json: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
 331:     """Flatten nested JSON structure"""
 332:     items: List[Tuple[str, Any]] = []
 333:     for key, value in nested_json.items():
 334:         new_key: str = f"{parent_key}{sep}{key}" if parent_key else key
 335: 
 336:         if isinstance(value, dict):
 337:             items.extend(flatten_json(value, new_key, sep=sep).items())
 338:         elif isinstance(value, list):
 339:             for i, item in enumerate(value):
 340:                 if isinstance(item, dict):
 341:                     items.extend(flatten_json(item, f"{new_key}_{i}", sep=sep).items())
 342:                 else:
 343:                     items.append((f"{new_key}_{i}", item))
 344:         else:
 345:             items.append((new_key, value))
 346:     return dict(items)
 347: 
 348: def create_inspection_folder_structure(date_str: str, lcd_inspection_id: str) -> Path:
 349:     """Create hierarchical folder structure for inspection.
 350: 
 351:     Creates a folder structure based on the date and LCD inspection ID in the format:
 352:     base_path/year/month/YYYY-MM-DD inspection_id
 353: 
 354:     If the date cannot be parsed, creates a folder under base_path/unknown_date/
 355: 
 356:     Args:
 357:         date_str (str): Date string in ISO format (e.g. "2023-05-20" or "2023-05-20Z")
 358:         lcd_inspection_id (str): Unique identifier for the LCD inspection
 359: 
 360:     Returns:
 361:         Path: Path object pointing to the created inspection folder
 362: 
 363:     Raises:
 364:         None: Exceptions are caught and handled internally with fallback folder creation
 365: 
 366:     Example:
 367:         >>> create_inspection_folder_structure("2023-05-20", "LCD123")
 368:         PosixPath('/base_path/2023/05/2023-05-20 LCD123')
 369:     """
 370:     try:
 371:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 372:         year: str = str(date_obj.year)
 373:         month: str = f"{date_obj.month:02d}"
 374:         formatted_date: str = date_obj.strftime('%Y-%m-%d')
 375: 
 376:         folder_name: str = f"{formatted_date} {lcd_inspection_id}"
 377:         inspection_folder: Path = base_path / year / month / folder_name
 378:         inspection_folder.mkdir(parents=True, exist_ok=True)
 379:         logger.debug(f"Created inspection folder: {inspection_folder}")
 380:         return inspection_folder
 381:     except (ValueError, AttributeError) as e:
 382:         logger.warning(f"Could not parse date '{date_str}': {e}")
 383:         folder_name = f"unknown-date {lcd_inspection_id}"
 384:         fallback_folder: Path = base_path / "unknown_date" / folder_name
 385:         fallback_folder.mkdir(parents=True, exist_ok=True)
 386:         logger.info(f"Created fallback folder: {fallback_folder}")
 387:         return fallback_folder
 388: 
 389: def construct_attachment_filename(lcd_inspection_id: str, date_str: str, attachment_num: int) -> str:
 390:     """Construct standardised attachment filename"""
 391:     sanitised_lcd: str = sanitise_filename(lcd_inspection_id)
 392: 
 393:     try:
 394:         date_obj: datetime = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
 395:         date_part: str = date_obj.strftime('%Y%m%d')
 396:     except (ValueError, AttributeError):
 397:         logger.warning(f"Could not parse date '{date_str}', using 'unknown'")
 398:         date_part = "unknown"
 399: 
 400:     attachment_num_str: str = f"{attachment_num:03d}"
 401:     filename: str = f"{sanitised_lcd}_{date_part}_{filename_image_stub}_{attachment_num_str}.jpg"
 402:     return filename
 403: 
 404: def calculate_md5_hash(file_path: Path) -> str:
 405:     """Calculate MD5 hash of file"""
 406:     md5_hash: hashlib._Hash = hashlib.md5()
 407:     try:
 408:         with open(file_path, 'rb') as f:
 409:             for chunk in iter(lambda: f.read(8192), b""):
 410:                 md5_hash.update(chunk)
 411:         return md5_hash.hexdigest()
 412:     except Exception as e:
 413:         logger.warning(f"Could not calculate MD5 for {file_path}: {e}")
 414:         return ""
 415: 
 416: def validate_attachment_file(file_path: Path, expected_min_size: int = 1024) -> Tuple[bool, int, Optional[str]]:
 417:     """Validate downloaded file integrity"""
 418:     try:
 419:         if not file_path.exists():
 420:             return False, 0, "File does not exist"
 421: 
 422:         file_size: int = file_path.stat().st_size
 423: 
 424:         if file_size < expected_min_size:
 425:             return False, file_size, f"File too small ({file_size} bytes)"
 426: 
 427:         with open(file_path, 'rb') as f:
 428:             header_bytes: bytes = f.read(10)
 429:             if len(header_bytes) == 0:
 430:                 return False, file_size, "File appears empty"
 431: 
 432:         return True, file_size, None
 433: 
 434:     except Exception as e:
 435:         return False, 0, f"Validation error: {e}"
 436: 
 437: def save_formatted_payload_text_file(inspection_folder: Path, response_data: Dict[str, Any],
 438:                                     lcd_inspection_id: str) -> Optional[Path]:
 439:     """Generate formatted text file with inspection data"""
 440:     sanitised_lcd_id: str = sanitise_filename(lcd_inspection_id)
 441:     payload_filename: str = f"{sanitised_lcd_id}_inspection_data.txt"
 442:     payload_path: Path = inspection_folder / payload_filename
 443: 
 444:     try:
 445:         with open(payload_path, 'w', encoding='utf-8') as f:
 446:             f.write("="*60 + "\n")
 447:             f.write("LOAD COMPLIANCE CHECK INSPECTION REPORT\n")
 448:             f.write(f"RECORD GENERATED: {datetime.now().strftime('%d-%m-%Y')}\n")
 449:             f.write("="*60 + "\n\n")
 450: 
 451:             f.write(f"LCD Inspection ID:     {response_data.get('lcdInspectionId', unknown_response_output_text)}\n\n")
 452:             f.write(f"Date:                  {response_data.get('date', unknown_response_output_text)}\n\n")
 453:             f.write(f"Inspected By:          {response_data.get('inspectedBy', unknown_response_output_text)}\n\n")
 454: 
 455:             vehicle_hash: str = response_data.get('vehicle', '')
 456:             vehicle_name: str = hash_manager.lookup_hash('vehicle', vehicle_hash, response_data.get('tip', ''), lcd_inspection_id) if vehicle_hash else unknown_response_output_text
 457:             f.write(f"Vehicle:               {vehicle_name}\n\n")
 458:             f.write(f"Vehicle ID:            {response_data.get('vehicleId', unknown_response_output_text)}\n\n")
 459: 
 460:             trailer_hash: str = response_data.get('trailer', '')
 461:             trailer_name: str = hash_manager.lookup_hash('trailer', trailer_hash, response_data.get('tip', ''), lcd_inspection_id) if trailer_hash else unknown_response_output_text
 462:             f.write(f"Trailer:               {trailer_name}\n\n")
 463:             f.write(f"Trailer ID:            {response_data.get('trailerId', unknown_response_output_text)}\n\n")
 464: 
 465:             trailer2_hash: str = response_data.get('trailer2', '')
 466:             if trailer2_hash:
 467:                 trailer2_name: str = hash_manager.lookup_hash('trailer', trailer2_hash, response_data.get('tip', ''), lcd_inspection_id)
 468:                 f.write(f"Trailer 2:             {trailer2_name}\n\n")
 469:                 f.write(f"Trailer 2 ID:          {response_data.get('trailerId2', unknown_response_output_text)}\n\n")
 470: 
 471:             trailer3_hash: str = response_data.get('trailer3', '')
 472:             if trailer3_hash:
 473:                 trailer3_name: str = hash_manager.lookup_hash('trailer', trailer3_hash, response_data.get('tip', ''), lcd_inspection_id)
 474:                 f.write(f"Trailer 3:             {trailer3_name}\n\n")
 475:                 f.write(f"Trailer 3 ID:          {response_data.get('trailerId3', unknown_response_output_text)}\n\n")
 476: 
 477:             f.write(f"Job Number:            {response_data.get('jobNumber', unknown_response_output_text)}\n\n")
 478:             f.write(f"Run Number:            {response_data.get('runNumber', unknown_response_output_text)}\n\n")
 479:             f.write(f"Driver/Loader Name:    {response_data.get('driverLoaderName', unknown_response_output_text)}\n\n")
 480: 
 481:             dept_hash: str = response_data.get('whichDepartmentDoesTheLoadBelongTo', '')
 482:             dept_name: str = hash_manager.lookup_hash('department', dept_hash, response_data.get('tip', ''), lcd_inspection_id) if dept_hash else unknown_response_output_text
 483:             f.write(f"Department:            {dept_name}\n\n")
 484: 
 485:             team_hash: str = response_data.get('team', '')
 486:             team_name: str = hash_manager.lookup_hash('team', team_hash, response_data.get('tip', ''), lcd_inspection_id) if team_hash else unknown_response_output_text
 487:             f.write(f"Team:                  {team_name}\n\n")
 488: 
 489:             if show_compliance_status:
 490:                 compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 491:                 compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 492:                 if compliant_yes:
 493:                     f.write("Load Compliance:       COMPLIANT\n\n")
 494:                 elif compliant_no:
 495:                     f.write("Load Compliance:       NON-COMPLIANT\n\n")
 496:                 else:
 497:                     f.write(f"Load Compliance:       {unknown_response_output_text}\n\n")
 498: 
 499:             straps_value = response_data.get('straps')
 500:             if straps_value is not None:
 501:                 f.write(f"Straps:                {straps_value}\n\n")
 502:                 no_of_straps = response_data.get('noOfStraps', UNKNOWN_TEXT)
 503:                 f.write(f"Number of Straps:      {no_of_straps}\n\n")
 504: 
 505:             chains_value = response_data.get('chains')
 506:             if chains_value is not None:
 507:                 f.write(f"Chains:                {chains_value}\n\n")
 508: 
 509:             mass_value = response_data.get('mass', UNKNOWN_TEXT)
 510:             f.write(f"Mass:                  {mass_value}\n\n")
 511: 
 512:             attachment_count: int = len(response_data.get('attachments', []))
 513:             f.write(f"Attachments:           {attachment_count}\n\n")
 514: 
 515:             if show_json_payload_in_text_file:
 516:                 f.write("-"*60 + "\n")
 517:                 f.write("COMPLETE TECHNICAL DATA (JSON FORMAT)\n")
 518:                 f.write("-"*60 + "\n\n")
 519:                 json.dump(response_data, f, indent=2, ensure_ascii=False)
 520: 
 521:         logger.info(f"Saved formatted payload to: {payload_path}")
 522:         return payload_path
 523: 
 524:     except IOError as e:
 525:         logger.error(f"IOError saving payload {payload_path}: {e}", exc_info=True)
 526:         return None
 527: 
 528: def make_api_request(url: str, headers: Dict[str, str], tip_value: str,
 529:                     max_retries: int = 5, backoff_factor: int = 2,
 530:                     timeout: int = 30, max_backoff: int = 60) -> requests.Response:
 531:     """Make API request with exponential backoff retry logic"""
 532:     last_exception: Optional[Exception] = None
 533: 
 534:     for attempt in range(max_retries):
 535:         try:
 536:             logger.debug(f"API request attempt {attempt + 1}/{max_retries} for TIP {tip_value}")
 537:             response: requests.Response = requests.get(url, headers=headers, timeout=timeout)
 538:             response._retry_count = attempt
 539:             logger.debug(f"Request attempt {attempt + 1} succeeded for TIP {tip_value}")
 540:             return response
 541: 
 542:         except requests.exceptions.ConnectionError as connection_error:
 543:             last_exception = connection_error
 544: 
 545:             if attempt == max_retries - 1:
 546:                 logger.error(f"All {max_retries} connection attempts failed for TIP {tip_value}", exc_info=True)
 547:                 raise connection_error
 548: 
 549:             wait_time: float = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 550:             logger.warning(f"Connection failed for TIP {tip_value}, retrying in {wait_time}s... "
 551:                           f"(attempt {attempt + 1}/{max_retries})")
 552:             time.sleep(wait_time)
 553: 
 554:         except requests.exceptions.Timeout as timeout_error:
 555:             last_exception = timeout_error
 556: 
 557:             if attempt == max_retries - 1:
 558:                 logger.error(f"All {max_retries} timeout attempts failed for TIP {tip_value}", exc_info=True)
 559:                 raise timeout_error
 560: 
 561:             wait_time = min((backoff_factor ** attempt) * backoff_factor, max_backoff)
 562:             logger.warning(f"Request timeout for TIP {tip_value}, retrying in {wait_time}s... "
 563:                           f"(attempt {attempt + 1}/{max_retries})")
 564:             time.sleep(wait_time)
 565: 
 566:         except requests.exceptions.RequestException as request_error:
 567:             last_exception = request_error
 568: 
 569:             if attempt == max_retries - 1:
 570:                 logger.error(f"Request failed permanently for TIP {tip_value}: {str(request_error)}", exc_info=True)
 571:                 raise request_error
 572: 
 573:             wait_time = backoff_factor
 574:             logger.warning(f"Request error for TIP {tip_value}, retrying in {wait_time}s... "
 575:                           f"(attempt {attempt + 1}/{max_retries})")
 576:             time.sleep(wait_time)
 577: 
 578:     if last_exception:
 579:         raise last_exception
 580:     else:
 581:         raise Exception(f"Unexpected error in retry logic for TIP {tip_value}")
 582: 
 583: def handle_api_error(response: requests.Response, tip_value: str, request_url: str) -> str:
 584:     """Generate detailed error message from API response"""
 585:     status_code: int = response.status_code
 586: 
 587:     try:
 588:         response_text: str = response.text
 589:         if response_text:
 590:             try:
 591:                 error_json: Dict[str, Any] = response.json()
 592:                 additional_info: str = f" Response body: {json.dumps(error_json, indent=2)}"
 593:             except json.JSONDecodeError:
 594:                 additional_info = f" Response body: {response_text[:500]}{'...' if len(response_text) > 500 else ''}"
 595:         else:
 596:             additional_info = " (No response body provided)"
 597:     except Exception:
 598:         additional_info = " (Could not read response body)"
 599: 
 600:     if status_code == 401:
 601:         error_message: str = (f"Authentication failed for TIP {tip_value}. "
 602:                              f"Status code: {status_code} (Unauthorised). "
 603:                              f"The access token is missing or invalid. "
 604:                              f"URL: {request_url}{additional_info}")
 605: 
 606:     elif status_code == 403:
 607:         error_message = (f"Access forbidden for TIP {tip_value}. "
 608:                         f"Status code: {status_code} (Forbidden). "
 609:                         f"You don't have permission to access this resource. "
 610:                         f"URL: {request_url}{additional_info}")
 611: 
 612:     elif status_code == 404:
 613:         error_message = (f"Resource not found for TIP {tip_value}. "
 614:                         f"Status code: {status_code} (Not Found). "
 615:                         f"The requested object does not exist. "
 616:                         f"URL: {request_url}{additional_info}")
 617: 
 618:     elif status_code == 429:
 619:         error_message = (f"Rate limit exceeded for TIP {tip_value}. "
 620:                         f"Status code: {status_code} (Too Many Requests). "
 621:                         f"URL: {request_url}{additional_info}")
 622: 
 623:     elif 400 <= status_code < 500:
 624:         error_message = (f"Client error for TIP {tip_value}. "
 625:                         f"Status code: {status_code}. "
 626:                         f"URL: {request_url}{additional_info}")
 627: 
 628:     elif 500 <= status_code < 600:
 629:         error_message = (f"Server error for TIP {tip_value}. "
 630:                         f"Status code: {status_code}. "
 631:                         f"URL: {request_url}{additional_info}")
 632: 
 633:     else:
 634:         error_message = (f"Unexpected response for TIP {tip_value}. "
 635:                         f"Status code: {status_code}. "
 636:                         f"URL: {request_url}{additional_info}")
 637: 
 638:     return error_message
 639: 
 640: def download_attachment(attachment_url: str, filename: str, lcd_inspection_id: str,
 641:                        attachment_tip: str, inspection_folder: Path,
 642:                        record_tip: str, attachment_sequence: int) -> Tuple[bool, int, float, Optional[str]]:
 643:     """Download and validate attachment with database tracking"""
 644:     if attachment_url.startswith('/media'):
 645:         attachment_url = attachment_url[6:]
 646: 
 647:     full_url: str = attachment_base_url + attachment_url
 648:     output_path: Path = inspection_folder / filename
 649:     temp_path: Path = output_path.with_suffix('.tmp')
 650: 
 651:     existing_attachment: List[Dict[str, Any]] = db_manager.execute_query_dict(
 652:         "SELECT attachment_status, file_size_bytes FROM attachments WHERE record_tip = %s AND attachment_tip = %s",
 653:         (record_tip, attachment_tip)
 654:     )
 655: 
 656:     if existing_attachment and existing_attachment[0]['attachment_status'] == 'complete':
 657:         if output_path.exists():
 658:             is_valid, file_size, error_msg = validate_attachment_file(output_path)
 659:             if is_valid:
 660:                 file_size_mb: float = file_size / (1024 * 1024)
 661:                 logger.info(f"Skipping existing valid attachment: {filename} ({file_size_mb:.2f} MB)")
 662:                 return True, 0, file_size_mb, None
 663: 
 664:     download_start_time: float = time.perf_counter()
 665:     logger.info(f"Downloading {lcd_inspection_id}: {filename}")
 666: 
 667:     db_manager.execute_update(
 668:         """
 669:         INSERT INTO attachments (
 670:             record_tip, attachment_tip, attachment_sequence, filename, file_path,
 671:             attachment_status, attachment_validation_status, download_started_at
 672:         ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
 673:         ON CONFLICT (record_tip, attachment_tip)
 674:         DO UPDATE SET
 675:             attachment_status = EXCLUDED.attachment_status,
 676:             download_started_at = EXCLUDED.download_started_at
 677:         """,
 678:         (record_tip, attachment_tip, attachment_sequence, filename, str(output_path.resolve()),
 679:          'downloading', 'not_validated', datetime.now())
 680:     )
 681: 
 682:     try:
 683:         response: requests.Response = make_api_request(full_url, headers, f"attachment {filename}", timeout=60)
 684:         retry_count: int = getattr(response, '_retry_count', 0)
 685: 
 686:         if response.status_code == 200:
 687:             with open(temp_path, 'wb') as f:
 688:                 f.write(response.content)
 689: 
 690:             is_valid, file_size, validation_error = validate_attachment_file(temp_path)
 691: 
 692:             if is_valid:
 693:                 temp_path.rename(output_path)
 694: 
 695:                 file_hash: str = calculate_md5_hash(output_path)
 696:                 download_duration: float = time.perf_counter() - download_start_time
 697:                 file_size_mb = file_size / (1024 * 1024)
 698: 
 699:                 db_manager.execute_update(
 700:                     """
 701:                     UPDATE attachments
 702:                     SET attachment_status = %s,
 703:                         attachment_validation_status = %s,
 704:                         file_size_bytes = %s,
 705:                         file_hash_md5 = %s,
 706:                         download_completed_at = %s,
 707:                         download_duration_seconds = %s
 708:                     WHERE record_tip = %s AND attachment_tip = %s
 709:                     """,
 710:                     ('complete', 'valid', file_size, file_hash, datetime.now(),
 711:                      round(download_duration, 2), record_tip, attachment_tip)
 712:                 )
 713: 
 714:                 logger.info(f"Downloaded: {filename} ({file_size_mb:.2f} MB) in {download_duration:.2f}s")
 715:                 return True, retry_count, file_size_mb, None
 716:             else:
 717:                 if temp_path.exists():
 718:                     temp_path.unlink()
 719: 
 720:                 error_msg: str = f"Validation failed: {validation_error}"
 721:                 logger.error(f"Download validation failed: {error_msg}")
 722: 
 723:                 db_manager.execute_update(
 724:                     """
 725:                     UPDATE attachments
 726:                     SET attachment_status = %s,
 727:                         attachment_validation_status = %s,
 728:                         validation_error_message = %s,
 729:                         last_error_message = %s
 730:                     WHERE record_tip = %s AND attachment_tip = %s
 731:                     """,
 732:                     ('failed', 'validation_failed', validation_error, error_msg, record_tip, attachment_tip)
 733:                 )
 734: 
 735:                 db_manager.execute_update(
 736:                     """
 737:                     INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 738:                     VALUES (%s, %s, %s, %s)
 739:                     """,
 740:                     (record_tip, 'attachment_failed', error_msg, json.dumps({
 741:                         'filename': filename,
 742:                         'attachment_tip': attachment_tip,
 743:                         'validation_error': validation_error
 744:                     }))
 745:                 )
 746: 
 747:                 return False, retry_count, 0, error_msg
 748:         else:
 749:             if temp_path.exists():
 750:                 temp_path.unlink()
 751: 
 752:             error_msg = f"HTTP {response.status_code}"
 753:             download_duration = time.perf_counter() - download_start_time
 754:             error_details: str = handle_api_error(response, f"attachment {filename}", full_url)
 755:             logger.error(f"Download failed: {error_details}")
 756: 
 757:             db_manager.execute_update(
 758:                 """
 759:                 UPDATE attachments
 760:                 SET attachment_status = %s,
 761:                     last_error_message = %s
 762:                 WHERE record_tip = %s AND attachment_tip = %s
 763:                 """,
 764:                 ('failed', error_msg, record_tip, attachment_tip)
 765:             )
 766: 
 767:             db_manager.execute_update(
 768:                 """
 769:                 INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 770:                 VALUES (%s, %s, %s, %s)
 771:                 """,
 772:                 (record_tip, 'attachment_failed', error_msg, json.dumps({
 773:                     'filename': filename,
 774:                     'attachment_tip': attachment_tip,
 775:                     'http_status': response.status_code,
 776:                     'url': full_url
 777:                 }))
 778:             )
 779: 
 780:             return False, retry_count, 0, error_msg
 781: 
 782:     except Exception as e:
 783:         if temp_path.exists():
 784:             temp_path.unlink()
 785: 
 786:         error_msg = f"Exception: {str(e)}"
 787:         download_duration = time.perf_counter() - download_start_time
 788:         logger.error(f"Download exception: {filename} - {error_msg}", exc_info=True)
 789: 
 790:         db_manager.execute_update(
 791:             """
 792:             UPDATE attachments
 793:             SET attachment_status = %s,
 794:                 last_error_message = %s
 795:             WHERE record_tip = %s AND attachment_tip = %s
 796:             """,
 797:             ('failed', error_msg, record_tip, attachment_tip)
 798:         )
 799: 
 800:         db_manager.execute_update(
 801:             """
 802:             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
 803:             VALUES (%s, %s, %s, %s)
 804:             """,
 805:             (record_tip, 'attachment_failed', error_msg, json.dumps({
 806:                 'filename': filename,
 807:                 'attachment_tip': attachment_tip,
 808:                 'exception': str(e)
 809:             }))
 810:         )
 811: 
 812:         return False, 0, 0, error_msg
 813: 
 814: def process_attachments(response_data: Dict[str, Any], lcd_inspection_id: str, tip_value: str) -> None:
 815:     """Process all attachments for an inspection with database tracking"""
 816:     global shutdown_requested
 817: 
 818:     if shutdown_requested:
 819:         logger.warning(f"Shutdown requested during {lcd_inspection_id}")
 820:         db_manager.execute_update(
 821:             "UPDATE noggin_data SET processing_status = %s WHERE tip = %s",
 822:             ('interrupted', tip_value)
 823:         )
 824:         return
 825: 
 826:     processing_start_time: float = time.perf_counter()
 827: 
 828:     date_str: str = response_data.get('date', '')
 829:     inspection_folder: Path = create_inspection_folder_structure(date_str, lcd_inspection_id)
 830:     save_formatted_payload_text_file(inspection_folder, response_data, lcd_inspection_id)
 831: 
 832:     if 'attachments' not in response_data or not response_data['attachments']:
 833:         processing_end_time: float = time.perf_counter()
 834:         processing_duration: float = processing_end_time - processing_start_time
 835: 
 836:         logger.info(f"No attachments found for {lcd_inspection_id}")
 837:         session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{lcd_inspection_id}\t0\tNONE")
 838: 
 839:         db_manager.execute_update(
 840:             """
 841:             UPDATE noggin_data
 842:             SET processing_status = %s,
 843:                 total_attachments = 0,
 844:                 completed_attachment_count = 0,
 845:                 all_attachments_complete = TRUE
 846:             WHERE tip = %s
 847:             """,
 848:             ('complete', tip_value)
 849:         )
 850:         return
 851: 
 852:     attachments: List[str] = response_data['attachments']
 853:     logger.info(f"Processing {len(attachments)} attachments for {lcd_inspection_id}")
 854: 
 855:     db_manager.execute_update(
 856:         "UPDATE noggin_data SET total_attachments = %s WHERE tip = %s",
 857:         (len(attachments), tip_value)
 858:     )
 859: 
 860:     successful_downloads: int = 0
 861:     attachment_filenames: List[str] = []
 862:     total_attachment_retries: int = 0
 863:     total_file_size_mb: float = 0.0
 864: 
 865:     for i, attachment_url in enumerate(attachments, 1):
 866:         if shutdown_requested:
 867:             logger.warning(f"Shutdown during attachment {i}/{len(attachments)} for {lcd_inspection_id}")
 868:             break
 869: 
 870:         attachment_tip: str = attachment_url.split('tip=')[-1] if 'tip=' in attachment_url else 'unknown'
 871:         filename: str = construct_attachment_filename(lcd_inspection_id, date_str, i)
 872: 
 873:         success, retry_count, file_size_mb, error_msg = download_attachment(
 874:             attachment_url, filename, lcd_inspection_id, attachment_tip,
 875:             inspection_folder, tip_value, i
 876:         )
 877: 
 878:         total_attachment_retries += retry_count
 879: 
 880:         if success:
 881:             successful_downloads += 1
 882:             attachment_filenames.append(filename)
 883:             total_file_size_mb += file_size_mb
 884: 
 885:         if attachment_pause > 0 and i < len(attachments):
 886:             logger.debug(f"Pausing {attachment_pause}s before next attachment")
 887:             time.sleep(attachment_pause)
 888: 
 889:     processing_end_time = time.perf_counter()
 890:     processing_duration = processing_end_time - processing_start_time
 891: 
 892:     if shutdown_requested:
 893:         final_status: str = 'interrupted'
 894:     elif successful_downloads == len(attachments):
 895:         final_status = 'complete'
 896:     elif successful_downloads > 0:
 897:         final_status = 'partial'
 898:     else:
 899:         final_status = 'failed'
 900: 
 901:     db_manager.execute_update(
 902:         """
 903:         UPDATE noggin_data
 904:         SET processing_status = %s
 905:         WHERE tip = %s
 906:         """,
 907:         (final_status, tip_value)
 908:     )
 909: 
 910:     logger.info(f"Inspection complete for {lcd_inspection_id}: {successful_downloads}/{len(attachments)} attachments")
 911: 
 912:     attachment_names_str: str = ";".join(attachment_filenames) if attachment_filenames else "FAILED"
 913:     session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\t{lcd_inspection_id}\t{successful_downloads}\t{attachment_names_str}")
 914: 
 915: def insert_noggin_data_record(tip_value: str, response_data: Dict[str, Any]) -> None:
 916:     """Insert or update noggin_data record with API response"""
 917:     meta: Dict[str, Any] = response_data.get('$meta', {})
 918: 
 919:     lcd_inspection_id: Optional[str] = response_data.get('lcdInspectionId')
 920:     coupling_id: Optional[str] = response_data.get('couplingId')
 921: 
 922:     inspection_date_str: Optional[str] = response_data.get('date')
 923:     inspection_date: Optional[datetime] = None
 924:     if inspection_date_str:
 925:         try:
 926:             inspection_date = datetime.fromisoformat(inspection_date_str.replace('Z', '+00:00'))
 927:         except (ValueError, AttributeError):
 928:             logger.warning(f"Could not parse date: {inspection_date_str}")
 929: 
 930:     vehicle_hash: Optional[str] = response_data.get('vehicle')
 931:     vehicle: Optional[str] = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, lcd_inspection_id) if vehicle_hash else None
 932:     if vehicle_hash:
 933:         vehicle = hash_manager.lookup_hash('vehicle', vehicle_hash, tip_value, lcd_inspection_id)
 934:         hash_manager.update_lookup_type_if_unknown(vehicle_hash, 'vehicle')
 935: 
 936:     trailer_hash: Optional[str] = response_data.get('trailer')
 937:     trailer: Optional[str] = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, lcd_inspection_id) if trailer_hash else None
 938:     if trailer_hash:
 939:         trailer = hash_manager.lookup_hash('trailer', trailer_hash, tip_value, lcd_inspection_id)
 940:         hash_manager.update_lookup_type_if_unknown(trailer_hash, 'trailer')
 941: 
 942:     trailer2_hash: Optional[str] = response_data.get('trailer2')
 943:     trailer2: Optional[str] = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, lcd_inspection_id) if trailer2_hash else None
 944:     if trailer2_hash:
 945:         trailer2 = hash_manager.lookup_hash('trailer', trailer2_hash, tip_value, lcd_inspection_id)
 946:         hash_manager.update_lookup_type_if_unknown(trailer2_hash, 'trailer')
 947:         
 948:     trailer3_hash: Optional[str] = response_data.get('trailer3')
 949:     trailer3: Optional[str] = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, lcd_inspection_id) if trailer3_hash else None
 950:     if trailer3_hash:
 951:         trailer3 = hash_manager.lookup_hash('trailer', trailer3_hash, tip_value, lcd_inspection_id)
 952:         hash_manager.update_lookup_type_if_unknown(trailer3_hash, 'trailer')
 953:         
 954:     department_hash: Optional[str] = response_data.get('whichDepartmentDoesTheLoadBelongTo')
 955:     department: Optional[str] = hash_manager.lookup_hash('department', department_hash, tip_value, lcd_inspection_id) if department_hash else None
 956:     if department_hash:
 957:         department = hash_manager.lookup_hash('department', department_hash, tip_value, lcd_inspection_id)
 958:         hash_manager.update_lookup_type_if_unknown(department_hash, 'department')
 959: 
 960:     team_hash: Optional[str] = response_data.get('team')
 961:     team: Optional[str] = hash_manager.lookup_hash('team', team_hash, tip_value, lcd_inspection_id) if team_hash else None
 962:     if team_hash:
 963:         team = hash_manager.lookup_hash('team', team_hash, tip_value, lcd_inspection_id)
 964:         hash_manager.update_lookup_type_if_unknown(team_hash, 'team')
 965: 
 966:     compliant_yes: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004Ye', False)
 967:     compliant_no: bool = response_data.get('isYourLoadCompliantWithTheLoadRestraintGuide2004No', False)
 968:     if compliant_yes:
 969:         load_compliance: str = 'COMPLIANT'
 970:     elif compliant_no:
 971:         load_compliance = 'NON-COMPLIANT'
 972:     else:
 973:         load_compliance = 'UNKNOWN'
 974: 
 975:     has_unknown: bool = any([
 976:         vehicle and vehicle.startswith('Unknown'),
 977:         trailer and trailer.startswith('Unknown'),
 978:         trailer2 and trailer2.startswith('Unknown'),
 979:         trailer3 and trailer3.startswith('Unknown'),
 980:         department and department.startswith('Unknown'),
 981:         team and team.startswith('Unknown')
 982:     ])
 983: 
 984:     api_meta_created: Optional[datetime] = None
 985:     api_meta_modified: Optional[datetime] = None
 986:     if meta.get('createdDate'):
 987:         try:
 988:             api_meta_created = datetime.fromisoformat(meta['createdDate'].replace('Z', '+00:00'))
 989:         except (ValueError, AttributeError):
 990:             pass
 991:     if meta.get('modifiedDate'):
 992:         try:
 993:             api_meta_modified = datetime.fromisoformat(meta['modifiedDate'].replace('Z', '+00:00'))
 994:         except (ValueError, AttributeError):
 995:             pass
 996: 
 997:     parent_array: Optional[List[str]] = meta.get('parent')
 998: 
 999:     db_manager.execute_update(
1000:         """
1001:         INSERT INTO noggin_data (
1002:             tip, object_type, inspection_date, lcd_inspection_id, coupling_id,
1003:             inspected_by, vehicle_hash, vehicle, vehicle_id,
1004:             trailer_hash, trailer, trailer_id,
1005:             trailer2_hash, trailer2, trailer2_id,
1006:             trailer3_hash, trailer3, trailer3_id,
1007:             job_number, run_number, driver_loader_name,
1008:             department_hash, department, team_hash, team,
1009:             load_compliance, processing_status, has_unknown_hashes,
1010:             total_attachments, csv_imported_at,
1011:             straps, no_of_straps, chains, mass,
1012:             api_meta_created_date, api_meta_modified_date,
1013:             api_meta_security, api_meta_type, api_meta_tip,
1014:             api_meta_sid, api_meta_branch, api_meta_parent,
1015:             api_meta_errors, api_meta_raw, api_payload_raw, raw_json
1016:         ) VALUES (
1017:             %s, %s, %s, %s, %s,
1018:             %s, %s, %s, %s,
1019:             %s, %s, %s,
1020:             %s, %s, %s,
1021:             %s, %s, %s,
1022:             %s, %s, %s,
1023:             %s, %s, %s, %s,
1024:             %s, %s, %s,
1025:             %s, %s,
1026:             %s, %s, %s, %s,
1027:             %s, %s,
1028:             %s, %s, %s,
1029:             %s, %s, %s,
1030:             %s, %s, %s, %s
1031:         )
1032:         ON CONFLICT (tip) DO UPDATE SET
1033:             object_type = EXCLUDED.object_type,
1034:             inspection_date = EXCLUDED.inspection_date,
1035:             lcd_inspection_id = EXCLUDED.lcd_inspection_id,
1036:             coupling_id = EXCLUDED.coupling_id,
1037:             inspected_by = EXCLUDED.inspected_by,
1038:             vehicle_hash = EXCLUDED.vehicle_hash,
1039:             vehicle = EXCLUDED.vehicle,
1040:             vehicle_id = EXCLUDED.vehicle_id,
1041:             trailer_hash = EXCLUDED.trailer_hash,
1042:             trailer = EXCLUDED.trailer,
1043:             trailer_id = EXCLUDED.trailer_id,
1044:             trailer2_hash = EXCLUDED.trailer2_hash,
1045:             trailer2 = EXCLUDED.trailer2,
1046:             trailer2_id = EXCLUDED.trailer2_id,
1047:             trailer3_hash = EXCLUDED.trailer3_hash,
1048:             trailer3 = EXCLUDED.trailer3,
1049:             trailer3_id = EXCLUDED.trailer3_id,
1050:             job_number = EXCLUDED.job_number,
1051:             run_number = EXCLUDED.run_number,
1052:             driver_loader_name = EXCLUDED.driver_loader_name,
1053:             department_hash = EXCLUDED.department_hash,
1054:             department = EXCLUDED.department,
1055:             team_hash = EXCLUDED.team_hash,
1056:             team = EXCLUDED.team,
1057:             load_compliance = EXCLUDED.load_compliance,
1058:             processing_status = EXCLUDED.processing_status,
1059:             has_unknown_hashes = EXCLUDED.has_unknown_hashes,
1060:             total_attachments = EXCLUDED.total_attachments,
1061:             straps = EXCLUDED.straps,
1062:             no_of_straps = EXCLUDED.no_of_straps,
1063:             chains = EXCLUDED.chains,
1064:             mass = EXCLUDED.mass,
1065:             api_meta_created_date = EXCLUDED.api_meta_created_date,
1066:             api_meta_modified_date = EXCLUDED.api_meta_modified_date,
1067:             api_meta_security = EXCLUDED.api_meta_security,
1068:             api_meta_type = EXCLUDED.api_meta_type,
1069:             api_meta_tip = EXCLUDED.api_meta_tip,
1070:             api_meta_sid = EXCLUDED.api_meta_sid,
1071:             api_meta_branch = EXCLUDED.api_meta_branch,
1072:             api_meta_parent = EXCLUDED.api_meta_parent,
1073:             api_meta_errors = EXCLUDED.api_meta_errors,
1074:             api_meta_raw = EXCLUDED.api_meta_raw,
1075:             api_payload_raw = EXCLUDED.api_payload_raw,
1076:             raw_json = EXCLUDED.raw_json,
1077:             updated_at = CURRENT_TIMESTAMP
1078:         """,
1079:         (
1080:             tip_value, object_type, inspection_date, lcd_inspection_id, coupling_id,
1081:             response_data.get('inspectedBy'), vehicle_hash, vehicle, response_data.get('vehicleId'),
1082:             trailer_hash, trailer, response_data.get('trailerId'),
1083:             trailer2_hash, trailer2, response_data.get('trailerId2'),
1084:             trailer3_hash, trailer3, response_data.get('trailerId3'),
1085:             response_data.get('jobNumber'), response_data.get('runNumber'), response_data.get('driverLoaderName'),
1086:             department_hash, department, team_hash, team,
1087:             load_compliance, 'api_success', has_unknown,
1088:             len(response_data.get('attachments', [])), None,
1089:             response_data.get('straps'), response_data.get('noOfStraps'), 
1090:             response_data.get('chains'), response_data.get('mass'),
1091:             api_meta_created, api_meta_modified,
1092:             meta.get('security'), meta.get('type'), meta.get('tip'),
1093:             meta.get('sid'), meta.get('branch'), parent_array,
1094:             json.dumps(meta.get('errors', [])), json.dumps(meta), json.dumps(response_data),
1095:             json.dumps(response_data)
1096:         )
1097:     )
1098: 
1099:     logger.debug(f"Inserted/updated noggin_data record for TIP {tip_value}")
1100: 
1101: def calculate_next_retry_time(retry_count: int) -> datetime:
1102:     """Calculate next retry time with exponential backoff"""
1103:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1104:     retry_backoff_multiplier: int = config.getint('retry', 'retry_backoff_multiplier')
1105: 
1106:     if retry_count >= max_retry_attempts:
1107:         return datetime.now() + timedelta(days=365)
1108: 
1109:     backoff_seconds: int = (retry_backoff_multiplier ** retry_count) * 60
1110:     max_backoff_seconds: int = 3600
1111:     backoff_seconds = min(backoff_seconds, max_backoff_seconds)
1112: 
1113:     return datetime.now() + timedelta(seconds=backoff_seconds)
1114: 
1115: 
1116: def get_tips_to_process_from_database(limit: int = 10) -> List[Dict[str, Any]]:
1117:     """
1118:     Query database for TIPs that need processing
1119:     Priority: failed  interrupted  partial  api_failed  pending
1120:     """
1121:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1122: 
1123:     query: str = """
1124:         SELECT tip, processing_status, retry_count, next_retry_at
1125:         FROM noggin_data
1126:         WHERE permanently_failed = FALSE
1127:           AND (
1128:               (processing_status IN ('failed', 'interrupted', 'partial', 'api_failed')
1129:                AND retry_count < %s
1130:                AND (next_retry_at IS NULL OR next_retry_at <= CURRENT_TIMESTAMP))
1131:               OR processing_status = 'pending'
1132:           )
1133:         ORDER BY
1134:             CASE processing_status
1135:                 WHEN 'failed' THEN 1
1136:                 WHEN 'interrupted' THEN 2
1137:                 WHEN 'partial' THEN 3
1138:                 WHEN 'api_failed' THEN 4
1139:                 WHEN 'pending' THEN 5
1140:                 ELSE 6
1141:             END,
1142:             csv_imported_at ASC
1143:         LIMIT %s
1144:     """
1145: 
1146:     tips: List[Dict[str, Any]] = db_manager.execute_query_dict(query, (max_retry_attempts, limit))
1147:     return tips
1148: 
1149: 
1150: def mark_permanently_failed(tip_value: str) -> None:
1151:     """Mark TIP as permanently failed after max retries"""
1152:     db_manager.execute_update(
1153:         """
1154:         UPDATE noggin_data
1155:         SET permanently_failed = TRUE,
1156:             processing_status = 'failed',
1157:             last_error_message = 'Max retry attempts exceeded'
1158:         WHERE tip = %s
1159:         """,
1160:         (tip_value,)
1161:     )
1162:     logger.warning(f"TIP {tip_value} marked as permanently failed")
1163: 
1164: def should_process_tip(tip_value: str) -> Tuple[bool, Optional[int]]:
1165:     """
1166:     Check if TIP should be processed based on database state
1167: 
1168:     Returns:
1169:         Tuple of (should_process, current_retry_count)
1170:     """
1171:     result: List[Dict[str, Any]] = db_manager.execute_query_dict(
1172:         """
1173:         SELECT processing_status, all_attachments_complete, retry_count,
1174:                permanently_failed, next_retry_at
1175:         FROM noggin_data
1176:         WHERE tip = %s
1177:         """,
1178:         (tip_value,)
1179:     )
1180: 
1181:     if not result:
1182:         logger.debug(f"TIP {tip_value} not in database - will process")
1183:         return True, 0
1184: 
1185:     record: Dict[str, Any] = result[0]
1186:     status: str = record['processing_status']
1187:     all_complete: bool = record['all_attachments_complete']
1188:     retry_count: int = record['retry_count'] or 0
1189:     permanently_failed: bool = record['permanently_failed']
1190:     next_retry_at: Optional[datetime] = record['next_retry_at']
1191: 
1192:     if permanently_failed:
1193:         logger.info(f"TIP {tip_value} permanently failed - skipping")
1194:         return False, retry_count
1195: 
1196:     if status == 'complete' and all_complete:
1197:         logger.info(f"TIP {tip_value} already completed successfully - skipping")
1198:         return False, retry_count
1199: 
1200:     if next_retry_at and datetime.now() < next_retry_at:
1201:         wait_seconds: float = (next_retry_at - datetime.now()).total_seconds()
1202:         logger.debug(f"TIP {tip_value} in backoff period - retry in {wait_seconds:.0f}s")
1203:         return False, retry_count
1204: 
1205:     max_retry_attempts: int = config.getint('retry', 'max_retry_attempts')
1206:     if retry_count >= max_retry_attempts:
1207:         mark_permanently_failed(tip_value)
1208:         return False, retry_count
1209: 
1210:     logger.info(f"TIP {tip_value} needs processing (status: {status}, retry: {retry_count})")
1211:     return True, retry_count
1212: 
1213: 
1214: def get_total_tip_count(tip_csv_file_path: Path) -> int:
1215:     """Count valid TIPs in CSV for progress tracking"""
1216:     try:
1217:         import csv
1218: 
1219:         with open(tip_csv_file_path, 'r', newline='', encoding='utf-8') as file:
1220:             tip_csv_reader = csv.reader(file)
1221:             header: List[str] = next(tip_csv_reader)
1222: 
1223:             header = [col.strip().lower() for col in header]
1224:             tip_column_index: int = header.index('tip')
1225: 
1226:             valid_tip_count: int = 0
1227:             for row in tip_csv_reader:
1228:                 if row and len(row) > tip_column_index and row[tip_column_index].strip():
1229:                     valid_tip_count += 1
1230: 
1231:             return valid_tip_count
1232: 
1233:     except Exception as e:
1234:         logger.warning(f"Could not count TIPs: {e}")
1235:         return 0
1236: 
1237: 
1238: def update_progress_tracking(processed_count: int, total_count: int, start_time_val: float) -> None:
1239:     """Display progress updates with time estimates"""
1240:     if processed_count == 0:
1241:         return
1242: 
1243:     elapsed_time: float = time.perf_counter() - start_time_val
1244:     tips_per_second: float = processed_count / elapsed_time
1245:     remaining_tips: int = total_count - processed_count
1246: 
1247:     if tips_per_second > 0:
1248:         estimated_remaining_seconds: float = remaining_tips / tips_per_second
1249: 
1250:         if estimated_remaining_seconds >= 3600:
1251:             time_estimate: str = f"{estimated_remaining_seconds/3600:.1f} hours"
1252:         elif estimated_remaining_seconds >= 60:
1253:             time_estimate = f"{estimated_remaining_seconds/60:.1f} minutes"
1254:         else:
1255:             time_estimate = f"{estimated_remaining_seconds:.1f} seconds"
1256: 
1257:         progress_percentage: float = (processed_count / total_count) * 100
1258: 
1259:         logger.info(f"Progress: {processed_count}/{total_count} ({progress_percentage:.1f}%) - "
1260:                    f"Rate: {tips_per_second:.2f} TIPs/sec - ETA: {time_estimate}")
1261: 
1262: 
1263: def log_shutdown_summary(processed_count: int, total_count: int, start_time_val: float, reason: str = "manual") -> None:
1264:     """Log comprehensive shutdown summary"""
1265:     elapsed_time: float = time.perf_counter() - start_time_val
1266:     completion_percentage: float = (processed_count / total_count) * 100 if total_count > 0 else 0
1267: 
1268:     logger.info("="*80)
1269:     logger.info("SHUTDOWN SUMMARY")
1270:     logger.info("="*80)
1271:     logger.info(f"Shutdown reason: {reason}")
1272:     logger.info(f"TIPs processed:  {processed_count:,} of {total_count:,} ({completion_percentage:.1f}%)")
1273:     logger.info(f"Processing time: {elapsed_time/3600:.1f} hours")
1274:     if processed_count > 0:
1275:         logger.info(f"Average rate: {processed_count/elapsed_time:.2f} TIPs/second")
1276:         remaining_estimate: float = (total_count - processed_count) * (elapsed_time / processed_count)
1277:         logger.info(f"Estimated time for remaining: {remaining_estimate/3600:.1f} hours")
1278:     logger.info("All work saved to PostgreSQL database")
1279:     logger.info("="*80)
1280: 
1281: 
1282: def main() -> int:
1283:     """Main processing function"""
1284:     global current_tip_being_processed
1285: 
1286:     tip_csv_file_path: Path = Path('tip.csv')
1287: 
1288:     if not tip_csv_file_path.exists():
1289:         logger.error(f"TIP CSV file not found: {tip_csv_file_path}")
1290:         return 1
1291: 
1292:     total_tip_count: int = get_total_tip_count(tip_csv_file_path)
1293:     logger.info(f"Found {total_tip_count} valid TIPs to process")
1294: 
1295:     processed_count: int = 0
1296:     main_start_time: float = time.perf_counter()
1297: 
1298:     logger.info(f"Opening TIP CSV file: {tip_csv_file_path}")
1299: 
1300:     import csv
1301: 
1302:     with open(tip_csv_file_path, 'r', newline='', encoding='utf-8') as file:
1303:         tip_csv_reader = csv.reader(file)
1304:         header: List[str] = next(tip_csv_reader)
1305: 
1306:         header = [col.strip().lower() for col in header]
1307:         logger.info(f"CSV headers: {header}")
1308: 
1309:         try:
1310:             tip_column_index: int = header.index('tip')
1311:             logger.info(f"TIP column found at index {tip_column_index}")
1312:         except ValueError:
1313:             logger.error(f"CSV must contain 'tip' column. Found: {header}")
1314:             return 1
1315: 
1316:         for row_num, row in enumerate(tip_csv_reader, start=2):
1317:             if not shutdown_handler.should_continue_processing():
1318:                 logger.warning(f"Graceful shutdown after processing {processed_count} TIPs")
1319:                 break
1320: 
1321:             if not row or all(not cell.strip() for cell in row):
1322:                 continue
1323: 
1324:             if len(row) <= tip_column_index:
1325:                 logger.warning(f"Row {row_num}: insufficient columns")
1326:                 continue
1327: 
1328:             tip_value: str = row[tip_column_index].strip()
1329:             if not tip_value:
1330:                 continue
1331: 
1332:             current_tip_being_processed = tip_value
1333: 
1334:             should_process, current_retry_count = should_process_tip(tip_value)
1335:             if not should_process:
1336:                 continue
1337: 
1338:             processed_count += 1
1339: 
1340:             if processed_count % 10 == 0:
1341:                 update_progress_tracking(processed_count, total_tip_count, main_start_time)
1342: 
1343:             try:
1344:                 circuit_breaker.before_request()
1345:             except CircuitBreakerError as e:
1346:                 logger.warning(f"Circuit breaker blocked request for TIP {tip_value}: {e}")
1347:                 time.sleep(10)
1348:                 continue
1349: 
1350:             endpoint: str = endpoint_template.replace('$tip', tip_value)
1351:             url: str = base_url + endpoint
1352:             logger.info(f"Processing TIP {processed_count}/{total_tip_count}: {tip_value} (retry: {current_retry_count})")
1353:             logger.debug(f"Request URL: {url}")
1354: 
1355:             try:
1356:                 api_start_time: float = time.perf_counter()
1357:                 response: requests.Response = requests.get(url, headers=headers, timeout=api_timeout)
1358:                 circuit_breaker.record_success()
1359:                 api_retry_count: int = 0
1360: 
1361:                 if response.status_code == 429:
1362:                     circuit_breaker.record_failure()
1363:                     logger.warning(f"Rate limited for TIP {tip_value}. Sleeping {too_many_requests_sleep_time}s")
1364:                     time.sleep(too_many_requests_sleep_time)
1365:                     try:
1366:                         circuit_breaker.before_request()
1367:                         response = requests.get(url, headers=headers, timeout=api_timeout)
1368:                         circuit_breaker.record_success()
1369:                         api_retry_count = 1
1370:                     except CircuitBreakerError as cb_error:
1371:                         logger.warning(f"Circuit breaker blocked retry: {cb_error}")
1372:                         
1373:                         new_retry_count: int = current_retry_count + 1
1374:                         next_retry: datetime = calculate_next_retry_time(new_retry_count)
1375:                         
1376:                         db_manager.execute_update(
1377:                             """
1378:                             INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1379:                                                     retry_count, last_retry_at, next_retry_at)
1380:                             VALUES (%s, %s, %s, %s, %s, %s, %s)
1381:                             ON CONFLICT (tip) DO UPDATE SET
1382:                                 processing_status = EXCLUDED.processing_status,
1383:                                 last_error_message = EXCLUDED.last_error_message,
1384:                                 retry_count = EXCLUDED.retry_count,
1385:                                 last_retry_at = EXCLUDED.last_retry_at,
1386:                                 next_retry_at = EXCLUDED.next_retry_at
1387:                             """,
1388:                             (tip_value, object_type, 'api_failed', str(cb_error),
1389:                              new_retry_count, datetime.now(), next_retry)
1390:                         )
1391:                         continue
1392:                     except requests.exceptions.RequestException as retry_error:
1393:                         circuit_breaker.record_failure()
1394:                         logger.error(f"Retry failed for TIP {tip_value}: {retry_error}", exc_info=True)
1395: 
1396:                         new_retry_count = current_retry_count + 1
1397:                         next_retry = calculate_next_retry_time(new_retry_count)
1398: 
1399:                         db_manager.execute_update(
1400:                             """
1401:                             INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1402:                                                     retry_count, last_retry_at, next_retry_at)
1403:                             VALUES (%s, %s, %s, %s, %s, %s, %s)
1404:                             ON CONFLICT (tip) DO UPDATE SET
1405:                                 processing_status = EXCLUDED.processing_status,
1406:                                 last_error_message = EXCLUDED.last_error_message,
1407:                                 retry_count = EXCLUDED.retry_count,
1408:                                 last_retry_at = EXCLUDED.last_retry_at,
1409:                                 next_retry_at = EXCLUDED.next_retry_at
1410:                             """,
1411:                             (tip_value, object_type, 'api_failed', str(retry_error),
1412:                              new_retry_count, datetime.now(), next_retry)
1413:                         )
1414: 
1415:                         db_manager.execute_update(
1416:                             """
1417:                             INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1418:                             VALUES (%s, %s, %s, %s)
1419:                             """,
1420:                             (tip_value, 'api_failed', str(retry_error), json.dumps({'url': url}))
1421:                         )
1422:                         continue
1423: 
1424:                 if response.status_code == 200:
1425:                     logger.info(f"Successful API response for TIP {tip_value}")
1426:                     response_data: Dict[str, Any] = response.json()
1427: 
1428:                     insert_noggin_data_record(tip_value, response_data)
1429: 
1430:                     lcd_inspection_id: str = response_data.get('lcdInspectionId', 'unknown')
1431: 
1432:                     process_attachments(response_data, lcd_inspection_id, tip_value)
1433: 
1434:                 else:
1435:                     circuit_breaker.record_failure()
1436:                     error_details: str = handle_api_error(response, tip_value, url)
1437:                     logger.error(error_details)
1438:                     session_logger.info(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\t{tip_value}\tAPI_ERROR_{response.status_code}\t0\tERROR")
1439: 
1440:                     new_retry_count = current_retry_count + 1
1441:                     next_retry = calculate_next_retry_time(new_retry_count)
1442: 
1443:                     db_manager.execute_update(
1444:                         """
1445:                         INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1446:                                                 retry_count, last_retry_at, next_retry_at)
1447:                         VALUES (%s, %s, %s, %s, %s, %s, %s)
1448:                         ON CONFLICT (tip) DO UPDATE SET
1449:                             processing_status = EXCLUDED.processing_status,
1450:                             last_error_message = EXCLUDED.last_error_message,
1451:                             retry_count = EXCLUDED.retry_count,
1452:                             last_retry_at = EXCLUDED.last_retry_at,
1453:                             next_retry_at = EXCLUDED.next_retry_at
1454:                         """,
1455:                         (tip_value, object_type, 'api_failed', error_details,
1456:                          new_retry_count, datetime.now(), next_retry)
1457:                     )
1458: 
1459:                     db_manager.execute_update(
1460:                         """
1461:                         INSERT INTO processing_errors (tip, error_type, error_message, error_details)
1462:                         VALUES (%s, %s, %s, %s)
1463:                         """,
1464:                         (tip_value, 'api_failed', error_details, json.dumps({
1465:                             'http_status': response.status_code,
1466:                             'url': url
1467:                         }))
1468:                     )
1469: 
1470:             except requests.exceptions.ConnectionError as connection_error:
1471:                 circuit_breaker.record_failure()
1472:                 logger.error(f"Connection error for TIP {tip_value}: {connection_error}", exc_info=True)
1473: 
1474:                 new_retry_count = current_retry_count + 1
1475:                 next_retry = calculate_next_retry_time(new_retry_count)
1476: 
1477:                 db_manager.execute_update(
1478:                     """
1479:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1480:                                             retry_count, last_retry_at, next_retry_at)
1481:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1482:                     ON CONFLICT (tip) DO UPDATE SET
1483:                         processing_status = EXCLUDED.processing_status,
1484:                         last_error_message = EXCLUDED.last_error_message,
1485:                         retry_count = EXCLUDED.retry_count,
1486:                         last_retry_at = EXCLUDED.last_retry_at,
1487:                         next_retry_at = EXCLUDED.next_retry_at
1488:                     """,
1489:                     (tip_value, object_type, 'api_failed', str(connection_error),
1490:                      new_retry_count, datetime.now(), next_retry)
1491:                 )
1492:                 continue
1493: 
1494:             except requests.exceptions.RequestException as request_error:
1495:                 circuit_breaker.record_failure()
1496:                 logger.error(f"Request error for TIP {tip_value}: {request_error}", exc_info=True)
1497: 
1498:                 new_retry_count = current_retry_count + 1
1499:                 next_retry = calculate_next_retry_time(new_retry_count)
1500: 
1501:                 db_manager.execute_update(
1502:                     """
1503:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1504:                                             retry_count, last_retry_at, next_retry_at)
1505:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1506:                     ON CONFLICT (tip) DO UPDATE SET
1507:                         processing_status = EXCLUDED.processing_status,
1508:                         last_error_message = EXCLUDED.last_error_message,
1509:                         retry_count = EXCLUDED.retry_count,
1510:                         last_retry_at = EXCLUDED.last_retry_at,
1511:                         next_retry_at = EXCLUDED.next_retry_at
1512:                     """,
1513:                     (tip_value, object_type, 'api_failed', str(request_error),
1514:                      new_retry_count, datetime.now(), next_retry)
1515:                 )
1516:                 continue
1517: 
1518:             except Exception as e:
1519:                 circuit_breaker.record_failure()
1520:                 logger.error(f"Unexpected error processing TIP {tip_value}: {e}", exc_info=True)
1521: 
1522:                 new_retry_count = current_retry_count + 1
1523:                 next_retry = calculate_next_retry_time(new_retry_count)
1524: 
1525:                 db_manager.execute_update(
1526:                     """
1527:                     INSERT INTO noggin_data (tip, object_type, processing_status, last_error_message,
1528:                                             retry_count, last_retry_at, next_retry_at)
1529:                     VALUES (%s, %s, %s, %s, %s, %s, %s)
1530:                     ON CONFLICT (tip) DO UPDATE SET
1531:                         processing_status = EXCLUDED.processing_status,
1532:                         last_error_message = EXCLUDED.last_error_message,
1533:                         retry_count = EXCLUDED.retry_count,
1534:                         last_retry_at = EXCLUDED.last_retry_at,
1535:                         next_retry_at = EXCLUDED.next_retry_at
1536:                     """,
1537:                     (tip_value, object_type, 'failed', str(e),
1538:                      new_retry_count, datetime.now(), next_retry)
1539:                 )
1540:                 continue
1541: 
1542:         current_tip_being_processed = None
1543: 
1544:     return processed_count
1545: 
1546: 
1547: if __name__ == "__main__":
1548:     try:
1549:         logger.info("Starting main processing loop")
1550:         processed_count: int = main()
1551: 
1552:         logger.info(f"Processing completed. Total TIPs processed: {processed_count}")
1553:         logger.info("Script completed successfully")
1554: 
1555:     except KeyboardInterrupt:
1556:         logger.warning("Processing interrupted by user")
1557:         log_shutdown_summary(0, 0, start_time, "keyboard_interrupt")
1558: 
1559:     except Exception as e:
1560:         logger.error(f"Unexpected error: {e}", exc_info=True)
1561:         log_shutdown_summary(0, 0, start_time, "error")
1562:         raise
1563: 
1564:     finally:
1565:         if 'db_manager' in locals():
1566:             db_manager.close_all()
1567: 
1568:         end_time: float = time.perf_counter()
1569:         total_duration: float = end_time - start_time
1570:         logger.info(f"Total execution time: {total_duration/3600:.2f} hours ({total_duration:.2f} seconds)")
1571:         logger.info(f"Session ID: {batch_session_id}")
1572:         logger.info("="*80)
1573: 
1574:         session_logger.info(f"\nSESSION END: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
1575:         session_logger.info(f"TOTAL EXECUTION TIME: {total_duration:.2f} seconds")
</file>

</files>
